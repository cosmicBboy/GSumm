Objective To explore whether there is a potential for greater use of research-based information in public health practice in a local setting. Secondly, if research-based information is relevant, to explore the extent to which this generates questioning behaviour. Design Qualitative study using focus group discussions, observation and interviews. Setting Public health practices in Norway. Participants 52 public health practitioners. Results In general, the public health practitioners had a positive attitude towards research-based information, but believed that they had few cases requiring this type of information. They did say, however, that there might be a potential for greater use. During five focus groups and six observation days we identified 28 questions/cases where it would have been appropriate to seek out research evidence according to our definition. Three of the public health practitioners identified three of these 28 cases as questions for which research-based information could have been relevant. This gap is interpreted as representing unrecognised information needs. Conclusions There is an unrealised potential in public health practice for more frequent and extensive use of research-based information. The practitioners did not appear to reflect on the need for scientific information when faced with new cases and few questions of this type were generated. Introduction In Norway, the practice of public health is organised somewhat differently than in other countries. Its core content as defined in central documents [1,2] is, however, not very different from definitions found elsewhere [3]. Broadly speaking, the practice of public health may be defined as the organisation and analysis of medical knowledge in such a way that it may be utilised by society in the making of decisions in health related questions. In Norway, this task of ensuring that health is taken into account in policy making at the local level is the responsibility of the local public health practitioner, and this post is always held by a physician. About 40-50% of physicians practising within public health hold a specialty in public health, the rest are in training. The postgraduate period lasts for five years and consists of both a theoretical course and supervised practical work. The public health practitioner collaborates extensively with other members of the health team and with several other professional groups in the community and thus holds an important position in the local decision-making process. The evidence-based health care approach implies that decisions should be based on a systematic appraisal of the best available evidence combined with an assessment of existing resources and values in society [4]. We took for granted the applicability of evidence-based decision-making in this specialty, as did Muir Gray in his article about evidence-based public health [5]. It seemed reasonable to assume that public health practitioners would have a need for research-based information and that they would have several unanswered questions in the course of one day or at least over a longer period of time. In an earlier survey [6], however, we found that public health practitioners in Norway, like clinical doctors [7,8,9,10,11,12,13,14,15], had access to or made use of research-based information sources to a very small extent. If they really had a need for this type of information, then why had they not organised access to relevant information sources, at the least by securing the services of the nearest medical library? We could see at least two possible reasons for this: Firstly, research-based information might be irrelevant for the kind of public health questions that public health practitioners face in their everyday work. We found no studies that could tell us precisely what tasks Norwegian public health practitioners actually have, but browsing through their main bulletin [16], we found no cases that appeared to require research-based information. Scattered impressions from earlier contacts with public health practitioners also implied that they do not handle many cases of this type. Could it be that the day-to-day reality of Norwegian public health practice is perhaps one of legal cases, complaints and administration, arousing the need for quite other types of information sources than research evidence? Studies of other practitioners indicate, however, that there is no reason to assume that the sources of information that are used are always the most appropriate for the problem in question [12,13]. Thus, another possible reason for this lack of access to and use of research-based information could be a lack of consciousness about the potential contribution of scientific information in decision-making processes. Given such circumstances, questions requiring research-based information are not likely to be generated and public health doctors are therefore not likely to feel a need for such resources. With a future intervention aimed at stimulating evidence-based practice in mind, we had to ask ourselves whether the use of research-based information was at all relevant in Norwegian public health practice. The main aim of this study was therefore to explore the potential for a greater use of research-based information in local public health practice. In order to achieve this goal, we focused on two issues: the practitioners' own attitudes towards the relevance of this information source and our identification of cases where it would be appropriate to seek for research-based information. Our final aim was to explore their questioning behaviour by seeing if the practitioners identified the same cases as we did. Methods Study design We defined any case that could be structured into the elements problem, intervention, and outcome [17] as cases that could benefit from research-based information. Information was defined as any stimulus that reduces uncertainty in a decision-making process and an information need as an individual's recognition of the existence of this uncertainty [18]. We also wanted to identify unrecognised or potential information needs, as defined by Osheroff and colleagues in their study of participants in a general medicine training program in the USA [19]: "Information that is important to the clinical circumstances at hand but which the physician does not realise is applicable". The use of interviews or questionnaires gives access to the respondent's recognised information needs [6,7,8,11,12]. Through observational studies, one can also obtain data about the information seeking behaviour these recognised information needs lead to [9,14,19,20]. One possible way to estimate the respondent's unrecognised or potential information needs is to look at their work tasks, i.e. what type of decisions there are to be made. We chose to start with focus groups [21] and aimed to identify practitioners' attitudes to the use of research-based information by inviting them to talk about the kind of cases they faced. During this process we also looked for cases where searching for information would be appropriate, and noted the degree in which the practitioners themselves identified the same cases. In addition, data on barriers to information use was collected, and will be reported elsewhere. We also carried out observational studies of six public health practitioners during their everyday work. The sole purpose of the observation was to identify cases where searching for scientific evidence would have been appropriate regardless of whether the physician himself did this or not. Our impressions and observations from both of these methods were explored further in individual interviews. Focus groups All of Oslo's 25 city borough doctors as well as 26 participants from a national course for public health specialists, were invited to the focus group interviews. Of these, 46 practitioners agreed to participate. A total of five focus group meetings were held in Oslo during 1997, each group consisting of 7 - 12 participants from cities and from rural communities. Two of the groups were held with only the city borough doctors participating. Most of the participants were between 40 and 50 years old, with varying degrees of experience and with varying amounts of time spent on public health work, except for the city borough doctors who worked full time (90% of public health practitioners in Norway are part-time general practitioners). Each group met once for two hours. A doctor, professor in public health (AB), conducted all meetings with LF, a library and information scientist, as an observer. At the beginning of each focus group discussion, the modifier explained to the participants that we were conducting a project to learn more about their work in order to find ways to support them. The modifier made reports after each group meeting while the observer took notes. Questions that were asked during the focus groups included what kind of cases the participants faced as public health practitioners, which information sources they used, and what we could do to help them search for and utilise information (table 1). These questions were introduced into the discussions as naturally as possible. The focus group discussions were tape-recorded and transcribed. Observation and interviews We aimed at a typical sample, i.e. with representatives from larger and smaller municipalities [22]. Six public health practitioners from two small and four larger municipalities in and around Oslo were accompanied and observed [23] during one ordinary working day, including during office work and at meetings. Unstructured field notes were taken. The reasons given for this observation were the same as those given to the focus groups participants. The interviews were semi-structured. The themes covered during the interviews varied slightly from those in the focus groups, and questions were phrased somewhat more directly (table 2). Two more questions were also added (table 2, questions no. 5 and 6). The interviews lasted for about 20 minutes and all interviews except one were tape-recorded. The tapes were transcribed. Cases were identified on the basis of the interviews and everything that we were exposed to during the observation, e.g. conversations, meetings and documents showing what they were working with now or had been working with recently. Analysing and validating data Several triangulation techniques were used to ensure the validity and comprehensiveness of our data, including methodological triangulation (focus groups, observation and interviews), investigator triangulation (focus groups were carried out by two investigators) and disciplinary triangulation (we had different disciplinary backgrounds). We carried out content analysis [24] of the data as it was collected and this gave direction to the data collection process. The amount of data was small enough to be coded and categorised manually. Since both researchers (AB and LF) participated in the focus groups we were able to discuss and compare the patterns that emerged. The reports written by the moderator after each group interview and the notes the observer had taken were compared to validate whether we assigned the same meaning to the data. Most emphasis was placed on those themes that were repeated in several contexts. We also looked for contradictory statements about the same issues in the data from the focus groups, observation and interviews respectively. Having finished the focus groups we were still unsure of the relevancy of research-based information in public health practice in Norway. The use of observations gave us an opportunity to explore this theme further and to see which cases actually turned up. Thus, the observation and the interviews were partly a control for focus group data validity and partly a supplement to them. Results Focus groups Self perceived relevance statements When asked what types of cases they faced, the practitioners were inclined to mention administrative cases or cases that could be solved with reference to a law or regulation, for instance regarding noise levels, water or air quality. When discussing health services planning, e.g. the planning of psychiatric services or youth health services, they seemed to be more occupied with the organisation of these services than with their content. They also felt that they had many cases that called for their professional judgement rather than factual knowledge: "A number of cases are [...] more about common sense. When the neighbour's bird sings so loudly in the mornings that it disturbs people's sleep. Well, it's hard to find any research-based knowledge about this, [it's better] to discuss this with someone who's dealt with something similar before." "A number of our cases are on that level, - and then there are cases like noise from a rifle range, when I phoned people all over the place and found things here and there, but it was hard to find people who knew much." "I think the most difficult cases are those that are not covered by law regulations. I would like to know how people around the table here deal with cases like that, for instance thirty cats in a housing area..." "This whole area is pretty inaccurate. [...] You have to use your head and your common sense in most cases. For instance, if you want to keep more than 202 cats the local board of health must approve this. What about a horse - is a horse more than 202 cats? What about 10 hens and 2 cats? Are they worth more than 202 cats? One could make a whole list of questions like this... Since they did not readily and voluntarily report cases that were suitable for searching research-based information, we started probing for answers. We got very different answers varying from the negative to the extremely positive: "I'm not very interested in the actual research, but I have to know everything on whether it is common practice to use that particular [noise] limit in that type of decision. "Experience-based public health, we mustn't make this too scientific, because we're using what we think is reasonable knowledge, which is based on regulations, right? "I think that working evidence-based has meaning for a great deal of the issues that are covered by environmental health care. But when dealing with other parts of public health, like management and organisation, I don't know how relevant it is. Then I think that examples and models that have been used elsewhere may be just as important. "For me, research is the truth" It would be fair to say, however, that their general viewpoint was that they rarely had to deal with questions requiring research-based information. They would, however, utilise this information source when such cases did arise: "Some cases you want to explore more deeply and then it would be useful with documentation". "[...] the point has to be that we have a university environment where we can look for knowledge, because knowledge is power in our setting too. And getting information from other places in this setting obviously gives weight to our arguments." They also thought that because of the possibilities created by the information technology for the public to stay well informed, working evidence-based would become more and more important: "As a small comment to the relationship between evidence-based and experience-based knowledge, I would think that evidence-based knowledge is becoming more important and that we're sort of being forced into being well documented." These viewpoints did not necessarily seem to lead to an identification and generation of more questions suitable for seeking research-based information. One of the participants, for example, reported that in their municipality, statistics showing high numbers of abortion among teenage girls had contributed to the establishment of a youth health centre. When asked how they knew that this was the most effective measure they could take, for instance compared to giving a more school-based intervention, the answer showed that this had not been an issue for discussion: "Well, establishing adolescent health services have been one of our priorities. We also have tried to work in other areas, e.g. giving information on sexual behaviour and about living together, in secondary school, - so we have been there as well." Cases identified During the focus group conversations we identified a number of broader themes where scientific information could be of relevance. These included psychiatry, noise pollution, infection control, problems relating to refugees, preventive measures against lifestyle diseases, planning rehabilitation, prevention of accidents and different initiatives towards children and youngsters. These themes were described very generally, with an unclear problem definition. We were able to identify six cases as clearly formulated problems qualifying for searching research-based information (in table 3). Observation and interviews Self perceived relevance statements In the interviews, as in the focus group discussions, the public health doctors generally expressed a positive attitude towards the use of research-based information. Good documentation was believed to be useful when promoting issues to be discussed by politicians: "...the more documentation and background material, the more the reason for advancing the case [.....] for political treatment. A case that is supported by research results would be of great value in advancing a case politically". They were also open to seeking out this kind of information to a greater extent: "When I think about it, there are probably more cases than we think. I suppose we could probably [identify] more cases based on the types of problems we have [....], but it's possible that we're a bit restrained about this because we don't feel that anyone is interested". If the barriers were fewer they would have made searches: "Yes, I think I would have [done this], especially when working on projects or when I work with prevention". They also felt that research-based information could be useful when preparing for talks: "In giving talks, yes, that would have been useful. Then you usually go through your own literature and often you find that there are [still things you don't know]. And instead of calling somebody and getting it over the 'phone, it would have been better to have it in print". Cases identified The main purpose of the observation was to explore whether there really were as few cases in public health practice where the use of research-based information would have been appropriate as expressed during the focus group discussions. To estimate a potential information need we took as our starting point the content of their work, and especially the cases they were to decide about. The questions faced by the public health practitioners may indicate the potential for utilising the knowledge generated from research. During the six observation days the observer (LF) attended 13 meetings. All meetings could be classified as interdisciplinary working, co-operation or information exchange meetings either within the same department as the physician or with health personnel in other departments (table 4). During the observation period we identified 22 cases/questions qualifying for searching research-based information (table 3), nine of which were identified in the largest municipality. These cases, adding to the cases identified in the focus groups, exemplify the questions a public health doctor in Norway may face. We did, in fact, observe research-based information being used, especially by the practitioner in the largest municipality. This was Norwegian research that had been gathered passively, either through the mass media or because it had been passed on by colleagues. Questioning behaviour None of the six doctors identified any questions that they had needed an answer to during the day of observation or during the past few days. When asked the following question: "Do you remember ever having a problem/question/case where you looked for research-based information?", three of the practitioners identified three questions from table 3 as questions they had either sought scientific information for or as questions where such information might have been useful (questions 4, 20, 22). Discussion Main findings We identified a potential for greater use of research-based information in public health practice. Especially when observing what public health practitioners actually do, it is easy to see that they work with problems that contain a number of questions where evidence could be of importance for decision-making. We identified 28 cases, six from the five focus group meetings and 22 from the six days of observation combined with interviews. Even though the practitioners had a general recognition of a potential for research-based information in the practice of public health, and three of them identified three of the questions, it was evident that generating these types of questions was not a common approach. When presented with relevant research-based information during the treatment of a case, the will to utilise it was present, but they did not seek it out explicitly or systematically. Strengths and weaknesses of the study It is possible that the data was somewhat biased by the fact that the focus groups were held at the Institute of Public Health and that the moderator of the focus groups was a professor in public health and thus could have been regarded as an expert in their field. Therefore, changing the setting and/or the moderator could have produced somewhat different results. On the other hand, the focus group participants seemed more keen to tell us about their everyday challenges and to make us understand their working conditions than to show us how good they were at doing their jobs. It is also possible that an increase in the number of observers, and thereby raters, could have influenced our analysis of the focus group data, as discussed by Weinberger et al. [25]. Although two of the focus groups were held with practitioners from large communities (city borough doctors) only, the results from all five groups were very similar. The same atmosphere of sincerity and a desire to show us what they really did was also communicated to us during the observations. If they guessed our hypothesis, the results indicate that this did not seem to affect them. We observed and interviewed only a small number of physicians, all of them coming from the central part of Norway. However, the analysis gave results that were consistent with the results from the analysis of the focus group material. From the second interview and on, few new issues arose, and data saturation was perceived to be achieved through the six interviews. The questions that came up during the observation period could have been coincidental, but the themes of the meetings they participated in were in accordance with the plans we had heard about during the focus groups. Our experience is that the use of focus groups, observations and interviews, in that order, were appropriate to elicit our data. These techniques gave us the opportunity to listen to the public health practitioners' own accounts and responses to the different topics as well as making our own observations. For instance, when the doctors in the focus groups and in interviews told us that they did not have many cases where it would be relevant to seek out scientific information, the observation revealed that they did at least have more than they themselves identified. As for the investigator and disciplinary triangulation, it was very useful to be able to discuss and analyse the data together, and in spite of having two totally different backgrounds we still reached the same conclusions. Unrecognised information needs Models and studies of information seeking-behaviour have mainly been occupied with recognised information needs that lead to question generating. Some studies show that clinicians do indeed generate many questions during the day [7,10,14,19], although Barrie and Ward dispute this [26]. They did not find that clinicians generated a large number of unanswered clinical questions, and attributed this to a missing questioning behaviour. Ely et al [9] in their study of information needs of family physicians also conclude that it is important to encourage physicians to ask more questions. We have tried to estimate the practitioners' unrecognised information needs regarding the use of scientific evidence by registering the cases they were or had been working with. By focusing on these cases, we excluded the needs for all other information types rising from other types of cases or decisions the practitioners had to make. Unrecognised information needs will surely result in a lack of questioning behaviour. Both the results from the analysis of focus group interviews and from the observations and interviews suggest that public health practitioners do not generate many questions that could, at least partly, be answered by using research-based information. Meaning of the study/Conclusions This study indicates that the potential for use of research-based information is greater than perceived by public health practitioners and that this gap represents an unrecognised information need. It could be claimed that our findings are the result of the unique way the Norwegian public health system is organised. Previous research has for example found that physicians working in solo practices, like most public health practitioners do in Norway, tend to ask fewer questions than those working in group practices [27]. It is more interesting, however, to compare our findings with the above mentioned study of Barrie and Ward [26], as well as with other studies indicating that general practitioners lack the attitudes, knowledge and behaviour which are necessary conditions for practising evidence-based health care [28,29,30]. In addition, a range of studies among different types of health care professionals [31,32,33] shows that scientific evidence is not the most extensively used information source. In summary, our findings support the view that lack of awareness of and use of scientific evidence is a common problem independent of health care profession and the particular health care system of each country. Also, the need for an evidence-based approach in public health is discussed in as different settings as England and USA [34,35], which shows that many of these questions and approaches may be the same across countries. The potential for facing cases that require research-based information is presumably greatest in larger communities. Common to all Norwegian municipalities, however, is the planning of services within for instance psychiatry, school health services and rehabilitation, and each of these services raises many and important questions. In situations where practitioners realise that they have an information need, they choose other information sources. This leads us to the following hypothesis: Research-based information does have a potential for greater use in public health, but while the practitioners themselves recognise this to a certain extent, too many barriers obstruct further use. Practical implications and future research The lack of use of scientific evidence by public health practitioners may be rectified by identifying what the barriers are and by designing an intervention aimed at reducing those barriers. The ultimate goal would be to implement an effective intervention as part of the speciality training, which up to now has been inadequate in this respect. Thus, in our next articles we will: 1. Present our results on why public health practitioners do not use research-based information even when they do realise they have an information gap, i.e. what barriers exist towards this information source 2. Test whether an intervention tailored to promoting questioning behaviour and addressing the identified barriers leads to an evidence-based public health practice. Pre-publication history The pre-publication history for this paper can be accessed here: Supplementary Material Table 1 Focus group interview guide Table 2 Interview guide Table 3 Identified questions/cases Table 4 Main themes of meetings during observation period
Background An association between mumps-measles-rubella (MMR) vaccination and the onset of symptoms typical of autism has recently been suggested. This has led to considerable concern about the safety of the vaccine. Methods A matched case-control study using data derived form the United Kingdom General Practice Research Database. Children with a possible diagnosis of autism will be identified from their electronic health records. All diagnoses will be validated by a detailed review of hospital letters and by using information derived from a parental questionnaire. Ten controls per case will be selected from the database. Conditional logistic regression will be used to assess the association between MMR vaccination and autism. In addition case series analyses will be undertaken to estimate the relative incidence of onset of autism in defined time intervals after vaccination. The study is funded by the United Kingdom Medical Research Council. Discussion Electronic health databases offer tremendous opportunities for evaluating the adverse effects of vaccines. However there is much scope for bias and confounding. The rigorous validation of all diagnoses and the collection of additional information by parental questionnaire in this study are essential to minimise the possibility of misleading results. Background The epidemiology of autism Autism is a pervasive developmental disorder characterised by abnormalities in the development of language, communication abilities, and social interactions and by a pattern of restricted play and behaviour which tends to be highly repetitive, unimaginative and rigid [1]. By definition, the abnormalities must be present by the age of three years, although the diagnosis is usually not made until the age of four or five years [2]. In studies of the consistency of diagnosis there has been a high consensus between psychiatrists and coding instruments [3]. The age at which parents first recognise an abnormality is variable, with 40% of autistic children having shown typical features by the age of one year and most by the age of two years[4]. This age is influenced by the degree of associated mental retardation and birth order (the less severe and first born children tending to have later age of parental recognition) [5]. Most population-based studies have found a prevalence of autism between 5 and 10 per 10,000 children [6]. MMR vaccination and autism In 1998 a link was suggested between mumps-measles-rubella (MMR) vaccination and autism [7]. This was based on an uncontrolled case series of 12 children referred to a paediatric gastroenterology unit with a history of normal development followed by loss of acquired skills, including language, together with diarrhoea and abdominal pain. It was suggested that the gastrointestinal and developmental symptoms were a syndrome that could have been triggered by MMR vaccination. The study was widely criticised [8,9] but generated considerable media interest [10] and led to a small fall in MMR coverage in the United Kingdom [11]. A larger case series of 60 children with the same combination of clinical findings has recently been published [12]. Since the first study by Wakefield et al, a number of published studies have looked specifically at this issue. In a small study from Finland, among 31 children who had reported a gastrointestinal adverse reaction to MMR vaccination, none had subsequently developed signs of autism [13]. A similar larger study looked at all notified serious adverse events following MMR vaccination in Finland over a 14 year period [14]. There were no new cases of autism among 173 notified adverse events. However such routine passive surveillance systems have a number of weaknesses for epidemiological studies [15]. There is no control group, the quality of the data may be suboptimal and detecting an effect depends entirely on clinicians believing a new illness was due to vaccination. In Sweden no increase was apparent in the incidence of autism following the introduction of MMR vaccination [16]. Both these studies included small numbers of children with autism and had limited ability to assess the link between MMR vaccine and autism. The United Kingdom Committee on Safety of Medicines set up a working party to assess parental and medical reports of children who had developed autism, Crohn's disease or similar disorders following MMR vaccination. The Working Party Report was, by its own description, solely a descriptive account of those children whose parents had sought legal advice about possible vaccine damage [17]. The Report highlighted bias in the way affected children were selected for inclusion in the study and the lack of any control group before concluding that they could not prove or refute the suggested associations between MMR vaccine and autism. A single large high quality epidemiological study has been published [18]. This study included 293 children with confirmed autism from North Thames health districts. From time series trends analysis, age of diagnosis in vaccinated and unvaccinated groups and a case series analysis, the authors concluded there was no evidence to support an association. The study did find a positive association between MMR vaccination and first parental concerns in the first six months following vaccination. Although the authors considered that this finding was likely to be either a chance finding or due to inaccuracy in recalling the date of onset of symptoms, this interpretation has been disputed [19]. It was also suggested that because the study only considered relatively short risk periods after vaccination, a causal link may have been missed [20]. The authors of the study have undertaken a re-analysis looking at longer post vaccination risk periods, and again found no evidence to support a link between MMR vaccination and autism (Farrington CP, personal communication). In the light of continuing concern about the proposed link between MMR vaccination and autism [21,22,23,24,25,26] we plan to undertake a case-control study using data derived form the General Practice Research Database. Objectives The study has two linked objectives with respect to MMR vaccination. Firstly to determine if autistic children are more likely to have received MMR vaccine prior to disease onset. Secondly to examine whether there is any association between clinical onset of disease and the timing of MMR vaccination. Materials and methods The General Practice Research Database (GPRD) The GPRD (previously known as the VAMP Research Bank) was set up in 1987 and is now held by the Medicines Control Agency [27,28]. It contains complete prescribing and diagnostic information from a large number of general practices and is the largest source of continuous data on illness and prescribing habits in the United Kingdom. Over 200 published studies have been completed using the database. Participating general practitioners were given instruction over a 12-month period regarding standardised recording of clinical information into their computing systems. The general practices are broadly representative of all practices in the United Kingdom in terms of geographical distribution and size and the age and sex distributions of the population included in the GPRD are similar to the whole United Kingdom population [29]. The data available directly from the database include all drug prescriptions and their indication, a record of every consultation and of every diagnosis. The data collected is audited regularly and the participating general practices are subjected to a number of quality checks. Of the practices contributing to the database, about 280 practices, with a combined population of around 2.1 million patients currently pass these rigorous quality checks. The quality of the information in the database has been validated in a number of independent studies and has been found to be high [30,31,32,33,34,35]. The general practitioners keep all referral letters, hospital discharge summaries and other clinically relevant letters in a manual file. In addition to the electronic health record, questionnaires can be sent to patients (or their parents) via general practitioners, and copies of letters relating to referrals and hospital care can be obtained. The data are held anonymously in the central GPRD database, with patient identifiers removed. Identification of affected children Children with putative autism will be identified by searching the whole electronic record of all people included in the GPRD for diagnostic codes which possibly relate to a diagnosis of autism. MMR vaccine was introduced in the United Kingdom for all children aged 12 to 15 months in October 1988. An MMR catch-up campaign was also launched for older children in 1988. We will separately identify those children with putative autism born after and before mid-1987, which separates out those children likely to have received the MMR vaccine around the age of 1 year and those likely to have received it at a later date. Separate analyses will be conducted on these two groups. Although all major past diagnoses are recorded in practice computers when new patients register with practices, such recording may be incomplete. To overcome this potential problem, we will identify children first diagnosed when they were registered with practices participating in the GPRD. Children diagnosed prior to registration with the GPRD will be analysed separately with their matched controls. The results from these two groups will be pooled if they are similar. Identification and selection of controls For each affected child we will sample two groups of matched controls from the GPRD. The first group will consist of five people with no record of autism matched on age ( one year), sex and practice. Matching is this group aims to control for possible confounding by the general practice with which participants are registered. The second group will be of similar size and will be matched on age and sex but not on practice, to avoid the possibility of overmatching. For children diagnosed while registered with a GPRD practice, the date of diagnosis will be called the index date. The controls will be selected from those patients registered with the GPRD on the index date of the affected child to whom they are matched. We will not be able to apply the same method for selecting controls for children with autism diagnosed prior to registering with a practice participating in the GPRD because they will not have an index date. Therefore the matched controls for children diagnosed prior to registering with a practice participating in the GPRD will be selected from all patients registered with the GPRD on the date the affected child registered with the GPRD. Questionnaire to parents of affected children and controls Subject to ethical approval, a questionnaire will be sent to the parents of all affected children and to two controls per affected child, one matched on practice and one not matched on practice and closest in age to the affected child. The questionnaire to parents of children with autism will include an autism screening questionnaire[36] and will solicit information on: the date of first symptoms of autism and earliest date of parental concern about symptoms possibly related to autism; the educational status of the child; the knowledge and beliefs of parents regarding the causes of autism; and family history of pervasive developmental problems. In addition the questionnaire will specifically ask about family history of pervasive developmental problems, genetic disorders and about regression (loss of skills) allowing us to classify affected children into those with reported regression and those with no regression. For both affected children and controls the questionnaire will include questions about: the socioeconomic status of the parents; birth order and family size; history of bowel disturbance in the child; and vaccination history. Diagnosis: definition and validation As a first step to validate the diagnoses, copies of all hospital summaries will be requested from the GPs concerned. Previous studies using the GPRD have obtained full copies of hospital summaries on over 90% of patients still registered with a collaborating practices [35,37]. We will obtain copies of letters relating to both autism and to all other reasons for hospital investigation or attendance, including bowel investigations and inflammatory bowel disease (see below). The basis for the diagnosis of autism, evidence of associated genetic disorders and the date of first attendance for possible autism will be extracted from the records. There is strong agreement among child psychiatrists about concepts of and operational definitions for autism [3]. We believe that no child will be labelled as autistic in the GP record without referral to child psychiatry services. Two studies have specifically documented the completeness of the information in the GPRD about referrals occurring and their outcome [30,31]. All information about children possibly affected by an autistic spectrum disorder, including information about the current educational status of the child from the questionnaire, will be reviewed independently by two child psychiatrists. They will use DSM-IV / ICD 10 research diagnostic criteria to define autistic spectrum disorders, and will attempt to subtype the disorders according to their phenomenology. In particular they will separate and sub-classify autistic disorder in DSM-IV or childhood autism in ICD-10, Asperger's disorder, atypical autism / pervasive developmental disorder not otherwise specified, and other forms of pervasive developmental disorders (i.e. Rett's syndrome and childhood disintegrative disorder). This will be achieved by rating the developmental abnormalities on a symptom basis and then applying diagnostic algorithms. They will also make an overall global judgement about the clinical pattern and rate their confidence in this final diagnostic judgement in order to allow for difficult or improbable diagnoses to be treated separately. Inter-rater reliability estimates will result from this exercise. Separate analyses will be carried out for children with a definite diagnosis and for children with a definite or probable diagnosis in order to assess the potential impact of misclassification. Exclusion of affected children with an alternative aetiology Inclusion of affected children who have an established alternative aetiology may bias the estimated odds ratio for the association between vaccination and adverse outcome towards unity [38]. Some children will have medical disorders thought to have a causal association with autism (fragile X disorder, tuberous sclerosis, phenylketonuria, congenital rubella) and will be excluded. A recent review estimated that this will lead to the exclusion of at most 6% of affected children [6]. Determination of date of onset From the GP record, hospital letters and parental questionnaire for each affected child we will extract the date of: first attendance to the GP with symptoms or problems potentially relating to a future diagnosis of autism, such as behavioural difficulties (e.g. sleeping or eating difficulties), delay in motor development and milestones, delay in language development, abnormalities in social development (for example delayed smiling, lack of reciprocity, lack of anticipation, odd behaviours); first concerns or symptoms as recorded in the hospital letters; definitive diagnosis from the hospital letters; first parental concern of symptoms of autism collected retrospectively. The first three dates will be based on existing records and both the date and the relationship of the date to the timing of MMR vaccination will not be affected by errors of memory. First parental concerns about autism may have occurred many years ago and some error in accurately remembering the exact date is to be expected. In addition, it is possible that parental recall of the date of onset of symptoms relative to the timing of MMR vaccination may be affected by the recent publicity about a possible link between MMR and autism. The proposed link between MMR vaccine and autism was first publicised in February 1998. After this date public and media concern about the possible link may have affected the likelihood of a child attending the GP with problems and in particular the timing of the presentation relative to MMR vaccination. Children with a date of first symptoms after February 1998 will be analysed separately to assess the effect of possible bias. For the main analyses the date of onset will be the earliest of either the date of first attendance to the GP with symptoms potentially relating to a future diagnosis of autism or the date of first concerns or symptoms as recorded in the hospital letters. Assessment of exposure Exposure to MMR vaccine will be extracted from the GP electronic record. This method has two advantages. Firstly it will avoid recall bias either about vaccine status or about the timing of vaccination relative to the onset of symptoms. Secondly there are good reasons to expect the vaccine data to be complete. All general practitioners participating in the GPRD undertake to include all medications prescribed or administered in the computerised record. In addition, United Kingdom general practitioners have a financial incentive to accurately record childhood vaccination status. Finally there is excellent agreement between prescribing data from the GPRD and national data from the Prescription Pricing Authority[34]. Confounding Potential confounding factors include those factors known to affect uptake of vaccination in the United Kingdom: the knowledge and attitude of the health care provider; presence in the family of a child with a major illness; social class; birth order; family size; education of parents; and religion[39]. Matching on general practice for one of the control groups will control for confounding by health care provider. Data on the other potential confounding factors will be derived from the questionnaire to parents of affected children and controls. Very little is known about factors associated with autism and its diagnosis, although a family history of autism is a clear risk factor. Age of parental recognition is known to be associated with sibship order. We will be collecting information on these variables in the questionnaire. These potential confounders will be controlled for in the case-control analysis and in the case series analysis. Analyses Case-control Conditional logistic regression will be used to undertake matched case-control analyses. We will initially undertake a series of univariate analyses. Factors that appear to be associated with autism (P < 0.2) will be carried forward to a multivariate model. Likelihood ratio tests will be used for all tests of significance. Two analyses will be carried out. The first will estimate the odds ratio for the development of symptoms in specific time periods after vaccination with MMR. This method provides an alternative approach to the case series approach outlined below. The second will assess exposure to MMR vaccine at any time prior to symptom development. This analysis differs from the case series approach in that no assumption is required about the likely interval from vaccination to disease onset if there is a causal association. We will examine the effects of the age matching: comparing the results for those children very closely matched on age (for example within 6 months) with the results for any children less well matched on age. The two control groups will be analysed separately. If the odds ratios differ substantially, this will indicate that practice was an important confounding factor (i.e., that some practices were better at diagnosing autism and also had a higher vaccine coverage). The results for the two groups will then be reported separately, but we will consider the correct result to be that from the practice matched group. If the results for the two groups are similar, they will be pooled. In this situation it is possible we may have "over-matched" in the practice matched group, but this will only lead to a loss of power, not to a bias in the estimate. Case series The case series uses data on affected children only to estimate the relative incidence of clinical events either in a defined interval after vaccination compared to time periods outside this defined interval, or at any time after vaccination compared with the time period before vaccination [40,41]. The method has been used to estimate the relative incidence of febrile convulsions following DTP and MMR vaccines [42] and was also used in a recent study of the onset of autism following MMR vaccine [18]. We will examine periods of 1 month, 2 months, 4 months, 6 months and 1 and 2 years after vaccination. The reference period for each individual will consist of every month from birth up until February 1998, which was when the possible link between MMR vaccine and autism became widely known, excluding the post-vaccination period being studied. All analyses will be finely stratified for age, the exact stratification will depend on the age distribution of the affected children. The two approaches estimate different parameters. The case-control approach will estimate the odds ratio for whether children who are vaccinated have an increased chance overall of developing autism than children who are not vaccinated. The case series will estimate the relative incidence of autism in the period following MMR vaccination. Power We estimate we will be able to include a minimum of 400 children with a diagnosis of autism in the analyses. Over the entire study period we estimate the proportion of children in the control group who will have received MMR vaccination to be around 85% [11]. With 5 controls per affected child in the case-control analysis we will be able to detect the following minimum odds ratios for the association between autism and MMR vaccination with 90% power at the 5% significance level: 1.8 if average MMR coverage among controls is 85%, or 2.0 if average MMR coverage among controls is 90%. For the case series analysis assuming an 85% vaccine coverage rate (a conservative estimate), we will have 90% power at the 5% significance level to detect a minimum relative incidence for autism of 1.6 in the 1 month following MMR vaccine. Ethical approval The Scientific and Ethical Advisory Group is a central ethical committee specially set up by the Department of Health to oversee use of the GPRD. They have approved the study, subject to approval of the questionnaire, as have the ethics committee of the London School of Hygiene and Tropical Medicine. The use of confidential patient data in this study is fully within the recent guidelines from both the United Kingdom Medical Research Council [43] and the General Medical Council [44] about the use of personal information in medical research. Discussion Electronic databases offer several important advantages for epidemiological studies of adverse events from vaccination. All people affected by the adverse event (or a random sample) can be drawn from existing records, usually avoiding the problem of ascertainment being linked to exposure, although bias may not entirely be removed if people affected were diagnosed after the hypothesis was known. As controls can be sampled from all other participants in the database, biased selection of controls is less likely to occur. Records of date of vaccination and onset of symptoms, are also less likely to be biased, in particular if they precede the hypothesis coming into public domain. The major disadvantage of such databases is that data quality and completeness may not always be optimal. In particular, all diagnoses of autism will not have been made using the same criteria applied in a consistent manner. Vaccines are without doubt among the most effective public health interventions, but thorough investigation of suspected adverse effects is necessary. Case-control studies using electronic health databases offer a uniquely efficient method for evaluating adverse effects of vaccines. However they also offer scope for bias and confounding to produce misleading results. The rigorous validation of all possible diagnoses and the collection of additional information by parental questionnaire in this study will be both time consuming and expensive, but we view this as essential to minimise the possibility of biased results. Pre-publication history The pre-publication history for this paper can be accessed here:
Background In Brazil coronary heart disease (CHD) constitutes the most important cause of death in both sexes in all the regions of the country and interestingly, the difference between the sexes in the CHD mortality rates is one of the smallest in the world because of high rates among women. Since a question has been raised about whether or how the incidence of several CHD risk factors differs between the sexes in Brazil the prevalence of various risk factors for CHD such as high blood cholesterol, diabetes mellitus, hypertension, obesity, sedentary lifestyle and cigarette smoking was compared between the sexes in a Brazilian population; also the relationships between blood cholesterol and the other risk factors were evaluated. Results The population presented high frequencies of all the risk factors evaluated. High blood cholesterol (CHOL) and hypertension were more prevalent among women as compared to men. Hypertension, diabetes and smoking showed equal or higher prevalence in women in pre-menopausal ages as compared to men. Obesity and physical inactivity were equally prevalent in both sexes respectively in the postmenopausal age group and at all ages. CHOL was associated with BMI, sex, age, hypertension and physical inactivity. Conclusions In this population the high prevalence of the CHD risk factors indicated that there is an urgent need for its control; the higher or equal prevalences of several risk factors in women could in part explain the high rates of mortality from CHD in females as compared to males. Background Men and women share some coronary heart disease (CHD) risk factors such as age, dyslipidemia, hypertension, smoking, diabetes, obesity and physical inactivity. Besides that, women have additional risk factors, such as the use of contraceptives and the reduction of ovarian function with age [1]. There are important differences in the clinical manifestations of CHD between the sexes [1-7]. Besides that CHD was always considered a male problem, partly because of the 7-10 years time lag before its clinical appearance in women due to hormone differences and partly because of its higher incidence in men (although the difference decreases in postmenopausal women). After the sixties, with the entry of women into the labor market and exposure to stress, habitual smoking and fast food diets, their CHD mortality rates quickly increased. Recent studies have emphasized this increase and the fact that CHD is the leading cause of death among women [1-8]. Besides this, in the last 2 to 3 decades, the decrease in cardiovascular mortality rates as well as in the incidence of risk factors has been shown to be larger in men than in women [9]. In Brazil, the CHD mortality rate in women jumped from 10 to 25% from the sixties to the seventies [10-12] and now CHD constitutes the most important cause of death in both sexes and in all the regions of the country. Interestingly, in Brazil the difference between the sexes in the CHD mortality rates is one of the smallest in the world because of high rates among women [13]. Since a question has been raised about whether or how the incidence of several CHD risk factors differs between the sexes in Brazil, we evaluated data on the occurrence of these risk factors in a population from the city of Campinas, State of So Paulo. We also measured the influence of the CHD risk factors on blood cholesterol. Materials and Methods The study comprised eight hundred and seventy-three individuals who volunteered to have their total blood cholesterol (CHOL) measured and to fill in a questionnaire with information on anthropometric data and on the presence of the following risk factors for the development of CHD: diabetes mellitus (DM), hypertension (HY), obesity (OBES), sedentary lifestyle (SEDEN) and cigarette smoking (SMOK). The data were collected in four different places of the city of Campinas, So Paulo, representing different social-economic populations - the City Government building, a branch of the Bank of Brazil, the State University of Campinas and a large Shopping Center - over a seven-day period, in november 1997. The volunteers were adults, from 20 to 82 years of age (y), 53% males (M) and 47% females (W), 6% blacks and 94% non-blacks mainly "Hispanics", similar frequencies described for the country and the state of So Paulo according to the 1998 census data. Their CHOL was measured in fingertip capillary blood by an enzymatic method (CHOD-PAP) through reflectance photometry utilizing the Reflotron (Roche Diagnostics, Indianapolis, IN, USA). Body mass indexes (BMI) were calculated from the informed height and weight (kg)/height(m2) and used as a marker of excessive weight and obesity when equal to or above 25 kg/m2. The data were analyzed by the SAS statistical package utilizing the tests Student's "t", Chi-Square with or without Fleiss's method [14] and Fisher's exact, all at the significance level of 5%. Comparisons were made between the sexes in the total population and at different age groups and between the age sub-groups in each sex. The cut off ages for CHD risk were established as:  45 versus <45 y for men, and  55 versus <55 y for women, according to the United States National Cholesterol Expert Panel" (NCEP) [15] recommendations followed by the "2 Consenso Brasileiro de Dislipidemias" (Brazilian Consensus on Dyslipidemia) [16]. A multiple logistic regression analysis was used to assess the influence of CHD risk factors on blood cholesterol concentration. The independent predictors of blood cholesterol were sex (men/women); age - all ages and the age group:  50 versus <50 y; BMI  25 versus <25 kg/m2; race (blacks versus non-blacks) and the binary criteria (yes/no) of diabetes, hypertension, sedentary lifestyle and cigarette smoking. The dependable variable was defined as CHOL<5.18-desirable levels, 5.18-6.19-borderline risk for CHD and  6.22 mmol/L- high risk for CHD, according to NCEP recommendations followed by the "2 Consenso Brasileiro de Dislipidemias" (Brazilian Consensus on Dyslipidemia) [15,16]. In this case, since we have three levels of cholesterol, a proportional odds model was used in the logistic regression. Results In this population women had a trend toward higher ages than men did (p = 0.052), as seen in Table 1. The BMI for the whole population is in the limit range for obesity grade I (24.97  3.80 Kg/m2). In men it was significantly higher than in women but the average difference between the sexes was only 3% (Table 1). Table 1 Blood cholesterol and anthropometric data among men and women and in the total population. Data presented as mean SD, interval, (n = number of subjects) VARIABLE MEN WOMEN TOTAL CHOL 4.92  1.09 a 5.10  1.22 a 5.00+1.22 (mmol/L) 2.33-9.04 2.67-9.14 2.33-9.14 (459) (410) (869) AGE 46  15 50  15 47+15 (y) 20-82 20-81 20-82 (462) (411) (873) BMI 25.31 + 3.50b 24.59  4.00 b 24.97+3.80 (kg/m2) 15.6-41.1 16.7-38.9 15.6-41.1 (454) (403) (857) Student's "t" test; axa: p = 0.033, bxb: p = 0.005 The population average CHOL (mean  SD) was 5.00  1.22 mmol/L. Women showed statistically significant higher CHOL when compared to men, although the difference was small, around 3.5% (Table 1). This sex difference was found in the CHD risk age groups: 5.75  1.14 (age  55 y, women) versus 5.23  1.19 mmol/L (age  45 y, men, "t" test, p = 0.001). There was no difference in the younger age groups: 4.66  1.09 (age 20-54 y, women) versus 4.64  1.11 mmol/L (age 20-44 y, men, "t" test, p = 0.630). Postmenopausal women presented higher CHOL levels when compared with younger women: 5.75  1.14 versus 4.66  1.09 mmol/L (p = 0.001) and in men CHOL also increased with age: 5.23  1.19 versus 4.64  1.11 mmol/L (p = 0.001) as expected. Table 2 shows that CHOL was higher than the recommended values for adults in 41% of the population. We found fewer women with CHOL in the reference range than men (54 versus 63%). This statistically significant difference was due to the sub-population of women in the age group equal to and above 55 y (p= 0.001, Chi-square as expected [4,7,17]. Table 2 Percentage distribution of different levels of total blood cholesterol between the sexes by age GROUP AGE <5.18 5.18-6.19 6.22 (years) (mmol/L) (mmol/L) (mmol/L) MEN (462) 63 (290)a 23 (106) 14 (66) <45 61 40 32 45 39 b 60 68 WOMEN (411) 54 (222)a 28(114) 18 (15) <55 80 45 31 55 20 b 55 69 TOTAL (873) 59 (512) 25 (220) 16 (141) ()=number of subjects; Chi-square and Fleiss tests : axa: p = 0.030; bxb: p = 0.001 As seen in Table 3, obesity (44%) and sedentary lifestyle (48%) were the most common risk factors in this population and diabetes the least common (4%, as expected), but the data do not differ from those found in developed countries. Hypertension and smoking habit had practically the same frequencies around 20%. Among men, in decreasing order of frequencies, the data showed: obesity, sedentary lifestyle, smoking, hypertension and diabetes. Among women, however, the decreasing sequence was different: sedentary lifestyle, obesity, hypertension, smoking and diabetes. Table 3 Prevalence (%) of risk factors for coronary heart disease among men and women and in the total population GROUP DM HY OBES SEDEN SMOK MEN 3 (460) 19a(456) 48 b(462) 46 (461) 24 c(461) WOMEN 4 (411) 27a(410) 39 b(411) 52 (410) 18 c(410) TOTAL 4 (871) 23 (866) 44 (873) 49 (871) 21 (871) ()=number of subjects; Chi-Square test; axa: p = 0.003,bxb: p = 0.009; cxc: p = 0.029 DM-diabetes; HY- hypertension; OBES- obesity; SEDEN- physical inactivity; SMOK- smoking The comparison between the sexes showed that there were significant statistical differences in the frequencies of hypertension (1.4 times higher in women), obesity and smoking (respectively 1.2 and 1.3 times higher in men). Physical inactivity, the most frequent risk factor in this population, and diabetes were similar in the sexes. Table 4 shows age distribution of risk factors in both sexes. Diabetes was not present in younger men. Its prevalence did not differ between the group ages in women. In the older groups, no differences were found between the sexes. Women presented higher frequencies of hypertension than men did in both age groups, indeed twice as high in the younger group (8X18%). In both sexes the prevalence of hypertension was 2 to 3.5 times higher in the older ages as compared to the younger. The distribution of obesity between the age groups was not different in men, but in women the frequency was higher in older ages. In men the prevalence was higher than in women only in the younger group and no differences were found among the older groups. Physical inactivity was the only risk factor similar between the sexes in both age groups and equally frequent in the two age groups. The higher prevalence of smoking in men was found only in the older group. It was high in the younger groups and there were no differences between the sexes in those age groups (29 versus 26%). Younger women smoked 4 times more than the older ones and younger men 1.6 times more. Table 4 Prevalence (%) of risk factors for coronary heart disease according to sex and age GROUP AGE(y) DM HY OBES SEDEN SMOK MEN <45 0 a 8 b 46c 42 29 d (239) (238) (241) (240) (240) 45 7e 30f 51 50 18 g (221) (218) (221) (221) (221) WOMEN <55 3 h 18 i 33 j 50 26 k (251) (251) (251) (250) (250) 55 6l 41m 50n 56 6 (160) (159) (160) (160) (160) Statistical comparisons by the Fischer test; axe: p = 0.001, axh: p = 0.007; by the Chi-Square test; bxf, ixm,jxn, kxo, gxo: p = 0.001; dxg: p = 0.005; bxi, cxj: p = 0.020; fxm = 0.019 DM-diabetes; HY-hypertension; OBES-obesity; SEDEN-physical inactivity; SMOK-smoking Race, presence of diabetes and smoking habit did not have any significant effects in CHOL in this population as shown by the logistic univariate regression analysis. According to the multiple regression analysis, sex, age and BMI, the most significant effects to the model, did influence CHOL and were associated with hypertension and physical inactivity; because of the high association with BMI, they were not significant to the model in the presence of this variable. In the final model, age and sex presented a very significant interaction at all ages. Also BMI and sex interacted significantly at and after age 50 y. Table 5 presents the results of the logistic regression analysis with the selected test variables associated with CHOL-sex, age, BMI- and their respective logistic coefficients, SE, p-value, odds ratios and confidence intervals in the total population. In Table 6 the same associations between variables for the cut off age of 50 y are shown. Table 5 Influence of coronary heart disease risk factors on blood cholesterol in the total population VARIABLE LOGISTIC SE p-VALUE ODDS RATIO a 95% CI COEFFICIENT SEX 1.655 0.534 0.002 5.232 1.839; 14.885 AGEb,c 0.075 0.008 0.009 1.078 1.061; 1.095 BMI 0.208 0.141 0.141 1.231 0.933; 1.624 AGE*SEX -0.039 0.010 0.009 0.962 0.943; 0.981 a = odds ratio = esum of logistic coefficients X age; CHOL above 6.22 mmol/L; b = all ages; c = <25 and  25 kg/m2; CI = confidence interval Table 6 Influence of coronary heart disease risk factors on blood cholesterol by age VARIABLES LOGISTIC SE p-VALUE ODDS RATIO a 95% CI COEFFICIENT SEX 0.195 0.211 0.355 1.215 0.804; 1.836 AGE 50 b 2.074 0.243 0.009 7.961 4.946; 12.813 BMI c 0.576 0.207 0.005 1.778 1.185; 2.668 AGE 50*SEX - 0.953 0.285 0.001 0.386 0.221; 0.674 AGE 50*BMI -0.551 0.281 0.050 0.576 0.332; 1.000 a = odds ratio = esum of logistic coefficients; CHOL above 6.22 mmol/L; b = by age group (<50 and  50 y); c = <25 and  25 kg/m2; CI = confidence interval Figure 1 summarizes the changes of prevalence of CHOL above 6.22 mmol/L according to sex, age (all ages up to 80 y) and BMI. Figure 1 Probability of CHOL > 6.22 mmol/dL by age, sex and BMI. Discussion The literature shows that there are differences between the sexes in the prevalence and impact of CHD risk factors [2,9,18]. In the U.S.A. for example, some risk factors have high prevalence among women in the 20 to 74 year age group [19]: more than 1/3 have hypertension, more than 1/4 are hypercholesterolemic, more than 1/4 present excessive weight and more than 1/4 are sedentary. Furthermore, 51% of white women and 79% of black women above 45 y have hypertension. The same is true for 71% of all women over 65 y [3]. Hypertension is more frequent among women over 65 y than among men at that age [20]. Diabetes has a prevalence of 7.7% [21]. In Brazil, the studies indicate the following prevalence [12]: hypercholesterolemia not determined [21], but more recently described in one study as 41% [22]; hypertension equal to 20-30% [23], excessive weight and obesity, 53% [21], physical inactivity, 74% [21], diabetes, 7% [21] and smoking habit, 63% [21]. The data presented here confirm the fact that the presence of CHD risk factors is a major health problem in women in Brazil (Table 3). Our findings of a lower prevalence of diabetes (4%), excessive weight and obesity (44%), physical inactivity (49%) and smoking (21%) in this study can be partially related to the fact that the information was self-reported although high CHOL and hypertension frequencies are the same as the ones described in the literature [22,23]. It is important to emphasize that two of the major risk factors for CHD, high CHOL (Table 2) and hypertension (Table 3) were more prevalent among women as compared to men. High CHOL was present in the sub-population of women in the postmenopausal age, as expected, but pre-menopausal women in this population presented equal levels of CHOL when compared to men suggesting an additional risk at pre-menopausal ages. Several studies demonstrate that the HDL-cholesterol level is a better predictor of mortality from CHD in women while the LDL-cholesterol level is a better predictive in men [2,3]. We did not fractionate lipoproteins in this study, but since hypercholesterolemia is one of the major well established risk factors for CHD in both sexes [1,4], we can speculate that this could account for a possible higher risk for CHD in women in this population. Postmenopausal women tend to present a much higher chance of hypercholesterolemia due to elevated LDL- cholesterol levels [17]. In Table 4 it is seen that the higher frequency of hypertension was present in both age groups. The higher frequency in postmenopausal women is in accord with the literature [24] but hypertension was also two times higher in younger women as compared to men from ages 20 to 44 y. Again, younger women in this population are more exposed to this major risk factor. Hypertension is a well-established risk factor in both sexes. On the other hand, women have more complications from hypertension than men do. A more recent work has shown a strong association between CHD and hypertension in women [20]. The prevalence of diabetes was not different between the sexes (Table 3), but diabetes was not present in the younger group of men in this study, only in the women's group, indicating the presence of another very relevant risk factor in younger women. No differences were found between the sexes in the older groups (Table 4). Diabetic women lose their "relative immunity" to CHD in relation to men: female diabetics are two times as likely to die from CHD than are male diabetics, so the same or a higher prevalence of diabetes among the sexes found in this study is quite unfavorable to women [7]. The prevalence of sedentary lifestyle was not different between men and women, but showed a small trend to higher frequencies in women (Table 4). Physical inactivity increases CHD in both sexes [8]. Regular physical exercise favors women more than men, with respect to CHD [25]. Men presented a higher prevalence of obesity than women did only in the younger group, but there were no differences in the older age groups. The prevalence increased with age only in women aggregating another risk factor to the postmenopausal period. Obesity was found to be an independent predictor of CHD in both men and women [26,27]. The distribution of fat is considered to be more important in both sexes than the overall degree of obesity, the centripetal distribution being a component of the atherogenic syndrome X [9,24,28]. Smoking, very prevalent in the younger groups, had higher incidence in men (Tables. 3, 4). There were no differences between the sexes in the younger age groups leading to another undesirable risk at younger ages in women. Tobacco use triples the risk of heart attacks among women, even during the pre-menopause period [3]; also, smoking women who are diabetic double their risk of death in relation to diabetic non-smokers [2]. Men are giving up tobacco at an increasing rate, which is not the case among women [21]. Also smokers have their first heart attack earlier than non-smokers: women 19 and men 7 years earlier [2]. The logistic analysis indicates that race, presence of diabetes and smoking habit did not contribute to CHOL in this population. It is probable that these risk factors do not operate through high CHOL. On the other hand, sex, age, BMI, as described earlier in other studies [4,29], hypertension and physical inactivity were the risk factors that influenced CHOL. Since sex and age are not modifiable, the control of obesity should be seriously implemented in Brazil. Women had a higher risk than men of having CHOL >6.22 mmol/L as their age increased (Table 5). These findings are in accord with the literature [4,7] and in fact the population CHOL seemed similar to data obtained from an N.I.H. report [31]. From Table 5 it can be observed that the chance of having CHOL  6.22 mmol/L was 23% higher in individuals presenting overweight and obesity. A more detailed analysis of these effects of sex, age and BMI on CHOL is seen in Figure 1. The prevalence of high CHOL was higher in post-menopausal women as compared to men, and it increased for both with age. The sex difference was present at all ages (up to 80 y), men having a higher prevalence in younger ages. After age 60 y, women presented a much higher frequency than men. The curves crossed around the age of 42.5 y (close to the age of menopause, 47.5 y as determined in the State of So Paulo [31]) for either overweight and obese individuals or not. The higher BMI is associated with a higher prevalence of CHOL. Analyzing Table 6, in women at and above 50 y the risk of having CHOL >6.22 mmol/L was 2 times higher than for men; before 50 y men had a 20% higher chance than women; at and above age 50 y the chance of high CHOL for BMI<25 is 1.02 as compared to BMI  25; below 50 y the chance of high CHOL for BMI  25 is 1.8 higher than for BMI below 25, showing that factors related to obesity are much less influential in older ages, a period when the interaction age/sex predominated. The results of this study demonstrate the need for the prevention of atherosclerosis by controlling the CHD risk factors in Brazil. This is shown by the high frequencies of a conglomerate of these risk factors such as high CHOL, obesity, sedentary lifestyle, hypertension, diabetes and smoking in this population. Very important also is the fact that hypertension, diabetes and smoking were highly prevalent among younger women, the two first having higher frequencies than among men. These results could partially explain the small difference in the mortality rates from CHD between the sexes in Brazil [12]. Pre-publication history The pre-publication history for this paper can be accessed here:
Background Good prescribing practice has an important part to play in the fight against antimicrobial resistance. Whilst it was perceived that most hospitals and Health Authorities possessed an antibiotic policy, a review of antibiotic policies was conducted to gain an understanding of the extent, quality and usefulness of these policies. Methods Letters were sent to pharmacists in hospitals and health authorities in across the South East region of the National Health Service Executive (NHSE) requesting antibiotic policies. data were extracted from the policies to assess four areas; antibiotic specific, condition specific, patient specific issues and underpinning evidence. Results Of a possible 41 hospital trusts and 14 health authorities, 33 trusts and 9 health authorities (HAs) provided policies. Both trust and HA policies had a median publication date of 1998 (trust range 1993-99, HA 1994-99). Eleven policies were undated. The majority of policies had no supporting references for the statements made. All policies provided some details on specific antibiotics. Gentamicin and ciprofloxacin were the preferred aminoglycoside and quinolone respectively with cephalosporins being represented by cefuroxime or cefotaxime in trusts and cephradine or cephalexin in HAs. 26 trusts provided advice on surgical prophylaxis, 17 had meningococcal prophylaxis policies and 11 covered methicillin resistant Staphylococcus aureus (MRSA). There was little information for certain groups such as neonates or children, the pregnant or the elderly. Conclusion There was considerable variation in content and quality across policies, a clear lack of an evidence base and a need to revise policies in line with current recommendations. Background In the United Kingdom, concerns around resistance to antibiotics have been expressed for some time, leading to a House of Lords select committee report [1], a report from the Standing Medical Advisory Committee (SMAC) [2] and a subsequent Health Service Circular [3] setting out a course of action for the National Health Service. The Health Service Circular (HSC) was based on four elements of strategy, surveillance, prudent antibiotic use, and infection control. These concerns focussed on the need to treat patients on the one hand but not to so compromise the environment that there is no effective treatment for certain infections. In the HSC, Regional Directors of Public Health were tasked to lead the work addressing the problem of antimicrobial resistance with colleagues who were the Regional Epidemiologists and the Regional Prescribing Leads. Regional Prescribing Leads were asked 'to identify good prescribing practice in the use of antimicrobials' and to ensure that information in widely disseminated among those who are involved in prescribing antimicrobials. This project was undertaken to review one of the long-standing measures for encouraging good practice and controlling antibiotic use: the antibiotic policy, i.e. guidance or instruction on when and how antimicrobial drugs are to be used [1]. The primary aim was to gain insight into the use of antibiotic policies as a mechanism to encourage good, evidence-based, practice and facilitate the appropriate use of antimicrobials. The advice provided in the policies was assessed against recognised best practice [4]. Methods The South East region of the NHSE had 14 health authorities within which there were a total of 41 NHS trusts providing hospital and/or community care. Letters were sent in June 1999 to the Chief Pharmacists of the NHS trusts and the Pharmaceutical Advisers of the Health Authorities requesting their antibiotic policies. Those which did not respond were followed up by up to two telephone calls. The authors selected four sets of features for the assessment of the policies based around the current guidance [1,2,3]: antibiotic specific issues, a set of conditions to which policies might apply, a set of patient groups who might have special mention in a policy such as neonates or pregnant women. Reference to underpinning evidence for the statements made was sought. The authors formed an opinion of the presentation and user-friendliness of each policy from how easy it was for them, coming new to each policy, to find information. Each set of features was broken down into a series of specific items of advice or information that users might seek in the policies. These items were considered to be important enough to be included in the policies. The data were extracted by one author (PJW) and checked by the other (RTMW). Discrepancies were discussed and a consensus reached. Anonymised data for all responses was sent back to Trusts and Health Authorities with a code enabling them to identify their own data. Perceived errors in data extraction were reported back and discussed with the authors. A workshop was held in January 2000 for microbiologists and pharmacists in the region to discuss the findings of this work and the development of future policies. Results Thirty-three trusts and nine health authorities provided their policies. There were two groups of trusts (one group with two trusts and one with three) that shared policies, so there were 39 policies to review. Eight trusts did not provide a policy, one teaching trust provided two policies covering three sites, and one HA provided policies for three trusts which were planning to merge. One Mental Health Trust uses its HA policy which, like other HA policies, was written for patients outside acute hospitals. The trust policies were focussed on the needs of patients in acute hospitals. The trusts and health authorities that sent in their antibiotic policies were distributed across the whole region. The region had been created in 1999 from parts of three other NHSE regions (Anglia-Oxford, South Thames and South-West). There was no association of non-respondents with their former NHS regions. As far as the authors could determine, the non-respondents did not have current antibiotic policies, rather than refusing to submit their policies to independent review. The policies provided had a wide range of presentation styles ranging from an interactive computer programme to a single folded card. The most popular formats were A4 (11 policies), pocket size, including filofax (12 policies) or A5 (11 policies) the remainder being in mixed format or unusual sizes. One trust (the Winchester & Eastleigh NHS Trust) used an electronic format with its policy on the hospital intranet, from which a complex set of printouts, based on a decision analysis process, were supplied for the review. Trust policies had a median publication date of 1998 (range 1993-99) and Health Authority policies also had a median publication date of 1998 (range 94-99). Policies from eight Trusts and three Health Authorities were undated. The level of information in Trust policies suggested that most policies were written for junior hospital doctors but this was not generally explicit. Policies from Health Authorities were intended for general practitioners. It was not clear if most policies were advisory or mandatory; the only three policies to include a statement on this said that they were advisory. Fourteen of the trust policies and four of the Health Authority policies provided at least one contact telephone number for advice. Only two of policies contained references to support their statements (one reference each). At the workshop to discuss the review, the comment was made that references might make the policies to large to carry in the pocket. Antibiotic Specific Issues All policies provided some details on specific antibiotics and the majority (34 out of 41) provided some advice on length of treatment, although this advice was sparse in five of the 34. Fifteen Trusts and five Health Authorities policies provided information on side effects, and contraindications but the advice was often not clearly expressed, lacking in detail and specificity. In terms of the drugs of choice, gentamicin was the favoured aminoglycoside where mentioned and ciprofloxacin the favoured quinolone. The cephalosporin group showed a range with cefuroxime and cefotaxime being more popular than cephalexin and ceftriaxone in trusts. Cephradine and/or cephalexin (both oral) were recommended by eight out of nine Health Authorities in their guidance for general practice. Conditions Specific Policies All the Trust policies and most (eight out of nine)of the Health Authority policies recommended treatment for specific conditions. Trust policies Prophylaxis Twenty-seven Trust policies provided advice on surgical prophylaxis policies. There was wide variation in the number of regimens (Table 1), from a trust with one recommendation to two Trusts had twenty prophylaxis regimens each specific for different types of surgery. Five Trusts did not have a stated policy despite having surgical departments. Table 1 Numbers of surgical prophylactic regimens in NHS trusts Numbers of different regimens recommended in the trust Number of trusts 1- 5 4 5-10 9 11-15 8 16-20 3 Each department has own regimen 3 No stated policy 5 Total 32 (a psychiatric hospital was excluded from this analysis) Meningococcal prophylaxis was in the policies of 17 Trusts, and the remaining Trusts either have no policy or referred the reader to the British National Formulary. Fifteen Trusts did not have an explicit policy for antibiotic prophylaxis after splenectomy and three Trusts did not recommend a treatment period for post splenectomy antibiotic prophylaxis. Lower respiratory tract infections The majority of Trusts favoured amoxicillin for bronchitis, either as the only recommendation (11 trusts) or as one of two or three choices (13 trusts). Erythromycin, trimethoprim, doxycyline, tetracycline and benzylpenicillin were also recommended. Seven Trusts have no stated policy for treating bronchitis (Table 2). Amoxicillin or ampicillin was most frequently recommended as a treatment for pneumonia, followed by the cephalosporins and benzyl penicillin (table 3). Seven other antibiotics appeared one or more times. Table 2 Antibiotics recommended for bronchitis Antibiotic Hospital Trusts Health Authorities Amoxicillin alone 12 2 Amoxicillin with alternatives 13 5 Co-amoxiclav (amoxicillin with 1 0 clavulanic acid) or tetracycline No policy 7 2 Total 33 9 Table 3 Antibiotics recommended for pneumonia Antibiotic Hospital Trusts Health Authorities Ampicillin/Amoxicillin alone 3 0 Amoxicillin and/or erythromycin 5 7 Ampicillin/Amoxicillin with other 5 1 alternatives Benzylpenicillin 8 0 Cephalosporins 10 0 Co-amoxiclav 2 0 No policy 0 1 Total 33 9 Urinary Tract Infections Trimethoprim was recommended for cystitis in twenty-eight Trusts (Table 4), but only fifteen restricted the choice to trimethoprim or another specific urinary antimicrobial (nitrofurantoin). Nitrofurantoin was given as an alternative to trimethoprim by six Trusts. Three trusts gave cephalosporins as the first line treatment for cystitis and 13 suggested broad spectrum antibiotics as a alternative to trimethoprim. One policy said that antibiotics were not normally indicated for cystitis. Table 4 Antibiotics recommended for cystitis Antibiotic Hospital Trusts Health Authorities Trimethoprim 13 2 Trimethoprim or nitrofurantoin 2 1 Trimethoprim or a cephalosporin 7 4 Trimethoprim or another broad 6 1 spectrum Cephalosporins first 3 0 Antibiotic not indicated 1 0 No policy 1 1 Total 33 9 Only 18 Trust policies made recommendations on treating pyelonephritis and severe urinary tract infections. Injectable cephalosporins were the preferred treatment. Amoxicillin, ciprofloxacin and gentamicin were also mentioned. Diarrhoeal Diseases In the policies which covered gastro-enteritis, it was recommended in 17 trusts that no antibiotic treatment be given for most patients, whether there was a known pathogen or not. Ciprofloxacin was listed a treatment for more severe cases in two Trust policies. Twelve Trust policies did not cover gastro-enteritis. For the treatment of C. difficile infection, 14 Trusts advised metronidazole, three advised oral vancomycin and three said that either were suitable. MRSA Eleven policies cover the issue of MRSA. The advice ranged from a brief mention to a lengthy document covering a wide range of issues about the treatment of carriers and infected patients Health Authority Policies Prophylaxis Meningococcal prophylaxis (rifampicin for two days) was described by four of the nine Health Authorities. A fifth health authority recommended contacting the Consultant in Communicable Disease Control (CCDC), i.e. the public health physician responsible for controlling infection in the area covered by the health authority. Only one Health Authority mentioned splenectomy prophylaxis, recommending penicillin or erythromycin for at least two years. Lower respiratory tract infections Amoxicillin was most frequently recommended drug for bronchitis (Table 2) with alternatives as cefaclor, tetracyclines or erythromycin. For pneumonia treated by general practitioners, amoxicillin and erythromycin were treatments most frequently mentioned (Table 3). Urinary Tract Infections For cystitis, all Health Authorities favoured trimethoprim with alternatives as nitrofurantoin, cephalosprorin or co-amoxiclav (amoxicillin plus clavulanic acid). Similar treatments were recommended for pyelonephritis, but one Health Authority recommended ciprofloxacin as the first choice (Table 5). Table 5 Antibiotics recommended for pyelonephritis Antibiotic Hospital Trusts Health Authorities Ampicillin/Amoxicillin plus another 3 0 antibiotic Cephalosporins 14 0 Trimethoprim or other antibiotic 0 4 Ciprofloxacin 1 1 No specific policy 15 4 Diarrhoeal Diseases All Health Authorities that included a section that stated antibiotics were not generally recommended. The five Health Authorities that mentioned C. difficile favoured metronidazole. Patient Specific Policies Generally, there were no policies for neonates or children although some policies included a few neonatal or paediatric doses. Two Trusts did provide guidelines for those groups. There were no policies for the elderly and little advice on antibiotics in pregnancy. Workshop Issues The main feature of the workshop was to discuss key elements of antibiotic policy production and maintenance. There was a perception of a lack of quality evidence to underpin policy decisions and implementation with a concern that an evidence base be worked up centrally to the benefit of all. There was consensus that the grade of evidence supporting a particular statement be indicated in the policy document. Updating policies was considered an important task with a full review at approximately two-year intervals with policies having a clear issue date and perhaps an expiry date. Concerns were expressed about the general lack of local audit underpinning policy implementation. Discussion around surgical prophylaxis raised a number of issues including lack of evidence, an appearance of defensive practice with regimes for one type of surgery being extrapolated to other surgical interventions and the need to stimulate multicentre trials to answer questions around effectiveness. In terms of ownership and target audience for policy there was a clear steer to bring this activity into the clinical governance agenda. It was agreed that authors of policies should be responsible to Drugs and Therapeutics committees in Trusts. The group considering the content of policies had a majority in favour of a mandatory status for antibiotic policies. A full workshop report can be found as an Appendix, see additional file 1: Appendix. Discussion The stimulus for this work was the Standing Medical Advisory Committee (SMAC) [2] report and the linked Health Service Circular [4]. The SMAC report states that antimicrobial guidelines should be: Evidence based; State the date the document was created/revised; Contain information on antimicrobial, dose, frequency, and length of course; Indicate strength of the evidence for the recommendation; Show local variation from national guidelines. It was clear that some policies were the products of much careful preparation, and could be excellent examples for others who have no policies or are updating policies. The variety in the presentations of antibiotic policies came as a surprise. Some were clearly designed for ease of use, with good quality printing, contents lists and pocket sized. Some on a single sheet of A4 paper seemed to be almost too simple. Others were larger, where the policies were part of a hospital formulary or a compendium of advice for hospital doctors. In some of the larger documents, the antibiotic policy was presented in different sections referring to different specialities. The authors tried to put themselves in the position of doctors starting at the hospital trust or in general practice, and we preferred the pocket sized formats dedicated to antibiotics. The computer-based policy used in Winchester could not be compared with the paper-based policies used elsewhere, although it appeared to lie more at the large and complex end of the spectrum of styles. The trusts that were sharing policies had done so because of mergers, either of the whole trusts or their microbiology departments. In three health districts, there was a relationship between the trust policies and the guidance given to GPs. Greater collaboration between trusts, and with health authorities would enable them to reduce the labour in producing better policies and to lessen the confusion for staff who move between hospitals. This view was also clearly stated during the workshop with the construction of the evidence base seen as an area requiring collaboration at least at regional level and possibly nationally. Several trusts were in the process of updating their policies during this study, and some have subsequently submitted their latest drafts, this is reflected in the tables. It is important that the date that a policy is written should be prominent on the front cover, to avoid out-dated policies being used. Some trusts and health authorities regularly update their policies and this should be encouraged as good practice. Although some of the policies were conceived or written before the current emphasis on evidence-based practice, we were surprised that there were no policies which statements that were backed with references. Examples of easily available reviews are the guidance of meningococcal treatment and prophylaxis [5], and prophylaxis for asplenic patients [6]. These would have provided junior doctors with more detailed information to supplement a pocket sized antibiotic policy. Given the wide possibilities for surgical prophylaxis, supporting references would be a valuable addition to the text. An evidence base needs to be developed for surgical prophylaxis that covers the vast number of potential permutations when considering type of surgical intervention, choice of antibiotic, timing and number of doses among others. It was expected that all policies would cover the common infections of the urinary and lower respiratory tracts, and that there would be guidance on the treatment of gastro-enteritis. Other infections, which were not reviewed in detail, but covered by most policies, were pharyngeal and skin infections. Most policies gave guidance on treating meningitis, but it is surprising that only approximately 50% of the policies gave useful instructions on prophylaxis for the contacts of meningitis. It is also surprising that there were some trust policies that did not cover surgical prophylaxis and it is disappointing that post-splenectomy prophylaxis was infrequently described. The wide publicity on MRSA does not seem to have greatly affected antibiotic policies as less than a half included any statement on the subject. At the workshop following the survey, it was pointed out that trusts may have sections on MRSA in their infection control policies. MRSA policies should be explicitly mentioned in trust antibiotic policies with appropriate cross-references to other policies. It might be argued that because antibiotics are generally safe, dosage related to age is not an important issue. However two examples where this is important concerns ciprofloxacin which is contra-indicated below 12 years of age and in pregnancy; also aminoglycoside dosage is affected by age. It was evident that some policies had been prepared with a premium placed on brevity. Nevertheless, it appears that the policies should give advice on contra-indications and side effects, and on dosages appropriate to very young children and the elderly. If policies are to be useful, they should aim to cover all the important prescribing points. One factor that merits further consideration is whether policies should be mandatory or advisory. Whilst health authority policies may have to be issued as guidance to independent contractors such as general practitioners, it appeared that trust policies were written more as advice than as firm instruction. The development of primary care trusts and clinical governance may lead to antibiotic policies becoming more directive. If this is to be acceptable, policies will have to be evidence based, and be able to stand up to scrutiny. Some research has been carried out on the place of antibiotics and attitudes towards such policies. One US based study indicated that prescribers preferred and adhered more closely to policies, which involved an educational, rather than a restrictive approach [7] . Spending on antimicrobial drugs represents approximately 20% of drug expenditure in UK hospitals with between a quarter and a third of all patients receiving an antimicrobial agent whilst in hospital [8]. This overview is not essentially about cost savings but there may be savings by managing antibiotics effectively [8]. The important point is that the investment of developing sound antibiotic policies including the associated educational role has the potential to be cost effective. While it is true that there is no absolute proof of a causative association between antibiotic use and resistance, most authorities believe the association to be 'virtually certain' [9]. In a working party report for the British Society for Antimicrobial Chemotherapy [10] details of a much larger survey were reported. The authors published an appendix of nine minimum control measures. These cover broadly similar issues but have generally not been implemented. To the list, the need for an evidence base should be added. National guidelines have now been issued for primary care [11] but the authors are not aware of a model policy for secondary care. Conclusions There is clearly a wide variation across the structure and content of antibiotic policies in the SouthEast. It is hoped that this review will lead to a revision of policies to bring them into line with current recommendations. The overall aim is to ensure that an effective range of antibiotics is maintained. Policies alone will not achieve this, and there needs to be local ownership by all prescribers with effective monitoring to ensure that compliance with the local antibiotic policy can be demonstrated. Perhaps the most pressing need is the development of an evidence base to underpin not only the content of policies but also their implementation and use. This needs to be carried out by a suitable group nationally in order that all may benefit. Pre-publication history The pre-publication history for this paper can be accessed here: Supplementary Material Additional files 1-5 Additional File 1. Appendix - Word file describing feedback from the workshop to discuss the findings of the survey attended by clinicians and pharmacists. Additional File 2. Policysummary - Spreadsheet providing description of individual antibiotic policies Additional File 3. Antibiotic - Spreadsheet providing antibiotic recommendations by individual antibiotic policy Additional File 4. Condition - Spreadsheet providing condition specific recommendations in individual antibiotic policies Additional File 5. Patient - Spreadsheet providing patient specific recommendations in individual antibiotic policies Appendix Policysummary Antibiotic Condition Patient
Objectives This study examined condom acquisition by persons in a hospital setting when single versus assorted brand name condoms were provided. Methods Condom receptacles were placed in exam rooms of two clinics. During Phase 1, a single brand name was provided; for Phase 2, assorted brand names were added. Number of condoms taken was recorded for each phase. Results For one clinic there was nearly a two-fold increase in number of condoms taken (Phase 1 to Phase 2); for the second clinic there was negligible difference in number of condoms taken. Conclusions The provision of assorted brand name condoms, over a single brand name, can serve to increase condom acquisition. Locations of condoms and target population characteristics are related factors. Public health professionals promote condom use as a means to prevent the spread of hepatitis B, HIV and other sexually transmitted diseases (STD) among drug users and other persons at risk for these diseases [1,2,3]. Condom distribution, a strategy for increasing condom availability, is a principal component of risk reduction interventions targeting these persons and condom availability has been enhanced via the application of social marketing principles [4,5,6]. With the establishment of effective condom distribution systems, public health professionals are then challenged to encourage the use of these systems by those at risk. Condom distribution setting, condom cost and provision of a variety of condoms are all factors that may influence people to acquire condom [7,8,9,10,11]. There are a wide variety of condoms on the market, with at least 70 types differing by brand name, size, texture, thickness, color, flavor, scent, lubrication level and lubrication type [12]. The range of condom properties offered suggests that manufacturers are not only responsive to matters related to condom utility, they are also seeking to satisfy user's particular preferences. Among condom users, condom acceptability is influenced by characteristics such as shape, size and amount of lubrication [13,14,15]. These and other potentially desirable characteristics can vary from one brand name to another. The current study examines condom acquisition when single versus assorted brand name condoms was made available to persons within a hospital-based condom distribution system. Methods Setting and Procedures This study was conducted from January to May of 1997 at the Alaska Native Medical Center located in Anchorage, Alaska. The medical center at the time of this study was an Indian Health Service facility that provided health care to Alaska Natives, American Indians and other non-Native American beneficiaries. Prior to beginning this study, the Outpatient Pharmacy and Women's Health Clinic distributed a single brand name condom, offering the LifeStyles lubricated condom, which is listed in the General Services Administration formulary. For this study, we established condom distribution locations in the Internal Medicine Clinic (IMC; open weekdays, 8-6 PM) and Emergency/Urgent Care Clinic (ECC; open 24 hours a day, 7 days a week). Condom receptacles, open-topped clear plastic containers (4  10  7 in), were placed in 5 IMC and 6 ECC exam rooms. Receptacles were wall-mounted with the opening 58 inches from the floor, an unreachable distance for most children. Characteristics of the clinic exam rooms (i.e., room size and furnishings) influenced placement of the receptacles. Exam rooms in the IMC were large and it was often necessary to mount receptacles amongst or over existing furnishings. Exam rooms in the ECC were small and contained few furnishings, allowing us to mount receptacles in more accessible locations. To familiarize patients with the new distribution locations, we provided LifeStyles lubricated condoms for a 30-day period prior to beginning Phase 1. During the study period, IMC and ECC receptacles were monitored and replenished with condoms on a biweekly basis. In addition, a supply of patient education pamphlets pertaining to disease prevention and condom use techniques was also maintained. For Phase 1 (1/23/97 to 3/17/97; 54 days) receptacles were filled with known quantities of LifeStyles lubricated condoms. For Phase 2 (3/18/97 to 5/6/97; 50 days), we continued providing LifeStyles lubricated condoms and added known quantities of assorted brand name condoms, including Love Gasket, Maxx Plus, Ramses, Fiesta, lubricated and mint-flavored Sheik, Kimono and Gold Coin. Data Analysis Clinic visits by patients age 14 years and younger were excluded as we reasoned that a majority of them would be less motivated or unable to take condoms. Hospital computer records were reviewed to determine diagnostic codes for patient encounters in each clinic. Paired samples t test and chi square analysis was used as appropriate. Results Table 1 illustrates sex and age of patients and details of patient diagnoses at encounter for the overall study period. During this time 6000 females and 3578 males were seen at the IMC and ECC; patient sex was significantly associated with clinic attendance (21 = 7.5, P = .006). The mean age for females (39.8 years) was not significantly different than that of males (40.2 years). Patients seen at the ECC were significantly younger (mean = 37.9 years) than those seen at the IMC (mean = 49.3 years; t9576 = -26.89, P < .001). Among IMC patients, diagnoses at encounter were generally chronic or maintenance in nature (e.g., hepatitis B carrier, diabetes) and visits among ECC patients were generally acute or episodic (e.g., trichomonas, drug intoxication). Encounters involving drug or alcohol-related conditions were much more common among ECC patients. Table 1 Number of patient visits, patient demographics and patient diagnoses at encounter by clinic for the overall study period Number of Patient Visits* n (%) Internal Medicine Clinic (IMC) 2444 Sex Female 1043 (60) Male 702 (40) Age 14-28 228 (13) 29-43 457 (26) 44-58 506 (29) 59-73 400 (23) 74-88 148 (8) 89 6 (<1) Patient Diagnoses at Encounter Liver, Heart or Other Internal Medicine Exam or Treatment (most frequent diagnoses) 642 Sexually Transmitted Disease (HIV and hepatitis B) 74 Drug or Alcohol-Related Condition 145 Emergency/Urgent Care Clinic (ECC) 12 528 Sex Female 4957 (63) Male 2876 (37) Age 14-28 2456 (31) 29-43 2945 (38) 44-58 1507 (19) 59-73 705 (9) 74-88 213 (3) 89 7 (<1) Patient Diagnoses at Encounter Upper Respiratory Disorder (most frequent diagnosis) 1293 Sexually Transmitted Disease (gonorrhea, chlamydia, HIV, hepatitis B, trichomonas and genital herpes) 38 Drug or Alcohol-Related Condition 1004 *Values include multiple visits by a single patient. Includes dependent and nondependent drug abuse, and alcohol dependence and intoxication. For the IMC there was slight increase in number of condoms taken from Phase 1 to Phase 2 (285 vs 286). For the ECC there was nearly a two-fold increase in number of condoms taken from Phase 1 to Phase 2 (3565 vs 6067). Discussion Study findings suggest that the provision of assorted brand name condoms, over a singular brand name, will serve to increase condom acquisition. Patients were often unattended in the exam rooms giving them opportunity to take condoms in private, a condition shown to be favorable to improving acquisition rates [7]. Substantial increases in condom acquisition were, however, only observed in the ECC. Previous research illustrates chronic drug users and injection drug users visit emergency rooms more often than nondrug users [16]. Indeed, in the current study ECC patients were more likely than ICC patients to have a drug or alcohol-related diagnosis. This condition and the finding that ECC patients were significantly younger than ICC patients may be indicative of increased sexual activity. Patients may have recognized their sexual behaviors to be high-risk (i.e., for STD or pregnancy); therefore condoms, especially when a variety of brand names were available for selection, were a valued commodity. This suggestion is supported in part by the data as ECC patients had more STD diagnoses that were acute in nature. Data from both study phases showed that ECC patients acquired far more condoms than did IMC patients. The IMC maintains daytime hours of operation on weekdays and an appointment is generally required. In contrast, the ECC is open 24-hours a day, seven days a week and can be visited without an appointment. Thus, ECC patients had greater opportunity and possible intent, as the data show they were much higher risk than IMC patients, to acquire condoms. The higher number of condoms taken from the ECC may also be attributable to condom receptacle placement. Receptacles in the ECC were highly visible and accessible. However, receptacles in the IMC were, in cases, located above equipment making access potentially more difficult. The offering of assorted brand name condoms may appeal to persons in several ways. Merely the uniqueness of the packaging (e.g., wrapper color or design), condom brand name (e.g., Love Gasket or Maxx Plus), or condom style (e.g., flavored or colored) may induce people to take them. In addition, as previously stated, shape, size and amount of lubrication can influence condom acceptability. Those who find these characteristics acceptable may see them identified on the condom label or they may infer them from the brand name. Lastly, people may have familiarity with or loyalty to a particular brand name or they may simply be interested in experimenting with new ones. An intended outcome of this study was the formation of a strategy for improving condom accessibility and acquisition, with the hopeful goal to increase their use, among a population having members who demonstrate low rates of condom use and high rates of sexually transmitted disease [17,18,19]. Therefore, we did not attempt to collect additional patient-specific data given concerns that methods for doing so, such as, obtaining of informed consent, conducting focused interviews and observing condom taking behavior would negatively impact its potential benefit. Consequently, we were unable to determine who took condoms and whether the condoms were used, given away, or even sold or destroyed. In consideration, we limited statistical analyses to patient demographics and number of condoms distributed. However, we contend that the increased number of free condoms taken from the hospital is likely to have resulted in increased availability and usage in the community. The intervention as described herein is currently not being offered at the hospital. This is due in part to a move to a new hospital facility and departmental budget considerations. However, condoms are available at no cost through the hospital pharmacy and they can be taken anonymously from several other clinics. Given the patient population and the high volume of patient visits, the ECC is a key venue for condoms and we are developing a means to reintroduce a distribution system to this clinic. Results of our study suggest that programs maintaining a condom distribution system should offer a variety of brand name condoms to their participants. We note that before beginning this study we conducted no research to support our decision to purchase and distribute one brand name over another. However, we did have anecdotal reports from female patients, who were known to trade sex for money or drugs, that they preferred flavored or nonlubricated condoms for oral sex. Thus, we suggest that program administrators survey their target population to evaluate the acceptability of particular brand names or condom styles. Alternatively, programs could closely monitor condom inventories and subsequently provide brand names that have higher demand. Explicating the relationship between brand name preference, condom distribution location and condom use is a topic for future research. Pre-publication history The pre-publication history for this paper can be accessed here:
Background A review of the safety and efficacy of drinking water fluoridation was commissioned by the UK Department of Health to investigate whether the evidence supported a beneficial effect of water fluoridation and whether there was any evidence of adverse effects. Down's syndrome was one of the adverse effects reported. The aim of this review is to examine the evidence for an association between water fluoride level and Down's syndrome. Methods A systematic review of research. Studies were identified through a comprehensive literature search, scanning citations and online requests for papers. Studies in all languages which investigated the incidence of Down's syndrome in areas with different levels of fluoride in their water supplies were included. Study inclusion and quality was assessed independently by 2 reviewers. A qualitative analysis was conducted. Results Six studies were included. All were ecological in design and scored poorly on the validity assessment. The estimates of the crude relative risk ranged from 0.84 to 3.0. Four studies showed no significant associations between the incidence of Down's syndrome and water fluoride level and two studies by the same author found a significant (p < 0.05) positive association (increased Down's syndrome incidence with increased water fluoride level). Only two of the studies controlled for confounding factors and only one of these presented summary outcome measures. Conclusions The evidence of an association between water fluoride level and Down's syndrome incidence is inconclusive. Introduction A review of the safety and efficacy of drinking water fluoridation [1]  was commissioned by the UK Department of Health to investigate whether the evidence supported a beneficial effect of water fluoridation and whether there was any evidence of adverse effects. Other than dental fluorosis, bone fracture and cancer there was very little evidence available on adverse effects in humans. Down's syndrome was the most discussed of the other adverse effects reported and was therefore selected as the focus for this paper. In approximately 90% of cases, Down's syndrome is due to the non-disjunction of chromosome 21, most often in the oocyte, which may occur during two separate periods: before the completion of the first meiosis or around the time of ovulation.[2] Exposure to risk factors should therefore be measured at the time at which the abnormality may occur, around the time of conception. The main risk factor for Down's syndrome is maternal age with many studies having shown an increased incidence of Down's syndrome with increased maternal age.[3] There has also been some suggestion of an association with paternal age however this has not been confirmed. [4] Other suggested risk factors include race, with an increase rate among Hispanic mothers, [5,6] ionising radiation, [2, 7] increased parity, [3, 8] although this has not been confirmed by all studies, [9] and season, with a peak in births in summer. [10] Any study of a risk factor for Down's syndrome, such as water fluoride level, should consider these other suggested risk factors as possible confounding factors, and should certainly make attempts to control for the confounding effects of maternal age. Water fluoride level has been suggested as a possible risk factor for Down's syndrome and its association with water fluoride exposure has been investigated by a number of studies. If fluoride is associated with Down's syndrome then other sources of fluoride may act to confound the association of water fluoride level with the incidence of Down's syndrome. For example, two women living in different areas, one with a high water fluoride level and the other with a low water fluoride level, might be receiving similar amounts of fluoride if the woman in the low fluoride area was consuming fluoride from other sources, such as fluoride tablets, tea and fluoridated toothpaste. Exposure to other sources of fluoride should thus be considered and measured so that the effects can be controlled for in the investigation of the association of Down's syndrome with water fluoride levels. The objective of this report is to investigate the association of water fluoride level with Down's syndrome and discuss in detail the quality of the studies investigating this association. Methods Search strategy 25 specialist databases were searched by a qualified librarian, including Medline, Embase, Toxline and the Current Contents (Science Citation Index) from database inception to February 2000. In addition, hand searching of Index Medicus (19451959) and Excerpta Medica (19551973) was undertaken. Additional references were sought from individuals and organisations through a dedicated web site for this review  and through members of a specifically designated advisory panel. Published and unpublished studies in any language were included. Full details of the search strategy are reported elsewhere. [11] Inclusion criteria All study designs which compared the incidence of Down's syndrome in populations with different levels of fluoride, either artificially added or naturally occurring, in their water supplies were included in this review. Data extraction and quality assessment Two reviewers independently assessed each paper for inclusion, disagreements were resolved through consensus. Extraction of data from individual included studies was independently performed by two reviewers, and checked by a third reviewer. Disagreements were resolved through consensus. Study validity was formally assessed using a published checklist modified for this review. [12] The criteria used to assess study validity were developed for the main fluoridation review. [1]  and were used for all studies included in the main fluoridation review to allow a general comparison between the quality of all of these studies. These criteria were retained for this paper to allow the results of the studies which looked at Down's syndrome to be viewed in the wider context of all studies that looked at adverse effects of fluoridation, when this paper is considered together with the full fluoridation review. Each study was assigned a score, based on the number of checks achieved on the checklist, out of a maximum score of eight. The criteria used to score the studies are described in Table 1. Study validity was assessed independently by two reviewers, with disagreements resolved through consensus. Table 1 Validity criteria used to score studies Prospective Was the study prospective? Was it planned and started prior to the outcome of interest occurring? Score = 1 or 0 Study Design The study design hierarchy for this review = cohort > before-after > ecological > cross- sectional. Scores ranged from 0.251, with cohort = 1, cross-sectional = 0.25 Fluoride Measurement Was the Fluoride level reliably measured? Scores range between 01. Confounding Factors Were confounding factors addressed (measured)? Scores range between 01, with 3 or more factors measured = 1. Control for Confounding Was there adjustment for the possible effect of confounding factors in the analysis or study design? Scores range between 01, with stratification by age and sex = 0.5, other types of analysis (e.g. regression) = 1. Blinding Were those measuring outcomes and exposures blind to the exposure/outcome status of the person being assessed? Score = 0 or 1 Baseline Survey Was there a baseline survey at the point of initiation or discontinuation of water fluoridation? Score = 0 or 1 Follow-Up Was the study conducted an adequate time after the initiation or discontinuation of water fluoridation to assess effects (5 years)? Score = 0 or 1 Analysis The studies did not provide sufficient information to permit pooling of data or investigation of statistical heterogeneity. A narrative synthesis is presented. For studies which did not report on crude or adjusted summary measures such as the risk difference or relative risk but provided sufficient information to calculate this, a crude relative risk was calculated with 95% confidence intervals where possible. Where a study looked at more than 2 study areas the area with the lowest fluoride concentration was compared to the area with a fluoride level closest to 1 ppm. However, results for all study areas are presented in table 2. Insufficient data was available to investigate publication bias using standard methods (funnel plots). Table 2 Individual study details and results Study Details Outcome and exposure Inclusion/Exclusion Group Water Number of Results details Criteria fluoride level live births* (crude risk) (parts per per 100 000* million) Author (year) Method of outcome Inclusion criteria Group 1: 0.71.1 20760 159.0 Berry (1958) assessment: Children born in study Group 2: 1.92.0 14710 122.4 Region of study Institutions, death areas during study Group 3: 0.9 9492 137.0 Essex, England certificates, records of period, mothers living Group 4: <0.2 12620 190.2 Year study started medical officers of in study area at time of Group 5: <0.2 11587 164.0 1945 health authorities, birth Group 6: 0.2 22452 164.8 Study Length personal knowledge of Group 7: 0.2 14873 107.6 9 years health visitors Control: 0.2 6870 131.0 Author (year) Method of outcome Inclusion criteria Metropolitan Erickson(1976) assessment: Birth of white children area Region of study Cases identified through only, areas in which Group 1: High 95254 99 Georgia, USA surveillance mothers' usual place Group 2: Low 25373 85 Year study started programmes, data was of residence at birth of NIS p>0.05 19601973 supplemented by a child permitted surveillance Study Length retrospective determination of areas 13 years ascertainment (using exposure to fluoridated Group 1: 234300 49 multiple sources) of water Group 2: 1032100 51 children born between p>0.05 1960 and 1967. Author (year) Method of outcome Inclusion criteria Erickson(1980) assessment: Cities with 1970 Region of study Data from birth populations >= 250 USA certificates obtained 000, Cities fluoridated Group 1: >= 0.7 432580 41.1 Year study started from US Nation Center for >= 5 years by 1973 Group 2: 0.7 204185 44.1 1973 for Health Statistics, Exclusion criteria Study Length denominator number of Cities with mixed Indirect age 2 years live births in study areas fluoridation status standardised States which do not rates: report birth defects on 41.0 birth certificates 44.0 Cities fluoridated for <5 years by 1973 Author (year) Method of outcome Inclusion criteria Group 1: 1 81017 153.1 Needleman(1974) assessment: Children born with Group 2: 0.3 1752435 133.8 Region of study Cases identified through Down's's syndrome Massachusetts, USA maternity and paediatric Year study started hospitals, Departments 1950 of Public and Mental Study Length Health, private nurseries 17 years and school for mentally retarded children, karyotyping laboratories and several miscellaneous sources Author (year) Method of outcome Inclusion criteria Group 1: 1.02.6 67053 71.6 Rapaport (1963) assessment: All cases children with Group 2: 0.30.7 70111 47.1 Region of study Cases identified from Down's's syndrome Group 3: 0.10.2 132665 39.2 Illinois, USA birth and death born during study Group 4 0.0 63521 23.6 Year study started certificates, registers of period 1950 specialist medical Town (of mother's Study Length educational state residence) size 10 000 6 years institutions -100 000 Author (year) Method of outcome Inclusion criteria Dakota Rapaport (1957) assessment: Not stated Group 1: >3 31575 34.8 Region of study Alive subjects with Exclusion criteria Group 2: <3 467685 15.2 USA Down's's syndrome Not stated Illinois Year study started identified through Group 1: 1.62.6 41618 14.4 Not stated institutions (cases living Group 2: 1.01.2 210628 11.4 Study Length in the community not Group 3: 0.40.7 196258 12.2 Not stated identified) Group 4 0.3 151167 6.6 Group 5: 0.10.2 670120 6.0 Group 6: 0.0 7049 3.9 Wisconsin Group 1: 2.8 52735 30.3 Group 2: 1.4 21538 32.5 Group 3: 0.5 51189 25.4 Group 4 0.1 1076876 13.5 * Rapaport (1957) did not report the total number of births, for this study the population figures are provided and the crude risk is the risk per 100 000 population Results Six studies investigating the association of Down's syndrome with water fluoride level were identified [3, 14, 15, 16, 17, 18] these were all ecological in study design. The studies ranged in publication date from 1957 to 1980, five were conducted in the USA [14, 15, 16, 17, 18] and one was conducted in England. [13] Two of the studies were published in French [17, 16] the others were published in English. Study duration ranged from 2 to 17 years. Case ascertainment was from a variety of sources including birth and death certificates, institutions, surveillance programmes, hospitals, nurseries and schools for mentally retarded children. Most studies attempted to measure incidence by identifying all cases born during the study period, [13, 14, 15, 17, 18] however, one study only measured prevalence, by identifying cases living in institutions and hospitals. [16] The denominator used to calculate risks in most studies [13, 14, 15, 17, 18] was the number of live births in the study areas during the study period however, one study used the populations of the study areas as the denominator. [16] Exposure was classified according to the area of maternal residence for all but one of the studies [15] which classified exposure according to the town of maternal residence 9 months prior to the birth. None of the studies stated how the areas selected for the study were chosen, although for one study [15] the data were originally assembled for a large scale epidemiologic study of Down's syndrome, and data which could be related to water fluoride exposure were included for this study. Details of baseline information and results from each study are presented in table 2. The quality of these studies was generally poor; the average validity checklist score was 2.3 with a range of 1.8 to 3.5 out of a possible score of 8. None of the studies had a prospective follow-up, incorporated any form of blinding, had a baseline survey or stated how the level of fluoride in the water was calculated. Controlling for confounding factors was generally inadequate. All studies scored marks for study design (1/2 for using an ecological design) and for adequate length of follow-up i.e. the survey was carried out more than 5 years after the water supply had been fluoridated (Table 3). Table 3 Validity Assessment (Score out of 8) Author Prospective Study Fluoride Confounding Control for Blinding Baseline Follow- Score Design Measurement Factors Confounding Survey Up Erickson (1976) 0 1/2 0 1 1 0 0 1 3.5 Erickson (1980) 0 1/2 0 1 1 0 0 1 3.5 Needleman (1974) 0 1/2 0 1/4 0 0 0 1 1.8 Rapaport (1963) 0 1/2 0 1/4 0 0 0 1 1.8 Rapaport (1957) 0 1/2 0 1/4 0 0 0 1 1.8 Berry (1958) 0 1/2 0 0 0 0 0 1 1.5 Table 4 shows the association of water fluoride level and the incidence of Down's syndrome, together with validity score and the confounding factors discussed and controlled for in each study. Table 4 Association of Down's syndrome with water fluoride level Author (Year) Crude relative risk Confounding factors discussed in Controlled for Validity score study Erickson(1976) 1.16(p > 0.05) Maternal age, race Yes 3.5 0.96 (p > 0.05) Erickson(1980) 0.93 (0.7, 1.2) Maternal age, race Yes 3.5 Needleman(1974) 1.14 Maternal age No 2.0 Rapaport (1957) 2.3 (p < 0.01) Maternal age No 2.0 2.9 (p < 0.01) 2.4 (p < 0.05) Rapaport (1963) 3.0(p < 0.001) Maternal age, minerals in water No 2.0 Berry (1958) 0.841.48 None No 1.8 Four of the six studies provided a measure of the significance of the association of water fluoride level with Down's syndrome.[18, 14, 17, 16] Two of these studies found no significant difference in Down's syndrome incidence between high and lower water fluoride areas. [18, 14] The other two studies, by the same author, found an increased incidence of Down's syndrome in areas with higher water fluoride levels (p < 0.01, RR ranged from 2.3 to 3.0).[16, 17] One of the other studies did not find any association between water fluoride level and Down's syndrome incidence, [13] depending on the control area selected, the crude relative risk ranged from 0.84 to 1.48. The remaining study [15] suggested a positive association between water fluoride level and Down's syndrome incidence (increased incidence with increased water fluoride concentration) when only the crude incidence rates were compared. To achieve some control for maternal age the analysis was limited to the 30 towns that initiated fluoridation. The rate of Down's syndrome among births in fluoridated areas was compared to the combined rate among births occurring before fluoridation and, for towns that stopped fluoridation, after fluoridation. Limiting the analysis in this way produced two groups comparable in maternal age, and produced similar estimates of the incidence of Down's syndrome in the two groups. Another factor thought to be confounding the association of Down's syndrome with water fluoride exposure was time. Time trend was controlled for and produced a maximum likelihood estimate for the relative risk was 0.95 (95% CI: 0.8, 1.2), suggesting no significant association between Down's syndrome and water fluoride level. Although all but one study [13] mentioned maternal age as a confounding factor only two studies controlled for this in the analysis. [14, 18] Both studies included white births only and presented results separately for 5 year age groups. One study [14] also presents age-adjusted rates. A negative non-significant association of water fluoride level with Down's syndrome (decreased incidence with increased water fluoride concentration) was found by this study, when controlling for the effects of maternal age. The other study [18] shows no overall significant differences between the study areas for the results stratified on maternal age, although this study does suggest an increased incidence of Down's syndrome at young maternal ages and a decreased incidence at older ages in the fluoridated areas. A third study [15] presented the mean maternal age in the two study areas and stated that the mean age of mothers was higher in the high fluoride areas than the low fluoride areas (34.0 versus 33.2) and suggested that this difference was large enough to account for the observed difference in Down's syndrome incidence between the two areas, when crude rates were compared. Another study [17] showed that the proportion of cases among mothers aged over 40 was less (11%) in high fluoride areas than low fluoride areas (24%). A study by the same author [16] reported that maternal age was higher in low fluoride areas (34.3) compared to areas fluoridated at 1 ppm (33.2), although the groupings of areas by water fluoride level differ for the description of maternal age compared to the groupings for Down's syndrome incidence. Discussion This systematic review suggests that the evidence for an association between water fluoride level and the incidence of Down's syndrome is weak, and that all the identified studies were of poor quality. All results, positive and non-positive, should therefore be considered together with the methodological weaknesses of the studies which could have lead to spurious results. In particular, the results of the two studies [16, 17] which showed a significant positive association with water fluoride level should be interpreted with extreme caution due to the methodological limitations of these studies discussed below. The major weakness of these studies was the failure to control sufficiently for confounding factors. All six studies used study designs that measured population rather than individual exposure to fluoridated water and because of this are particularly susceptible to confounding. If the populations being studied differed in respect to other factors that are associated with the outcome under investigation then the outcome may differ between these populations leading to an apparent association with water fluoride level. [19] The incidence of Down's syndrome is known to be strongly associated with maternal age. [20] If the average maternal age of the high fluoride population is higher than that of the low fluoride population an association with water fluoride level would most likely be found, even if such an association does not in fact exist. Maternal age was considered by all but one of the included studies, however only two of the six studies appropriately controlled for the effects of maternal age. The two studies [16, 17] which found a positive association between Down's syndrome and water fluoride level were two of the studies which did not control appropriately for the possible confounding effects of maternal age and so the results of these studies should be interpreted with some degree of caution. Another factor which may affect the association of Down's syndrome with water fluoride level is maternal exposure to other sources of fluoride, such as fluoridated toothpaste, mouthwashes and fluoride tablets. None of the studies controlled for or measured any of these factors. Other factors which could have led to misleading study results include selection of study areas, ascertainment of cases, population selected for the denominator, migration, classification of exposure and blinding of investigators to the fluoridation status of cases. If study areas are not selected at random there is a possibility that selection may be biased, for example, a fluoridated area with a relatively high incidence of Down's syndrome (possibly for reasons other than fluoride concentration of the water) and a non-fluoridated area with a relative low incidence may be selected which would result in biased results. Case ascertainment must be as complete as possible and must be uniform across study areas otherwise cases in one area may be more likely to be identified than those in another area and possibly result in a misleading finding. All but one [16] of the studies attempted to locate all cases born in the study areas during the study period by searching a variety of sources, these studies all state that they believe that they located the majority of cases. The other study [16] limited case ascertainment to live cases living in institutions and hospitals. Limiting the cases in this way may result in a large proportion of cases (more than half) being missed. [14] This would be a particular problem if the proportion of cases identified differed between the different areas, for example if a higher proportion of cases lived in institution in the fluoridated area compared to the control area this would result in a misleading association. Also, if there more deaths among people with Down's syndrome in one area than another this could result in fewer living people with Down's syndrome in one of the study areas, leading to a possibly biased association. The population selected for the denominator may also affect the associations found. One of the two studies [16] which found a positive association used the total population of the study areas as the denominator while all the other studies [13, 14, 15, 17, 18] used the number of live births as the denominator. For studies of birth defects it is more usual to use the total number of births as the denominator. If the population structure of two areas differ, with one area having a higher proportion of women of childbearing age, then the birth rate in this area will also be higher and thus the incidence of birth defects, such as Down's syndrome, is likely to be higher. Using this figure as the denominator can thus lead to false conclusions. Classification of exposure is another area where bias can be introduced. Down's syndrome is a genetic defect that occurs at around the time of conception [2] and so water fluoride exposure should be classified according to the area in which the mother was resident at the time of conception. Only one study classified exposure at the time of conception, [15] the others classified exposure at the time of birth, this may lead to the misclassification of births to mothers who moved during their pregnancy. The length of exposure to fluoride necessary to have an effect could be several years in which case the exposure should be classified as women exposed or not exposed to water fluoride for a certain number of years prior to conception. Exposure was not classified in this way in any of the included studies. The effects of migration were not discussed in any of the studies. Whether migration could bias study results depends on when the water fluoride level is thought to have an effect on the woman: whether it is a long term build up or a short term effect around the time of conception. If it is the latter then as long as exposure status was identified as exposure at time of conception not birth this should not a problem. However, if there is a fluoride effect with a long induction period, any study of this effect would have to take account of migration. Investigators should be blinded to the fluoridation status of the cases that they are identifying otherwise their views on fluoridation may affect the thoroughness of their search for cases. For example, if an investigator believes that there is an association between water fluoride level and down's syndrome, and knows that the sources they are searching to identify cases relate to cases whose mothers have been exposed to high levels of water fluoride, they may be more thorough in their search for cases. None of the studies mentioned blinding of investigators. The studies included in the review were all conducted at least 20 years ago. This may be a problem in generalising results to the present time if factors that would affect the incidence of Down's syndrome, and especially its association with water fluoride levels, have changed in that time. It may be that if fluoride has an effect on the incidence of Down's syndrome the mother has to be exposed to fluoride over a long period of time. Fluoridation was first initiated in the 1940s [21] thus many of the women included in these studies may only have been exposed to water fluoride for a short period of time. Another factor which has changed since most of these studies were conducted is the total fluoride exposure of the mothers, fluoride is now available from other sources, to which women would not have been exposed in the earlier studies. Other factors which may affect the incidence of Down's syndrome is the changing demographics of maternal age at birth, with women in the developed world now giving birth at older ages than they did 20 years ago. [22] Abortion is now more acceptable [23] and screening for Down's syndrome is routine, especially in older women, [24] and so the option to terminate a birth if the child is diagnosed with Down's syndrome is now a possibility. Conclusions The evidence of an association between water fluoride level and Down's syndrome incidence is inconclusive. However, the quality of the studies included in the review was relatively low and further high quality research is needed. Future studies investigating the association of Down's syndrome with water fluoride levels should measure individual exposure to water fluoride and control appropriately for confounding factors, especially maternal age, incidence of termination of pregnancies in which the child is diagnosed with Down's syndrome, and exposure to other sources of fluoride. Study areas should be chosen at random and investigators should be blinded to the fluoridation status of mothers when identifying cases. The denominator selected to measure the risk of a Down's syndrome birth should relate to the total number of births, not to the overall population of the study area. Case ascertainment should be as complete as possible, and should be identical in all populations studies. Pre-publication history The pre-publication history for this paper can be accessed here:
Introduction Clustered within the nomenclature of Asian American are numerous subgroups, each with their own ethnic heritage, cultural, and linguistic characteristics. An understanding of the prevailing health knowledge, attitudes, and screening behaviors of these subgroups is essential for creating population-specific health promotion programs. Methods Korean American women (123) completed baseline surveys of breast cancer knowledge, attitudes, and screening behaviors as part of an Asian grocery store-based breast cancer education program evaluation. Follow-up telephone surveys, initiated two weeks later, were completed by 93 women. Results Low adherence to the American Cancer Society's breast cancer screening guidelines and insufficient breast cancer knowledge were reported. Participants' receptiveness to the grocery store-based breast cancer education program underscores the importance of finding ways to reach Korean women with breast cancer early detection information and repeated cues for screening. The data also suggest that the Asian grocery store-based cancer education program being tested may have been effective in motivating a proportion of the women to schedule a breast cancer screening between the baseline and follow-up surveys. Conclusion The program offers a viable strategy to reach Korean women that addresses the language, cultural, transportation, and time barriers they face in accessing breast cancer early detection information. Background Breast cancer is the most common form of cancer in Asian American women, and their cancer screening rates are lower than  any other ethnic group [1,2]. Incidence of breast cancer among Asian American women increases with the duration of their US residency, making breast cancer education even more important as U.S. duration increases [3]. With the assistance of key informants, the social patterns of Asian American women were evaluated for places where health-related data might be collected and educational interventions offered. Asian grocery stores appeared to offer a valuable site for a community based health education program [4]. A recent study of 1,202 Asian American women assessed their baseline breast cancer knowledge, attitudes, and screening behaviors, as well as their receptivity to the Asian Grocery Store-Based Breast Cancer Education Program they were offered [5]. While health-related information about Asian American women is important, health educators could be more effective if they had more specific information related to the subgroups within the Asian American cultural cluster [6-9]. The Korean population accounts for a relatively small portion (11%) of the Asian American community. As a result, its characteristics and health needs are often overlooked by health researchers, educators, and providers. However, the Korean community is also a growing community. From 1980 to 1990 it increased by 123% (357,393 to 799,000) [10-12]. This makes Korean women an important community to consider when trying to improve the overall health of the Asian community. To address this community optimally, health educators need specific information about Korean American's culture, acculturation, socioeconomic status, and health related knowledge, attitudes, and behaviors. This paper presents data pertaining to Korean American women's breast cancer knowledge, attitudes and behaviors. Methods The Health Belief model was selected as the intervention's theoretical framework because it can be used to help explain, predict, and influence individual's health related behaviors [13]. It can also be used as the frame of reference for initiating health promotion dialogues with patients. The theory presumes that five conditions must usually be present for individuals to initiate a health related action. First, individuals must be aware that a health threat of considerable magnitude exists. Second, individuals must perceive that they are personally vulnerable to the health threat. Third, individuals must believe that taking a particular action could effectively reduce the chance of incurring the health threat. Fourth, individuals must believe that the benefit of the health promotion action outweighs the consequences of not taking the action. Finally, patients are more likely to take the health action if there is repeated cuing. Asian grocery stores were recruited as educational sites throughout San Diego County in anticipation that they would be culturally acceptable community education sites. It was also anticipated that Asian grocery stores would attract shoppers who were diverse in age, socioeconomic status, acculturation, and language proficiency. Outreaches were held on weekends (62%), as well as week days. Displays of ethnically enriched materials were offered along with sun screen samples in recognition of the high value Asian women place on beautiful skin, and as a gentle segue to the discussion of cancer prevention and early detection. Bilingual, bicultural Asian American university students were trained to work as community health educators at the Asian grocery stores where they provided breast cancer education to Asian women and gained experience that encouraged many of them to pursue health and research careers [14,15]. Students were recruited using campus newspapers, electronic message boards on campus buses, academic internships, and word-of-mouth. Student health educators were trained using the same National Cancer Institute (NCI) and American Cancer Society' (ACS) training materials on breast cancer that were disseminated at the grocery store outreach sites. They were trained by the senior author (Sadler) about the benefits of early breast cancer detection, how to conduct [breast-self exams], the recommended screening guidelines, and the most frequently asked questions. Weekly meetings were held with student community health educators to continue their training, monitor progress, provide feedback to fine tune the program, and promote students' consideration of careers in health and science. The health educators, fluent in the preferred language of the participants, disseminated this same information to the women in either English or Korean. They taught them how to do a breast self-exam using simulated breast models and a variety of visual, hands on teaching aids. They also promoted access to the State's free and low cost screening services. The women who agreed to participate in the evaluation of the education program completed an IRB-approved consent document and a short baseline survey. The survey was self-administered with supervision and as needed assistance in the women's preferred language from the community health educators. A translation of the survey was also available in Korean. The survey included demographic questions and close-ended questions that assessed the acceptability of the intervention, identified barriers to education, and measured breast cancer screening practices and attitudes. The surveys were limited to a brief series of questions to encourage participation in the study and allow time for the educational program to be offered. Follow-up telephone surveys, in both English and Korean, were initiated two weeks post intervention. These included both open-ended and close-ended questions focused on the women's willingness and ability to schedule a breast cancer screening. Community health educators made up to ten telephone attempts to contact the women for the follow up survey. When telephone contact failed or was not an option, follow-up surveys were mailed to those who had provided an address. Data from the baseline and follow up surveys were analyzed using frequency accounts, percentages, chi-square tests and the American Cancer Society's breast cancer screening guidelines. Description of the Sample During the time educational sessions were held, an estimated 12,060 women entered the grocery stores during the data collection sessions, with 8,877 participating in individualized or small group discussions about breast cancer. Approximately 1,600 women were invited to take part in the study. Of the 1,202 Asian American women who completed IRB approved consent forms and baseline surveys, 123 were Korean. Their ages ranged from 23 to 86 years (mean = 41.49 s.d. = 11.48). When women failed to report their age (n = 13), the health educators estimated their age to facilitate evaluation of their adherence to screening guidelines by age group. These women were not included in the average age calculation, but were included in the assessment of women's adherence to the American Cancer Society's recommended screening guidelines. Of these women, all were estimated to be over 50, and hence candidates for annual mammography and clinical breast exams. Of the women interviewed, 96.7% reported Korean as their native language (1 spoke Chinese and 3 spoke English). Results Baseline Survey Results A breast self-exam within the past month was reported by 30.9% (38/123) of all participants. Of the 64 women aged 40 years and older, 48.4% (31) reported a clinical breast exam and 21.9% (14) reported having had a mammogram in the past 12 months. Of the 37 women 50 and older, 43.2% (16/37) reported a clinical breast exam and 27% (10/37) reported a mammogram in the past 12 months (Table 1.). Table 1 Adherence to Breast Cancer Screening Guidelines by Age (N = 123) Age Groups 2039 years 4049 years >=50 years Unspecified Age Screening Total N = 59 Total N = 27 Total N = 24 Total N = 13 Behaviors N (%) N (%) N (%) N (%) Monthly self 32.2% (19) 22.2% (6) 29.2% (7) 46.2% (6) breast exam Annual Clinical 35.6% (21) 55.6% (15) 41.7% (10) 46.2% (6) breast exam Annual 16.9% (10) 14.8% (4) 29.2% (7) 23.1% (3) Mammogram *American Cancer Society guidelines in effect at the time of the study were: 1.) Beginning at age 20, monthly breast self examination and clinical breast exam every three years is recommended. 2.) Beginning at age 40, clinical breast exam is recommended annually plus mammography every one or two years. 3.) Beginning at age 50, annual mammogram is recommended along with continuation of monthly breast self exams and annual clinical breast exams. When asked if they had adequate breast cancer knowledge, 16.3% of the participants reported that their knowledge was adequate. The majority of women (89.4%) were interested in receiving more information to keep their families healthy. Most women (83.7%) felt their loved ones would be interested in receiving such information and an equal number of women were also willing to share the knowledge they gained with family and friends. Of the 123 women, 82.9% (102) said they were willing to receive personal educational information, such as breast cancer information. The women indicated that they preferred to receive such information by mail (76.4% (94)), telephone (34.1% (42)), and educational programs (35.8% (44)). The grocery store program was not given as an option. Most of the women (77.2%) were willing to be contacted again if there were further questions related to the current study, while 35.8% (44) said that they would being willing to participate in other research studies like the present one. When women were asked about their most common barriers to participating in additional breast cancer education sessions, 55.3% (68) reported lack of time, 34.1% (42) reported language barriers, and 5.7% (7) did not believe that breast cancer education was important. In addition, 4.9% (6) of the women did not want to think about breast cancer, 4.9% (6) reported lack of money, 1.6% (2) reported transportation as a barrier, and 0.8% (1) felt the topic was embarrassing to discuss. Follow-Up Survey Results Of the 123 Korean American women who completed the baseline survey, 76.4% (94) completed the follow up surveys. The non-participants either did not provide contact information (1), did not provide correct contact information (4), left the country for an extended period of time (2), or could not be reached after 10 telephone attempts and failed to return surveys that were mailed to them (22). Those who participated in the follow-up survey did not significantly differ from the non-participants in age. Of the 94 women who completed the follow up, 50 women were at least 40 years of age and older. Of these 50 women, 14% (7) reported that they had set up a clinical breast exam in the interval between baseline and follow up. Of the 7 women who set up a screening in the interval, 28.6% (2) had not been in compliance with screening guidelines at baseline. Looking at this reported change from another perspective, of the total group of women (33) who had not been in screening compliance at baseline, 6.1% (2/33) had set up a screening in the interval. Since the community of health educators felt that the women who did not give their ages were well beyond 50 years of age, these women are included in the analysis of women 50 and older. Of these 37 women, 13.5% (5) reported having set up a mammogram between baseline and follow up. Of the women that set up a screening in the interval, 60% (3) had not been in compliance with screening guidelines at baseline. Looking at this reported change another way, 27 women in this age group reported they were not in compliance with the mammography screening guideline when they completed their baseline survey. Of those 27 women, 11.1% (3) reported setting up a screening mammogram by the follow up survey. Discussion Our results confirm the low breast cancer screening rates previously reported among Korean women [1-3,10]. The lack of sufficient breast cancer knowledge reported by the study participants may have contributed to this low screening adherence. Women's willingness to learn, discuss, and share breast cancer information with their family and friends suggests that these low screening rates might be improved by focused educational intervention programs. The follow up results demonstrated a slight improvement in screening adherence that might have been the result of the breast cancer information the women received. Since this was a demonstration project rather than a randomized trial, it is not possible to determine if the intervention did in fact have a causal role in helping to facilitate the reported increased screening or if the increased screening rates were the results of chance alone. Further research is needed. Based upon the results of previous studies, embarrassment was anticipated to be a frequently reported barrier to screening [2,3]. In fact, less than 1% of the women in this study reported it to be a barrier. Given that 96.7% of the participants reported Korean as their primary language and 34% reported language as a barrier to participating in cancer education programs, it is unlikely that this infrequent reporting of embarrassment is secondary to the sample having a high degree of acculturation. This lack of reported embarrassment may help to explain the high degree of acceptability women demonstrated toward learning about this very personal health concern in a very public venue, from a culturally aligned, but veritable, stranger. Alternatively, the acceptability of the grocery store-based education program may also have been a consequence of the participants' view that the grocery store venue is within the exclusive purview of women's social milieu. Few men were present, and those who were present, were there to provide transportation and physical assistance to the women they accompanied. While they waited for the women, male health educators engaged them in discussions of the important role men can play in promoting adherence to breast cancer screening guidelines. Consistent with previous studies, virtually all women preferred to be interviewed in Korean [2,3,10]. With 34% of the women reporting language as a barrier to breast cancer education, lack of language proficiency appears to be an important barrier to Korean women's access to health care. On the other hand, 64% of the women did not report language as a barrier to accessing breast cancer education, suggesting that there was sufficient bilingual proficiency within at least a proportion of the sample, to negotiate confidently the complex American health care environment. It is also possible that the participants' awareness of the availability of bilingual health educators and educational materials in Korean contributed to the lack of perceived language barrier. The program's success in recruiting minority students to service learning opportunities and promoting their consideration of graduate training in health and science careers is a positive by- product of this educational program. The increased availability of health providers and scientists who are fluent in the Korean culture and language is recognized to be an important contributor to improving the health and well being of the Korean community. Time was the most frequently reported barrier to participation in cancer education programs, suggesting that part of this program's success at reaching the women was due to its ability to combine education with women's routine patterns of daily living and hence, ease of accessibility. Women were able to incorporate education into their daily routine and received educational booklets for later reading. They also received the phone number to schedule a free breast cancer screening through the State's Breast Cancer Early Detection Program. Eligible women were given appointments when they called. Women who were not economically challenged and at least 40 years of age, and hence not program-eligible, were directed to other appropriate, local screening services. Since the program was offered at the participating grocery stores, women could return with additional questions or bring loved ones for training. Even on days when the education program was not being offered, women might still be reminded to schedule appointments by subliminal cuing associated with passing the usual location for the educational program. The relatively low rate of Koreans who reported having had a clinical breast exam and/or mammogram could also be attributed to limited attention that has been given to the role of filial piety within the Korean family. In the Korean household, the eldest son and his wife care for the elders and make all important decisions for the family [16]. Breast cancer educational programs that provide information to these younger, guardians of family well being could work synergistically with educational programs focused on the women themselves. Previous studies have documented the common misconceptions among Korean women related to breast cancer. The cause of breast cancer has been attributed to air pollution, moral wrong doing, hitting or bumping the breasts, and temperature change [3]. The educational program did not challenge these disbeliefs, but instead, offered information the women could use to alter their risk of late stage detection and its consequences. Following the Health Belief Model, the intervention strategy was to give women information that would foster a greater internal locus of control and encourage proactive screening behaviors. Traditional medicine such as acupuncture and herbs were also identified as alternatives to breast cancer screening in previous studies [3]. Mistrust, or lack of a personal relationship with a physician could also discourage an older woman from going to the doctor and levitate towards the comforts of traditional medicine. The education program and the breast cancer screening activities it promoted, incorporated strategies for facilitating Korean women's prompt access to health care providers and the breast cancer screening they could provide, without undermining their respect for traditional medicine. Limitations Since the data collection tools were created specifically to assess the educational needs of this community and the acceptability of the Asian grocery store as a site to meet identified educational needs, the instruments have not previously been validated. The need to keep the survey brief, prevented inclusion of many questions that would have further enriched the findings. The data were drawn from a convenience sample within one geographic region. The very act of taking part in the study's educational program, the consenting process, and the extended data collection may have created a sample of women who are not representative of the population of Korean women. Therefore, these findings must be applied with caution. However, for the 123 Korean women who did participate, the data offers additional insights into their prevailing breast cancer knowledge, attitudes, and behaviors and demonstrates the importance of gaining a better understanding of the ethnic subgroups that are included and stereotyped within the nomenclature of Asian American. Conclusion Given the Korean American women's low screening rates and their willingness to learn and share breast cancer information, the provision of culturally sensitive learning opportunities appears to be worthwhile. Programs that are easy for women to access and also recognize the role of the eldest son in family matters would appear to be both acceptable and effective methods of encouraging Korean women to adhere to recommended breast cancer screening guidelines. Given the demonstrated acceptability of the Asian Grocery-Store-Based Cancer Education Program, this program's format may also be applicable for promoting awareness of other health and social welfare issues. Competing Interests None declared Pre-publication history The pre-publication history for this paper can be accessed here:
Background This study examined the relationship between the bacteriological contamination of drinking water from private wells and acute gastrointestinal illness (AGII), using current government standards for safe drinking water. Methods A prospective cohort study was conducted using 235 households (647 individuals) randomly selected from four rural hamlets. Data were collected by means of a self-administered questionnaire, a self-report diary of symptoms and two drinking water samples. Results Twenty percent of households sampled, had indicator bacteria (total coliform or Escherichia coli (E. coli)) above the current Canadian and United States standards for safe drinking water. No statistically significant associations between indicator bacteria and AGII were observed. The odds ratio (OR) for individuals exposed to E. coli above the current standards was 1.52 (95% confidence interval (CI), 0.336.92), compared to individuals with levels below current standards. The odds ratio estimate for individuals exposed to total coliforms above the current standards was 0.39 (95% CI, 0.101.50). Conclusions This study observed a high prevalence of bacteriological contamination of private wells in the rural hamlets studied. Individual exposure to contaminated water defined by current standards may be associated with an increased risk of AGII. Background Although the incidence of waterborne illness has decreased in North America, contaminated water remains a problem in rural areas that rely on private drinking water supplies. In Ontario and in the United States at least 30% of rural wells are fecally contaminated and exceed current government standards for safe drinking water. [1-7] Because of the high prevalence of contaminated wells in rural areas it is important to determine the health effects and thus quantify the burden of illness. Studies worldwide, however, have been unable to demonstrate associations between fecally contaminated water characterized by indicator organisms and self-reported acute gastrointestinal illness (AGII). [8-10] This lack of association may be related to the specific indicator bacteria used to quantify risk, as well as, the presence of relatively low levels of these indicator bacteria.[8,11] Total coliform is a non-specific indicator of fecal contamination and can originate from a number of different plant and soil sources. Fecal coliform represents a more specific indicator of fecal contamination, however, it does not specifically quantify Escherichia coli (E. coli), the most common coliform inhabiting the intestinal tract. Three studies which reported positive findings found associations with alternative indicator bacteria, albeit, the associations were limited to specific populations. A study of children in the Philippines showed substantially more illness among children drinking contaminated water quantified by the presence of E. coli, but the effect was limited to children less than 2 years of age and only for those children drinking highly contaminated water.[12] A study conducted in French alpine villages found a positive association between fecal streptococcus only. No independent associations were found for levels of total coliform or fecal coliform and AGII. In fact, fecal coliforms were found to be protective for small villages.[11] As well, a study of farm wells in Ontario found a significant association between the presence of E. coli and AGII at the individual level modified by the distance from the septic tank.[13] We sought to examine the association between drinking contaminated water defined according to current standards in Ontario and AGII in a random sample of households in four typical rural communities in southern Ontario. Using prospectively collected symptom diaries we studied all individuals in the household and used both non-specific (total coliform counts) and specific (E. coli counts) measures of fecal contamination. Methods Four rural communities located in eastern Ontario were selected for this study, representing a cross section of rural populations in this area. The hamlets were largely composed of English speaking residents (98%) averaging 3.1 individuals per household.[14] The mean age of the residents averaged between 32.7 and 38.2 years and the mean household income ranged between $41,000 and $56,000.[14] Households were randomly selected using a phone book database and subsequently, contacted by phone to enlist their participation. All households in the study area were eligible with the exception of those not consuming water from a private well, and residents of retirement or nursing homes. Relevant study information was collected on each member of the participating household. The study was conducted over a three month period commencing April 3, 1995 and ending July 22, 1995. Each household was blinded to the exposure during the 28 day study period and followed for a period of 28 days. Sample size was determined so as to be able to detect a 9% difference in incidence of AGII. Data collection Data collection was accomplished through a self-administered questionnaire, a self-report diary, and two drinking water samples. The questionnaire ascertained information on demographic factors (age, sex, and number of residents in house), other factors possibly predictive of AGII (living on a farm, presence of pets and livestock, recent travel and number of years at current residence), and tap water consumption. The diary determined the occurrence of AGII through a checklist of AGII symptoms.[15] To determine water contamination, two water samples were collected from each household, two weeks apart on day 8 and day 22 of the 28 day observation period. Water samples were immediately placed on ice in a cooler and transported within six hours of collection to the laboratory. Each water sample was collected in a sterile 300 ml bottle containing sodium thiosulphate and subsequently tested for total coliform, background bacteria, and E. coli within 24 hours of collection.[16] Total coliforms were enumerated by pouring the water sample through a cellulose acetate membrane filter, placed on an m-ENDO-LES agar plate and incubated at 35C for 24 hours. Background bacteria are associated with water pollution because of contamination by soil, sediment, fecal wastes and/or sewage. The background bacteria were counted as non-target colonies in the total coliform analysis. E. coli was enumerated by pouring 100 mls of the water sample through a cellulose ester membrane filter. The filter was placed on m-FC-BCIG agar and incubated at 44.5C for 24 hours. Exposure to contaminated water Exposure to contaminated water was defined using the water sample with the highest quantity of indicator bacteria per 100 ml. Subsequently, bacteriologically contaminated drinking water was examined according to the presence of any indicator bacteria, and the current standards set by the Ontario Ministry of Environment / Ontario Ministry of Health (>0 colony forming units (cfu) E. coli/100 ml or >5 cfu total coliform/100 ml).[17] To evaluate a dose response relationship three strata were created based on the mean of the two water samples. Categories were determined according to zero indicator bacteria in both samples, and the median of the remaining distribution of sample means. Acute gastrointestinal illness AGII was defined by the following combination of symptoms reported in the diary for any day during the 28 day study period: 1) vomiting or liquid diarrhea or 2) nausea or soft, loose diarrhea combined with abdominal cramps.[15] Episodes were further defined as one or more symptomatic days, with at least six consecutive symptom-free days between episodes.[15] For the purposes of this study, only the first episode was considered. Data analysis Odds ratios and 95% confidence intervals were used to quantify the relationship between AGII and bacteriological contamination of water supplies. Multivariable models were used to estimate the odds ratio controlling for important covariates. Covariates were selected through a backward stepwise regression. Covariates included in this process were age, sex, number of residents in the household, farm residence, livestock, pets, travel within the past six months, education and number of years at current residence. To adjust for the lack of independence between members of the same household, logistic-binomial regression for random effects was used.[18] As well, the indicator bacteria were categorized into three strata to investigate patterns of risk with respect to the degree of contamination. A test for trend was performed by treating this categorical representation as a continuous factor in the regression model. The influence of high background bacterial counts in the water samples was also examined by removing the samples having greater than 150 cfu/100 ml in the final logistic regression model. Background bacteria that exceeds 200 cfu/100 ml interferes with the analysis and interpretation of the total coliform test.[19] Although the Ministry of Environment's maximum acceptable limit for background bacteria on a total coliform analysis is 200 cfu/100 ml, some laboratory results were reported on a nominal scale (> 150 cfu/100 ml), therefore a cut-point of 150 cfu/100 ml was used to characterize samples with high background bacterial counts. Results Of the 327 households initially contacted by phone, 235 households (72%) representing 647 individuals agreed to participate in the study. Response rates for household completion of the questionnaire, diary and compliance with the water sample collections ranged from 92% for the diary to 99% for the water samples. Ninety-six percent of the households sampled were single family homes. Complete data, in terms of having a completed survey, and diary plus two water samples were available on 619 individuals. Demographic characteristics of the study population are depicted in Table 1. Table 1 Incidence of Acute Gastrointestinal Illness During a 28 Day Period Among Study Subjects Variable No Reported AGII Reported AGII N % N % Sex Female 272 90.7 28 9.3 Male 296 92.8 23 7.2 Age 50+years 189 96.9 6 3.1 1149 years 315 90.3 34 9.7 010 years 62 84.9 11 15.1 Education completed College/trade school/university 248 92.9 19 7.1 Grade school/highschool 316 90.8 32 9.2 AGII is acute gastrointestinal illness for specific defination, see methods Of the 235 private wells sampled, 91% of the wells were drilled wells, 3% were sandpoint or dug wells, 2% were bored and 4% were unsure of the well type. Twenty-nine percent of wells sampled were less than 11 years of age, 63% of wells were between 11 and 60 years of age, 3% were greater than 60 years of age and 5% of were unsure of the age of the well. Furthermore, sixty-four percent of wells were between 31 and 100 feet deep, 22% of wells were greater than 100 feet deep, 6% of wells were less than 30 feet deep and 8% were unsure of the depth of their well. Twelve percent of wells were located on a farm. According to current MOH standards, 17.1% of houses exceeded acceptable levels of total coliform and 9.5% exceeded acceptable levels of E. coli, for at least one water sample. In total, 20% of households had at least one sample exceeded MOH standards for safe drinking water. One or more episodes of AGII were identified in 51 of the 619 participants (8.2%). Table 2 presents the number of AGII events and adjusted odds ratios according to exposure to the indicator bacteria. The risk estimates are adjusted for age and years at current residence. No statistically significant association was observed for the indicator bacteria E. coli or for total coliform. The adjusted odds ratio for AGII among individuals living in households whose drinking water had E. coli present in at least one water sample compared to those with no positive water sample was 1.52 (95% CI 0.331.50). Using total coliforms that exceeded 5 cfu/ml as the measure of contamination, the adjusted OR for AGII was 0.39 (95% CI 0.11.50). After adjusting for the presence of indicator bacteria in the water supply, age and years of residence were both associated with acute gastrointestinal symptoms (Table 3). Children 10 years of age or younger were more likely to have acute gastrointestinal symptoms (OR 4.23, 95% CI 1.1116.18) and individuals who had lived for 10 or more years at their current address were less likely to report acute gastrointestinal symptoms. (OR 0.25, 95% CI 0.080.85). Table 2 Adjusted Measures of Association Exposure No AGII AGII Odds Ratio (95% CI)* E. coli 0 cfu/100 ml 523 46 1+ cfu/100 ml 45 5 1.52 (0.336.92) Total Coliform < 6 cfu/100 ml 483 48 6+ cfu/100 ml 85 3 0.39 (0.101.50) Total Coliform 0 cfu/100 ml 366 32 1+ cfu/100 ml 202 19 1.07 (0.422.69) *Adjusted for age and number of years at current residence. AGII is acute gastrointestinal illness for specific defination, see methods Table 3 Significant Covariates in the Final Logistic Regression Model Covariate Percent Rate of AGII Odds Ratio (95% CI)* (N = 619) per 100 /month Age 50+ 31.6 3.1 1.00 1149 56.6 9.7 3.24 (1.099.66) 010 11.8 15.1 4.23 (1.1116.18) Years of Residence 04 14.7 14.3 1.00 59 31.2 11.9 0.65 (0.202.17) 10+ 54.1 4.5 0.25 (0.080.85) *Adjusted for presence of indicator bacteria. AGII is acute gastrointestinal illness for specific defination, see methods Factors representing bacteriological contamination were categorized into ordered levels to investigate a dose response (Table 4). The pattern of risk observed for each factor was not consistent with an increased odds of developing AGII for each corresponding increase in the level of exposure. Table 4 Test for Trend using the Mean of the Two Water Samples Indicator No AGII Odds Ratio (95% CI)* Test For AGII Trend* E. coli 0 522 46 1.00 0.11.5 27 2 0.85 (0.107.19) 1.6700 18 3 2.69 (0.3421.56) P = 0.45 Total Coliform 0 366 32 1.00 0.12.5 103 16 2.00 (0.705.46) 2.67750 98 3 0.33 (0.071.52) P = 0.47 *Adjusted for age and number of years at current residence. AGII is acute gastrointestinal illness for specific defination, see methods Cold water consumption is assumed to be the primary route of exposure to contaminated water in this population. However, evaluation of the combined effects of exposure to contaminated water and volume of cold water consumed was limited by the small number of subjects in this study. The expected pattern of increased risk with increased water consumption among those using contaminated water was not observed. The presence of high background bacteria can influence the enumeration of the indicator bacteria. Typically, the number of cfu's for total coliform may be underestimated in the presence of high background bacteria. The agar used for the enumeration of total coliform is nonspecific and also supports the growth of background bacteria. Compared to the total coliform test, however, the agar used for the enumeration of E. coli is affected to a much lesser extent by the presence of background bacteria. One hundred and twenty observations with high background counts were removed leaving a sample size of approximately 499 observations. The point estimates of risk for E. coli and total coliform increased substantially but remained nonsignificant. (Table 5). Table 5 Association between Faecal Water Quality Indicators with Background Counts Less than 150 cfu/100ml and AGII Exposure No AGII AGII Odds Ratio (95% CI)* E. coli 0 cfu/100 ml 433 40 1+ cfu/100 ml 12 4 2.85 (0.978.39) Total Coliform < 6 cfu/100 ml 423 42 6+ cfu/100 ml 32 2 0.32 (0.034.08) Total Coliform 0 cfu/100 ml 332 29 1+ cfu/100 ml 123 15 1.77 (0.943.30) *Adjusted for age and number of years at current residence. AGII is acute gastrointestinal illness for specific defination, see methods Conclusions We found that approximately 20% of private wells are contaminated with coliforms sufficient to exceed current government standards for safe drinking water. This prevalence is similar to that seen in other studies in Canada[1,3,5,13] and the US.[4,6,7] These results confirm other studies and extend them because we have used a highly specific measure of fecal contamination (E. coli counts of 1 cfu/100 ml). Our results suggest that contaminated drinking water, according to government standards may pose a risk for AGII. Our study is limited mainly by its sample size. The key parameter driving the sample size was the relative risk estimate. The most comparable study in terms of exposure levels and outcome suggested that a relative risk of 2.8 for AGII could be expected in a comparison of individuals with contaminated supplies to those without.[15] The estimated sample size should have been able to detect a statistically significant association with an OR of at least 2.8. Our study used two water samples within a 28 day period to quantify drinking water quality. Although the quality of small, untreated water supplies may vary over a short period of time any reported episode of AGII in this study would be associated with a water sample taken at most 7 days from the episode. E. coli has been found to survive in drinking water anywhere from four to twelve weeks.[20] Nevertheless, the possibility of a differential exposure misclassification exists. Exposure misclassification may have arisen as well because we used fecal indicators that were currently used in Ontario. Some studies [11,21] have shown fecal streptococcus to be a better indicator of water quality. Lastly, the case definition used to define an episode of AGII was equivalent to the one used by Payment et al.[15] which found even mild episodes of AGII were associated with drinking water quality. The results of this study, therefore, may not be comparable to studies using a more rigid definition of AGII. None the less, the results provide hypotheses for further research. The difference in the risk estimates associated with the presence of E. coli versus total coliform may be contingent on the higher sensitivity and specificity of the E. coli measure for faecal contamination. The risk estimates associated with total coliform did not provide additional information in terms of health risk. In fact, the point estimate suggested that consuming water with total coliform of 5 or more cfu/100 ml was associated with fewer AGII symptoms, however, our sample size was too small to adequately pursue this hypothesis. These data tend to confirm results of other studies[8,11,13] and suggest that lower levels of indicator bacteria may represent an insufficient amount of microbial contamination to cause illness, that other bacterial indicators may be better associated with health risk and that some exposure to contaminated drinking water may in fact be protective for AGII. In addition, both older age and longer duration of residence were both associated independently with the presence of contaminated water, with a statistically significant less incidence of gastrointestinal symptoms. In each of these situations, the individual has had a greater chance of being exposed to bacteriologically contaminated groundwater and hence, a greater opportunity to develop resistance/tolerance to a number of enteric pathogens. This proposed immunity theory could have implications for individuals with no previous exposure to private well water, for the young and elderly and for immunocompromised individuals. This study confirms previous reports of a relatively high prevalence of bacteriological contamination of private wells in rural settings. The results of the study also suggest that consuming contaminated water characterised by the presence of E. coli and low background counts may not increase an individual's risk of AGII. This finding in concert with the protective effect of older age and greater number of years at current residence justify future research into mechanisms of resistance and tolerance and to possible harmful effects on immunocompromised individuals. The observed risk estimates for the indicator bacteria E. coli and total coliform warrants further investigation to provide evidence based information on their usefulness as a marker for nonpotable water and health risk and to provide evidence-based information to verify current government standards. Competing interests None declared. Pre-publication history The pre-publication history for this paper can be accessed here:
Background Breastfeeding rates in Scotland are very low, particularly in the more disadvantaged areas. Despite a number of interventions to promote breastfeeding very few women actually intend to breastfeed their baby. The aim of this study was to identify personal and social factors independently associated with intention to breastfeed. Methods Nine hundred and ninety seven women from two socio-economically disadvantaged housing estates located on the outskirts of Glasgow participated in a study that aimed to increase the prevalence of breastfeeding. Self-administered questionnaires completed by each participant collected information in early pregnancy, prior to exposure to the study intervention, on feeding intention, previous feeding experience and socio-demographic data. Results Five factors were independently predictive of breastfeeding intention. These were previous breastfeeding experience, living with a partner, smoking, parity and maternal age. After adjusting for these five factors, neither deprivation nor receipt of milk tokens provided useful additional predictive information. Conclusion In this population of socially disadvantaged pregnant women we identified five variables that were independently predictive of breastfeeding intention. These variables could be useful in identifying women at greatest risk of choosing not to breastfeed. Appropriate promotional efforts could then be designed to give due consideration to individual circumstances. Background Breastfeeding has been recognised as the optimal method of feeding the newbom infant [1,2]. It confers a number of health advantages while providing for optimal growth and development. In recent years, there have been increasing attempts to promote breastfeeding in Scotland and this is associated with a rise in breastfeeding from 35.6% in 19901 to 42.0% in 19978 (2.6% of the increase could be accounted for by an increase in maternal age) as measured at six postnatal weeks [3]. However, only around 50% of mothers express an intention to breastfeed [4]. The lowest incidence of breastfeeding occurs in some of the most disadvantaged urban areas. Children from disadvantaged areas are more likely to suffer illness in childhood, to place greater demands on health care services and to grow up to be unhealthy adults [5-7]. Even within a socially disadvantaged community, the population is heterogeneous with respect to factors that may influence infant feeding behaviour, including attitudes to breastfeeding. If we could predict which pregnant women were more or less likely to breastfeed within such a community, we could target health promotion interventions more precisely. This paper reports the results of an attempt to identify factors independently associated with intention to breastfeed by analysing data collected in the course of a community based intervention study. Methods A prospective controlled study to evaluate the impact of peer support on breastfeeding behaviour among socio-economically disadvantaged women was conducted from October 1994 until July 1997 [8]. Women were recruited at their first (booking) visit to the antenatal clinic. Nine hundred and ninety seven women were approached, and invited to participate in the study. The data were collected via a self-administered questionnaire covering feeding intention and previous feeding experience as well as demographic data. For the purpose of this paper the baseline data from the intervention and control groups were combined since the data were collected prior to commencement of the study intervention. The data were analysed in order to identify personal factors associated with a woman's intention to breastfeed. The outcome variable of interest was feeding intention. The variables identified as potentially being associated with feeding intention are summarised in Box 1. *Deprivation category (DEPCAT) is a small area based measure of deprivation based on % household with no car, overcrowding, male unemployment, low social class [9]. These variables were selected because they had been identified in previous studies as having an influence on feeding intention [4,10,11]. Univariate logistic regression analysis was used to assess individually the association between the various factors and feeding intention. Multivariate analysis based on stepwise logistic regression analysis was used to identify a set of independently useful predictors of feeding intention. Results Information was obtained from the 995 (99.8%) subjects who consented to participate in the study. Description of sample Of the 995 subjects, 926 (93%) lived in a highly disadvantaged urban community (DEPCAT 7) [9], 402 (41%) received milk tokens and 619 (62%) had smoked regularly in the 12 months prior to recruitment. Just under half (417) were primigravid (42%), 630 (63%) lived with a partner, 112 (11%) had previous experience of breastfeeding and 197 (20%) stated an intention to breastfeed. The age of the participants ranged from 1541 with an average age of 25 years. The average age of those intending to breastfeed was 26.3 compared to 24.9 for those not intending to breastfeed. Univariate analyses The results of the univariate analyses are summarised in Table 1. Table 1 Univariate Analysis Results Based on Logistic Regression Analysis Factor Odd Ratio C.I. for Odds Ratio p-value Comment Previous breastfeeding experience (Odds for Yes vs No) 5.26 3.49 to 7.96 <0.0001 Significantly more likely to intend to breastfeed if have previous experience Living partner (Odds Yes vs No) 2.05 1.44 to 2.94 <0.0001 Significantly more likely to intend to breastfeed if living with partner Regular smoker in the last 12 months (Odds Yes vs No) 0.56 0.41 to 0.77 <0.001 Significantly more likely to intend to breastfeed if non-smoker Receiving milk tokens (Odds Yes vs No) 0.55 0.39 to 0.77 <0.001 Significantly more likely to intend to breastfeed if not receiving milk tokens Age at booking visit (Increase in odds per 1 year advance in age) 1.05 1.02 to 1.08 <0.001 Significantly more likely to intend to breastfeed as age increases Deprivation Category (Odds for Depcat 7 vs 4) 0.46 0.28 to 0.80 <0.01 Significantly more likely to intend to breastfeed if in the lower deprivation category. Primigravid (Odds Yes vs No) 1.12 0.82 to 1.54 0.47 No significant difference in feeding intentions for the prim- and non-prim women Intention to breastfeed was significantly positively associated with previous breastfeeding experience, living with partner and increasing maternal age, and was significantly negatively associated with smoking, receipt of milk tokens and deprivation. There was no significant difference in feeding intentions for primigravid and multigravid women. Multivariate analyses The results of the multivariate analyses are shown in Table 2. Table 2 Results of Multivariate Stepwise Logistic Regression Analyses Factor Odds ratio C.I. for Odds Ratio p-value Previous breastfeeding experience (Odds for Yes vs No) 6.40 4.00, 10.31 <0.0001 Primigravid (Odds for Yes vs No 2.91 1.92, 4.46 <0.0001 Living with partner (Odds for Yes vs No) 1.92 1.29, 2.90 <0.01 Regular smoker in the last 12 months (Odds for Yes vs No) 0.61 0.44, 0.86 <0.01 Age at booking visit Increase in odds per 1 year advance in age) 1.05 1.01, 1.08 <0.01 Milk tokens (Odds for Yes vs No) 0.68 0.45, 1.01 0.06 Deprivation (Odds for 7 vs 4) 0.59 0.33, 1.04 0.14 The significance of each of the term in the final model is shown Five factors were independently predictive of breastfeeding intention. These were previous breastfeeding experience, living with a partner, smoking, parity and maternal age. After adjusting for these five factors, neither deprivation (p=0.14) nor receipt of milk tokens (p=0.06) provided useful additional predictive information. Discussion Our intention was to identify factors independently associated with intention to breastfeed. It is important to attempt to identify those women most at risk of choosing not to breastfeed in order to direct promotional efforts appropriately. The results of the univariate analyses agree with several other studies reporting an association of infant feeding intention with socio-economic status, maternal age, previous breastfeeding experience, maternal smoking [4,12-14] and living with a partner [15] and a lack of association between parity and feeding intention [13,14]. Other studies, however, have demonstrated an association between parity and feeding intention [4,12]. Most previous studies have identified socio-economic deprivation as being strongly associated with feeding choice. In our study, however, multivariate analysis indicated that socio-economic deprivation was not an independent predictor of infant feeding intention. This is consistent with research in the US reporting that breastfeeding was more strongly associated with maternal attitudes rather than deprivation per se [16]. However, some care must be taken in interpreting our results. Only 7% of the women in our study did not reside in a DEPCAT 7 area, and all of these lived in a DEPCAT 4 area. Our analysis may therefore have underestimated the association between deprivation and feeding intention. Different patterns may well have been seen had the study been based on a random sample of women from the full spectrum of deprivation categories. Multivariate analysis of the impact of milk tokens suggested that previous anxieties about the possible negative effects of milk tokens on breastfeeding may be unfounded [9]. However, our analysis, which demonstrated a borderline result, combined with the lack of published research into the impact of milk tokens on feeding choice suggests that there is a need for further research in this area. Previous breastfeeding experience and increasing maternal age were both independently associated with feeding choice. The fact that mothers who have previously breastfed are more likely to intend to breastfeed emphasises the need to support and encourage breastfeeding in first time mothers. The influence of the partner is likely to vary depending on the partner's attitudes to the feeding choice [17] and how influential he is in the relationship. Bryant [11] identified the partner as having a greater influence if he actually lived with the new mother. Our study did not measure the attitudes of the expectant father, but revealed a significantly positive relationship between choice of feeding and living with the partner. This may relate to physical environment and privacy. A study of low-income women in Glasgow [18] noted that the lack of a private place to breastfeed was a contributing factor to the mother's choice of feeding. Hally [15] also reported an association between not breastfeeding and overcrowding or living with the mother's family. This situation is more likely to occur where the mother lives at home with her parents and extended family. Smoking has also been shown to be associated with feeding choice in a number of other studies [13,14,19]. This has been generally attributed to deprivation status in that smoking is more prevalent among more socio-economically disadvantaged individuals. Our study found smoking to be independently predictive of infant feeding choice. An individual who smokes may represent an individual who has resisted health promotion attempts to address high smoking levels among women and particularly among pregnant women. Such an individual may also be resistant to attempts to promote breastfeeding as the healthy option. There is also some evidence to suggest that women who smoke think that they can not or should not breastfeed [20]. Conclusion In this population of socially disadvantaged pregnant women we identified five variables that were independently predictive of breastfeeding intention, namely: previous breastfeeding experience, living with a partner, non-smoking, being primigravid and increasing maternal age. These variables could be useful in identifying women at greatest risk of choosing not to breastfeed. Appropriate efforts could then be designed to give due consideration to individual circumstances that could influence receptiveness to certain health promotional messages. Competing interests None declared Pre-publication history The pre-publication history for this paper can be accessed here:
Background Investigating the validity of the self-reported values of weight allows for the proper assessment of studies using questionnaire-derived data. The study examined the accuracy of gender-specific self-reported weight in a sample of adults. The effects of age, education, race and ethnicity, income, general health and medical status on the degree of discrepancy (the difference between self-reported weight and measured weight) are similarly considered. Methods The analysis used data from the US Third National Health and Nutrition Examination Survey. Self-reported and measured weights were abstracted and analyzed according to sex, age, measured weight, self-reported weight, and body mass index (BMI). A proportional odds model was applied. Results The weight discrepancy was positively associated with age, and negatively associated with measured weight and BMI. Ordered logistic regression modeling showed age, race-ethnicity, education, and BMI to be associated with the degree of discrepancy in both sexes. In men, additional predictors were consumption of more than 100 cigarettes and the desire to change weight. In women, marital status, income, activity level, and the number of months since the last doctor's visit were important. Conclusions Predictors of the degree of weight discrepancy are gender-specific, and require careful consideration when examined. Background Individuals are often asked about common physical attributes such as weight and height in lieu of actual measurements. In some surveys and large epidemiologic studies, self-reported measurements of these characteristics may replace actual instrument-derived data. For instance, the National Health Interview Surveys, one of the oldest US national health surveys used to obtain data on the health of the resident, civilian, non-institutionalized population of the United States relies on the question "About how much [do you] weigh without shoes?". Previous studies have indicated that self-reported and actual weights correlate by more than 90% [1,2], but that more than 20% of adults underestimate their actual weight by 2 kilograms or more [1,3-5]. There is also a general overestimation of weight by overweight people and the elderly [6-8]. Recently, Kuczmarski et al.[9] examined the effect of age on the extent to which the body mass index (BMI) calculated from self-reported anthropometric information compared with estimates calculated from measured values. Using data from the Third National Health and Nutrition Examination Survey (NHANES III), the authors found that significant misclassification of overweight status could arise if self-reported values for height and weight were accepted as true. This was especially apparent in those of at least 60 years of age. Results from a Swedish population suggest that socioeconomic factors also play a role in these differentials [10]. The use of equations to predict measured variables from self-reported variables have been derived from regression models but have been unable to eliminate systematic error in their predictions [11,12]. Differences in findings across international settings also compound the problem. Results from a Scottish population seem to indicate that rates of misclassification are low [13] in contrast to those found in Wales [14], Australia [15], France [16], Spain [6], and Sweden [17]. Chiu et al.[18] suggest that a more comprehensive approach to the determinants of this misclassification phenomenon be examined. Investigating the validity of the self-reported values of weight allows for the proper assessment of studies using questionnaire-derived data. The same studies citing problems with the validity of self-reported measures also caution against their use in epidemiologic research [6,9,12,14-17]. While there is an understanding that certain specific characteristics (e.g., age, gender, socioeconomic status, etc.) are associated with misclassification, little has been written about the joint effect of a collection of presumed predictors and the degree of misclassification. In a sense, focus shifts from attempting to estimate measured variables from predictive coefficients derived from regression equations to qualitatively examining the relationship of the potential for misclassification given a set of putative covariates. This paper attempts to provide that information using a nationally-representative sample of US adults. Project investigators considering the use of self-reported values may be guided by these results in evaluating whether the potential for misclassification is present to an extent that suggests that measured values be derived. Specifically, this study assesses the effects of age, education, race and ethnicity, income, and general health and medical status on the degree of discrepancy between self-reported and measured weight in a nationally-representative sample of US adults. Methods Sample design Data were derived from NHANES III, conducted by the National Center for Health Statistics of the Centers for Disease Control and Prevention. NHANES III was designed to provide a nationally representative sample of the US population. The plan and operation of the NHANES III have been described elsewhere [19,20]. Briefly, the study was conducted from 1988 to 1994, and was composed of two 3-year phases: phase I from 1988 to 1991, and phase II from 1991 to 1994. To ensure reliable estimates, the oversampling of selected subgroups (including children, the elderly, Mexican-Americans, and non-Hispanic blacks) was performed. The current analysis combines both phases of the survey and focuses on individuals aged 17 and older. Individuals were interviewed at home and invited to participate in clinical examinations conducted either in the home or in a mobile examination center (MEC). Persons with missing weight data were excluded from the analysis, as were persons who had a proxy answer questions for them, and pregnant women. In some cases in which measured weight was not available during the clinical examination, an imputation procedure was performed that substituted an estimate of weight based on external characteristics or self-reported values. These cases were also excluded. All told, data from 15,944 subjects were available for analyses. No statistically significant differences were found between those excluded from the analysis and those retained (data not shown). Dependent variables Participants were asked how much they weighed "without clothes or shoes". Responses were given in pounds and converted to kilograms. Actual weight (in kilograms) was measured in the MEC using a Toledo 2181 self-zeroing digital weight scale which was calibrated at regular intervals. In the home examination, examiners were provided with a SECA Integra Model 815 Scale standardized against the MEC Toledo scale. All persons were asked to remove footwear and heavy outer clothing prior to measurement. The degree of discrepancy in weight was calculated as the difference between self-reported and measured weight, and was categorized into 7 levels (>10 kg too low, 510 kg too low, 24 kg too low, within 2 kg of actual value, 24 kg too high, 510 kg too low, and >10 kg too high). Independent variables The effects of four classes of variables were examined. Sociodemographic variables included sex, age, race-ethnicity, highest educational attainment, and annual family income. Weight perception related variables included questions about attempts to lose weight in the past 12 months, the desire to change weight ("Would you like to weight more, less, or stay about the same?"), participants' perception of current weight ("Do you consider yourself now to be overweight, underweight, or about the right weight?"), and level of activity ("How does the amount of activity you reported for the past month compare with your physical activity for the past 12 months? During the past month, were you more active, less active, or about the same?" and "Compared to most men or women your age, would you say that you are more active, less active, or about the same?"). Participants were also asked questions about their general state of health ("Would you say your health in general is excellent, very good, good, fair, or poor?"), length of time since their last visit to a health professional (including hospitalizations) to discuss their health, and the number of hospitalizations and visits to physicians during the past year. Anthropometric variables included calculation of BMI computed as weight in kilograms divided by square of height in meters using measured attributes, standing height, waist to hip ratio, and triceps and subscapular skinfold measurements. Statistical methods All analyses incorporated sampling weights consistent with the sampling design of the NHANES III survey in order to take into account the unequal selection probabilities resulting from the complex, multi-stage design of the study; adjustment for non-coverage and non-response; and the oversampling of subgroups [19]. Statistical analyses were carried out using Stata 7.0 [21]. Conventional statistical analyses with underlying distributional assumptions were inappropriate for variance estimation and statistical testing because of the multistage probability sampling design of the NHANES III. The use of conventional statistical analyses (which are based on simple random sampling) produces underestimates of the variance, thereby inflating statistical significance. Stata implements a method of variance estimation known as "linearization" in which linear approximates (ie., the estimated variance) of a nonlinear function (ie., the true variance) are derived by taking the first-order Taylor series of the approximation. An approximate F statistic incorporating an adjusted Wald test was used to compare continuous variables [22]. Design-based F statistics were calculated for cross-tabulations of categorical variables. The independent effects of particular independent variables on the degree of discrepancy were examined in univariable and multivariable models using regression analyses. A Type I error rate of five percent was used. Results Table 1 describes the characteristics of the study population. The mean (standard error [SE]) age was 43.14 (0.38) years, about 51.25% was female, and slightly more than three-fourths were non-Hispanic Whites with similar proportions reported completion of tertiary-level education or less, and annual incomes of less than $50,000. About half of the adult female population (44.45 percent) and almost two-thirds (61.74 percent) of the male population reported having smoked 100 cigarettes in the past. Table 1 General characteristics of males and females in the Third National Health and Nutrition Survey, 19881994. Characteristic Male (n = 9,401) Female (n = 10,649) Characteristic Male (n = 9,401) Female (n = 10,649) Age, years 42.28 (0.42)* 44.13 (0.48) Tried to lose weight in the past 12 months, % yes 29.14 (0.66) 49.84 (1.03) Race-ethnicity, % Trying to lose weight now, % yes 24.64 (0.95) 40.15 (1.11) Non-Hispanic White 76.02 (1.32) 75.39 (1.25) Would like to change weight, % Non-Hispanic Black 10.46 (0.55) 11.90 (0.78) Weigh more 12.67 (0.59) 4.72 (0.31) Mexican-American 5.72 (0.48) 4.80 (0.37) Weigh less 49.10 (0.89) 69.99 (0.78) Other 7.80 (0.92) 7.91 (0.79) Stay the same 38.23 (0.82) 25.29 (0.68) Highest educational attainment, % Perception of current weight, % Less than Primary 0.81 (0.14) 0.82 (0.17) Overweight 42.61 (0.84) 50.56 (0.90) Primary 6.22 (0.46) 5.97 (0.42) Underweight 8.08 (0.53) 4.36 (0.23) Secondary 20.66 (0.88) 19.31 (0.81) Just right 49.31 (0.75) 35.08 (0.88) Tertiary 45.90 (0.96) 53.26 (0.93) Level of activity with people of same age Beyond Tertiary 26.41 (1.16) 20.64 (1.01) More active 37.63 (0.95) 28.96 (0.89) Marital status, % Less active 18.65 (0.88) 25.26 (0.83) Never Married 23.24 (1.15) 17.67 (0.82) About the same 43.72 (0.94) 45.78 (0.69) Married 66.78 (0.96) 58.54 (0.92) Personal perception of health status Widowed 2.44 (0.21) 11.10 (0.56) Excellent 21.85 (0.80) 19.03 (0.88) Divorced 7.54 (0.47) 12.69 (0.61) Very good 31.38 (0.76) 30.26 (0.89) Annual income, % Good 32.42 (0.63) 33.35 (0.89) None 0.30 (0.11) 0.34 (0.14) Fair 11.25 (0.65) 14.01 (0.72) $1 to 9,999 9.00 (0.61) 13.48 (0.82) Poor 2.64 (0.19) 3.35 (0.28) $10,000 to 19,999 20.45 (0.80) 22.24 (0.77) Health status as assessed by medical doctor $20,000 to 29,999 16.97 (0.72) 16.85 (0.63) Excellent 47.79 (3.06) 45.80 (2.68) $30,000 to 39,999 15.79 (0.69) 13.52 (0.64) Very good 24.40 (2.10) 24.00 (1.78) $40,000 to 49,999 12.51 (0.75) 11.54 (0.69) Good 21.09 (2.16) 22.61 (2.43) $50,000 or over 24.98 (1.18) 22.02 (1.33) Fair 5.93 (0.57) 6.40 (0.57) Ever smoked cigarettes, % yes 61.74 (0.90) 44.45 (1.00) Poor 0.79 (0.12) 1.19 (0.19) Time since last visit to health professional, months 14.96 (0.44) 8.09 (0.30) Measured height, m 1.76 (0.00) 1.62 (0.00) Measured weight, kg 81.57 (0.36) 68.87 (0.38) Self-reported height, m 1.77 (0.00) 1.63 (0.00) Self-reported weight, kg 82.06 (0.33) 67.24 (0.35) Measured BMI, kg/m2 26.40 (0.11) 26.28 (0.15) * figures in parentheses are standard errors. body mass index = (weight in kilograms)/(height in meters)2 Almost 30 percent of males reported having tried to lose weight in the past 12 months with about one-fourth attempting to lose weight at the time of the interview. The proportions for females were about 50 and 40 percent, respectively. About half of the men interviewed (49.10 percent) said they wanted to weigh less than their current weight, while this figure for females was 69.99 percent. About half of the females interviewed felt that they were overweight, compared to only 42.61 percent of males. A quarter of all women and 19 percent of all men interviewed reported that they were less active than people of the same age. The distribution of levels of health perception by the participants and assessment by a medical examiner were similar for males and females. The average number of months since the last visit to a health professional was 15 months for males and 8 months for females. The mean BMI was 26 kg/m2 for both men and women. The average measured weight was 81.57 kg for males and 68.87 kg for females. Average self-reported weight was 82.06 kg and 67.24 kg for males and females, respectively. Using ordered logistic regression, the crude odds ratios of the association of the degree of discrepancy in weight and selected characteristics are given in table 2. Compared to the 30 to 39 age group, younger males and those greater than 80 years of age were more likely to overestimate their weight (p  0.01). In contrast, females of at least 60 years of age were more likely to overestimate their weight (p  0.01). In both sexes, increasing age was associated with a higher probability of discrepancy. Table 2 Odds ratios of crude ordered logistic regression analysis examining the probability of a discrepancy between self-reported and measured weight by selected characteristics. Third National Health and Nutrition Examination Survey, 19881994. Characteristic Male Female Characteristic Male Female Age, years Annual income < 20 1.401 (1.111, 1.750)** 1.039 (0.765, 1.411) None 0.781 (0.248, 2.435) 1.215 (0.499, 2.931) 2029 1.281 (1.092, 1.487)* 1.142 (0.912, 1.288) $1 to 9,999 1.236 (0.863, 1.769) 1.182 (0.938, 1.490) 3039 (Reference) 1.000 1.000 $10,000 to 19,999 1.036 (0.845, 1.270) 0.865 (0.717, 1.044) 4049 0.830 (0.678, 1.016) 1.004 (0.929, 1.074) $20,000 to 29,999 (Reference) 1.000 1.000 5059 0.951 (0.794, 1.139) 1.157 (0.934, 1.433) $30,000 to 39,999 0.921 (0.718, 1.181) 0.792 (0.665, 0.943)* 6069 0.927 (0.805, 1.067) 1.327 (1.125, 1.550)** $40,000 to 49,999 0.765 (0.629, 0.931)* 0.712 (0.616, 0.823)** 7079 1.061 (0.905, 1.232) 1.770 (1.457, 2.129)*** $50,000 0.659 (0.568, 0.765)*** 0.668 (0.597, 0.747)*** 8089 1.575 (1.299, 1.891)*** 2.188 (1.639, 2.892)*** > 90 2.439 (1.616, 3.681)** 4.761 (2.523, 8.894)*** Tried to lose weight in the past 12 months (versus "No") 0.488 (0.455, 0.524)*** 0.605 (0.560, 0.653)*** Race-ethnicity (versus Non-Hispanic While) Ever smoked cigarettes (versus "No") 1.273 (1.185, 1.353)*** 1.045 (0.913, 1.196) Non-Hispanic Black 1.657 (1.399, 1.943)*** 1.035 (0.906, 1.183) Mexican-American 1.157 (1.097, 1.209)* 1.171 (1.090, 1.245)* Trying to lose weight now (versus "No") 0.500 (0.485, 0.511)*** 0.631 (0.582, 0.684)*** Other 1.100 (0.945, 1.267) 1.288 (1.064, 1.543)* Would like to change weight (versus "Stay the same") Highest educational attainment Weigh more 1.925 (1.384, 2.651)*** 2.180 (1.253, 3.756)*** Less than Primary 0.720 (0.204) 2.484 (1.369, 4.508)** Weigh less 0.507 (0.484, 0.526)*** 0.465 (0.439, 0.488)*** Primary 1.364 (1.172, 1.571)** 1.598 (1.230, 2.055)*** Secondary 1.360 (1.282, 1.508)*** 1.325 (1.217, 1.428)*** Perception of current weight (versus "Just Right") Tertiary (Reference) 1.000 1.000 Overweight 0.515 (0.484, 0.548)*** 0.510 (0.485, 0.537)*** Beyond Tertiary 0.662 (0.594, 0.737)*** 0.912 (0.820, 1.014) Underweight 1.910 (1.368, 2.641)*** 2.335 (1.358, 3.974)*** Marital status (versus Married) Level of activity with people of same age (versus "About the same") Never Married 1.463 (1.250, 1.695)*** 1.077 (0.903, 1.285) More active 0.898 (0.781, 1.032) 1.183 (1.148, 1.207)** Widowed 1.341 (1.016, 1.751)* 1.718 (1.489, 1.965)*** Less active 0.855 (0.748, 1.177) 0.955 (0.897, 1.089) Divorced 1.256 (1.069, 1.461)* 0.947 (0.814, 1.101) Measured BMI, kg/m2 Personal perception of health status < 18.5 2.659 (2.022, 3.462)*** 3.642 (2.973, 4.461)*** Excellent 0.903 (0.767, 1.063) 0.899 (0.776, 1.041) 18.524.9 (Reference) 1.000 1.000 Very good 1.000 (0.950, 1.043) 0.977 (0.842, 1.134) 25.029.9 0.451 (0.420, 0.484)*** 0.532 (0.496, 0.571)*** Good (Reference) 1.000 1.000 30.034.9 0.260 (0.246, 0.275)*** 0.335 (0.312, 0.360)*** Fair 1.303 (1.113, 1.510)** 1.336 (1.146, 1.542)** 35.039.9 0.108 (0.103, 0.114)*** 0.239 (0.225, 0.253)*** Poor 1.536 (1.106, 2.111)** 1.376 (0.779, 2.429) 40 0.055 (0.053, 0.057)*** 0.129 (0.128, 0.134)*** Time since last visit to health professional, months 1.002 (0.990, 1.004) 0.997 (0.995, 0.999)* Measured weight, kg 0.962 (0.956, 0.968)*** 0.958 (0.954, 0.962)*** Health status as assessed by medical doctor Measured height, m 0.881 (0.510, 1.508) 0.057 (0.055, 0.060)*** Excellent 1.096 (0.889, 1.352) 1.197 (1.084, 1.308)* Very good 1.052 (0.850, 1.303) 0.987 (0.807, 1.208) Self-reported height, m 1.518 (0.521, 3.984) 0.093 (0.087, 0.099)*** Good (Reference) 1.000 1.000 Fair 0.908 (0.708, 1.165) 1.268 (0.811, 1.982) Poor 1.818 (1.253, 2.452)* 0.919 (0.644, 1.298) * p  0.05; ** p  0.01; *** p  0.001;  body mass index = (weight in kilograms)/(height in meters)2;  figures in parentheses are 95% confidence intervals. Non-Hispanic black and Mexican-American males were 66 percent (95% CI: 41% to 95%) and 16 percent (95% CI: 1% to 32%) more likely, respectively, to overestimate their weight compared to their non-Hispanic white counterparts. For females, non-Hispanic blacks were no more likely to report discrepancies in weight compared to non-Hispanic whites. However, Mexican-American women and women of other races were 17 percent (95% CI: 1% to 35%) and 29 percent (95% CI: 2% to 62%) more likely to overestimate their weight. The attainment of a primary education in males was associated with an increase of 1.364 (95% CI: 1.132 to 1.643) in the odds of overestimating measured weight by self report compared to those attaining a tertiary education. Secondary education was associated with an increase of a similar magnitude. Results were similar for females. The association between annual income and the degree of discrepancy in weight is statistically evident only in the upper income brackets. Compared to those earning $20,000 to $29,000 a year, males earning more than $40,000 a year are more likely to underestimate their weight (p  0.05). For females, this association is evident at a much lower income bracket ($30,000, p  0.05). Smoking more than 100 cigarettes in the past was associated with an increase of 27 percent (95% CI: 11% to 46%) in the chances of overestimating weight. This association was seen in males only. Participants who, in the past year, attempted to lose weight were more likely to underestimate their weight during the interview (males: OR = 0.488; 95% CI: 0.420 to 0.567; females: OR = 0.605; 95% CI: 0.532 to 0.689). Similar associations were found among those who were currently attempting to lose weight. Discrepancy in self-reported weight was found to reflect the desires and perceptions of the participants. The desire to weigh more and the perception of being underweight was each associated with a two-fold increase in the likelihood of overestimating weight during the interview. A similar two-fold increase in the chance of reporting a lower weight was seen when participants reported a desire to weigh less or when they perceived themselves to be overweight. Self-reported level of activity and the number of months since the last visit to a health professional were associated with a discrepancy in self-reported weight only in women. Compared to women who reported comparable levels of activity with people of the same age, women who considered themselves more active were 1.183 times (95% CI: 1.060 to 1.319) more likely to overestimate their weight. An increase in one year since the last visit to a health professional was associated with a 3 percent increase (95% CI: 0.3% to 6%) in the likelihood on underestimating one's weight. The participants' perception of their health status was associated with a disagreement between self-reported and measured weight only in the two worst levels. Compared to a rating of "good," males reporting their health as being "fair" and "poor" were 1.303 and 1.536 times more likely, respectively, to overestimate their weights (p  0.01). For females, this association was statistically evident only in those reporting "fair" health (OR = 1.336; 95% CI: 1.104 to 1.616). Similarly, a medical professional's assessment of health was associated with a discrepancy in self-reported weight only in the extremes of the scale. For males, this was apparent only in those whose health was assessed as being "poor;" in females, an association was found only in those with a rating of "excellent." Compared to those with a rating of "good," males with a "poor" rating were 1.818 times (95% CI: 1.102 to 3.000) more likely to overestimate their weights. Females with a rating of "excellent" were 1.197 times (95% CI: 1.015 to 1.413) more likely to overestimate their weights. BMI was associated with decreasing trends in the odds ratios in both sexes (p for trend < 0.0001). Using a BMI of 18.5 to 24.9 as the reference group, males and females with a BMI of less than 18.5 were 2.659 (95% CI: 1.522 to 4.646) and 3.642 (95% CI: 2.580 to 5.143) times more likely to overestimate their weights, respectively. Underestimation of weight was associated with higher BMI categories. For females, overweight, obesity I, II, and III statuses were associated with 47%, 66%, 76% and 87% increases in the likelihood of underestimating their weights, respectively. These values were 55%, 74%, 89%, and 94% for males, respectively. The application of a multiple ordered logistic regression model to the predictors produced the estimates of association shown in table 3. Separate models are given for males and females. For both sexes, important predictors of the degree of discrepancy in self-reported weight include age, race-ethnicity, highest educational attainment, and measured BMI. For males alone, additional predictors were cigarette smoking and the desire to change weight. For females, marital status, annual income, level of activity, and the length of time since the participant's last visit to a health professional were important. Table 3 Multiple ordered logistic regression of the predictors of the discrepancy between self-reported and measured body weight by sex in the Third National Health and Nutrition Examination Survey, 19881994. Characteristic Male (n = 9,401) Female (n = 10,649) Age, years < 20 0.703 (0.478, 0.955) * 0.558 (0.469, 0.664)*** 2029 0.997 (0.831, 1.196) 0.908 (0.751, 1.098) 3039 (Reference) 1.000 1.000 4049 0.920 (0.761, 1.113) 1.196 (0.960, 1.490) 5059 1.127 (0.903, 1.405) 1.607 (1.235, 2.070)*** 6069 1.000 (0.846, 1.181) 1.609 (1.198, 2.139)*** 7079 0.988 (0.801, 1.212) 1.707 (1.378, 2.094)*** 8089 1.174 (0.812, 1.697) 1.851 (1.226, 2.767)*** 90 1.474 (0.575, 3.741) 4.297 (3.390, 5.882)*** Race-ethnicity (versus Non-Hispanic White) Non-Hispanic Black 1.471 (1.254, 1.708)*** 1.369 (1.186, 1.565)*** Mexican-American 1.035 (0.864, 1.240) 1.421 (1.226, 1.631)*** Other 0.933 (0.764, 1.139) 1.310 (1.043, 1.629)* Highest educational attainment Less than Primary 0.652 (0.417, 1.019) 1.826 (0.303, 10.996) Primary 1.415 (1.162, 1.705)** 1.358 (1.068, 1.709)* Secondary 1.288 (1.135, 1.447)** 1.283 (1.118, 1.458)** Tertiary (Reference) 1.000 1.000 Beyond Tertiary 0.627 (0.564, 0.697)*** 0.839 (0.755, 0.933)** Marital status (versus Married) Never Married 1.009 (0.785, 1.297) DROPPED Widowed 1.085 (0.906, 1.299) Divorced 0.822 (0.714, 0.947)* Annual income None 1.293 (0.636, 2.604) $1 to 9,999 1.108 (0.954, 1.274) $10,000 to 19,999 0.916 (0.763, 1.099) DROPPED $20,000 to 29,999 (Reference) 1.000 $30,000 to 39,999 0.905 (0.744, 1.101) $40,000 to 49,999 0.749 (0.639, 0.878)** $50,000 0.709 (0.625, 0.804)*** Ever smoked cigarettes (versus "No") 1.190 (1.003, 1.411)* DROPPED Would like to change weight (versus "Stay the same") Weigh more 1.500 (1.289, 2.273)** DROPPED Weigh less 0.988 (0.846, 1.153) Level of activity with people of same age (versus "About the same") More active DROPPED 1.088 (0.945, 1.253) Less active 1.224 (1.129, 1.314)** Time since last visit to health professional, months DROPPED 0.996 (0.992, 0.999)* Measured BMI, kg/m2 < 18.5 2.161 (1.847, 2.529)** 3.930 (3.370, 4.583)*** 18.524.9 (Reference) 1.000 1.000 25.029.9 0.475 (0.434, 0.520)*** 0.419 (0.393, 0.447)*** 30.034.9 0.262 (0.244, 0.248)*** 0.254 (0.235, 0.274)*** 35.039.9 0.100 (0.095, 0.105)*** 0.186 (0.176, 0.196)*** 40 0.051 (0.049, 0.053)*** 0.086 (0.084, 0.088)*** * p  0.05; ** p  0.01; *** p  0.001;  body mass index = (weight in kilograms)/(height in meters)2;  figures in parentheses are 95% confidence intervals. Adjustment of other covariates reversed the direction of effect of age in males and females. Compared to those aged 30 to 39, males and females aged less than 20 years were found to be 20 (95% CI: 6% to 47%) and 44 percent (95% CI: 23% to 60%) more likely, respectively, to underestimate their weights. No other statistically significant associations were found for other age group in males. Females older than 60 years, however, were more likely to overestimate their weights with the oldest age group being 3.3 times more likely than those in the reference group. The adjusted effect of race-ethnicity in males was similar to the crude estimates. For females, however, non-Hispanic blacks, a group previously found to be no more likely to misreport their weights during the analysis of crude effects, were shown to have 1.369 times (95% CI: 1.145 to 1.636) the odds of overestimation compared to their non-Hispanic white counterparts. The estimates for Mexican-Americans and women of other groups were only slightly increased after adjustment. The estimated effects of BMI and highest educational attainment after adjustment were similar to estimates of their crude effects, as were the adjusted effects of cigarette smoking and desire to change weight in males, and annual income and the length of time since the participant's last visit to a health professional in females. Utilizing these figures, estimates of the proportions of males and females in the NHANES III by the degree of discrepancy between self-reported and measured weight stratified by age are given in table 4 and represented graphically in figure 1. Overall, more than 35 percent of males overestimate their weight by 2 kilograms or more; in females, this proportion is only about 14 percent. About a 35 percent of all females underestimate their weight, compared to about 25 percent for males. Although the proportion of people correctly reporting their weight to within 1 kilogram was approximately constant throughout the age range, a greater proportion of the elderly was shown to overestimate, and of the young to underestimate, their weights. Figure 1 Proportions with discrepancies between self-reported and measured weight, by extent of discrepancy, sex, and age in the Third National Health and Nutrition Examination Survey, 19881994. Table 4 Crude and adjusted proportions with discrepancies* between self-reported and measured weight, by extent of discrepancy, sex, and age in the Third National Health and Nutrition Examination Survey, 19881994. Age (years) > 4 kg less 23 kg less Within 1 kg 23 kg more >4 kg more Mean Discrepancy (kg) N ESTIMATES OF EFFECT AFTER ADJUSTMENT Males < 20 7.31 (0.35) 11.85 (0.40) 38.25 (0.35) 23.53 (0.42) 19.06 (0.61) 1.06 (0.17) 508 2029 8.40 (0.34) 12.68 (0.27) 38.14 (0.29) 22.48 (0.29) 18.30 (0.43) 0.70 (0.18) 1,579 3039 10.37 (0.43) 14.77 (0.26) 39.76 (0.26) 20.27 (0.30) 14.82 (0.35) 0.37 (0.11) 1,439 4049 12.29 (0.44) 16.47 (0.33) 40.09 (0.27) 18.48 (0.35) 12.67 (0.39) -0.03 (0.26) 1,193 5059 10.39 (0.28) 15.25 (0.26) 40.29 (0.28) 19.83 (0.28) 14.24 (0.38) 0.23 (0.14) 846 6069 10.25 (0.40) 15.22 (0.31) 40.58 (0.21) 19.85 (0.33) 14.09 (0.36) 0.29 (0.11) 1,152 7079 9.29 (0.36) 14.18 (0.36) 39.72 (0.26) 20.98 (0.38) 15.82 (0.48) 0.52 (0.18) 852 8089 6.29 (0.23) 10.81 (0.28) 37.17 (0.39) 24.22 (0.26 21.51 (0.60) 1.38 (0.17) 611 90 3.98 (0.33) 7.62 (0.56) 32.41 (1.33) 26.75 (0.47) 29.24 (1.92) 2.18 (0.48) 53 All Ages 9.93 (0.19) 14.42 (0.14) 39.46 (0.13) 20.66 (0.15) 15.52 (0.18) 0.42 (0.09) 8,233 Females < 20 20.86 (0.91) 20.43 (0.37) 46.01 (0.74) 9.80 (0.44) 2.90 (0.17) -1.69 (0.35) 570 2029 17.77 (0.44) 19.40 (0.22) 48.61 (0.34) 10.95 (0.27) 3.28 (0.11) -1.79 (0.20) 1,771 3039 21.57 (0.59) 20.88 (0.23) 45.77 (0.50) 9.16 (0.27) 2.62 (0.11) -1.72 (0.14) 1,801 4049 20.28 (0.70) 20.35 (0.20) 46.69 (0.60) 9.81 (0.24) 2.88 (0.10) -1.64 (0.21) 1,311 5059 18.05 (0.58) 19.14 (0.33) 47.81 (0.44) 11.44 (0.38) 3.56 (0.16) -1.38 (0.12) 970 6069 15.24 (0.39) 17.96 (0.26) 49.89 (0.38) 12.81 (0.31) 4.11 (0.16) -0.91 (0.13) 1,125 7079 12.87 (0.41) 16.16 (0.29) 51.25 (0.33) 14.84 (0.35) 4.88 (0.17) -0.37 (0.12) 945 8089 9.55 (0.34) 13.55 (0.32) 51.99 (0.32) 18.27 (0.45) 6.65 (0.30) -0.16 (0.13) 662 90 3.73 (0.31) 6.53 (0.48) 44.20 (1.64) 29.41 (1.01) 16.13 (1.60) 1.38 (0.42) 81 All Ages 18.32 (0.36) 19.30 (0.13) 47.83 (0.27) 11.12 (0.17) 3.43 (0.07) -1.41 (0.08) 9,236 * Discrepancy = (self-reported weight)  (measured weight)  Percentages may not sum to 100 due to rounding  Figures in parentheses are standard errors Adjusted for race-ethnicity, highest educational attainment, and BMI category. For males, additional covariates included cigarette smoking in the past and desire to lose weight. For females, additional covariates included marital status, annual income, level of activity, and time since last visit to a health professional. Discussion The systematic differences between self-reported and measured values of weight were documented by previous studies [1,3-5]. However, the lack of population-based samples made estimates difficult to generalize to larger population groups or to special subgroups. In this study, a nationally-representative sample was used to derive estimates of this bias and its determinants. The main finding suggests that personal attributes are associated with the tendency of adults to differ in their reporting of their correct weight. Overall, there was a general underreporting of weight by about half a kilogram. However, males overestimated their weights by half a kilogram and females underestimated their weights by almost 1.5 kilograms. These personal attributes were often gender-specific, a bias that extended to the predictors of the degree of discrepancy between the two measures. The results may be regarded as one of the instances in which gender-specific interaction is demonstrated [4]. While there were covariates that affected both sexes, the magnitudes and directions of effect were often not similar. Only the youngest males, for instance, showed any statistically significant probability of reporting weight discrepancy compared to the reference group and after adjustment for other covariates. In females, almost the entire age range is influenced. Some covariates were statistically significant in one sex but not in the other as was, for instance, level of activity in females or cigarette smoking in males. Variables were selected on the basis of a statistically significant association with the degree of discrepancy. While the use of a proportional odds model clearly delineates the importance of these predictors in terms of relative odds, this study was limited in its examination of the absolute magnitude of these discrepancies (i.e., the number of kilograms reported versus measured) after accounting for the study's complex sampling design and adjustment for multiple covariates. Some variables served as non-specific indicators of lifestyle and health behaviors. The better quantification of some of these behaviors (such as smoking or level of activity) may lead to a better characterization of their association with self-reported weight. The length of time since the participant's last visit to a health professional is particularly nettlesome, as one can premise that individuals seeking consultation for health conditions are more likely to be cognizant of their weight, especially in particular medical conditions such diabetes or cardiovascular disease. The inclusion and simultaneous adjustment of an additional set of variables, however, was deemed inadvisable from a model-building standpoint. Therefore, generalizations of these results to these subgroups must be tempered with caution. This study supports previous research in suggesting that the biases associated with self-reported weight precludes its use as an accurate surrogate for measured values in epidemiologic studies and field trials [6,9,12,14-17]. However, the results also suggest a finding that was not altogether unexpected  that in certain subgroups (e.g., the obese elderly) where accuracy is of prime concern, biases may be particularly significant. The implications for practice are clear: project investigators should endeavor to gain accurate measures of weight in adults where feasible. The ease of acquisition of self-reported values must be weighed against the overall utility of an analysis that incorporates potentially biased estimates of effect. Careful deliberation about the resource implications of such an endeavor and its likely gain in accuracy must also be carried out. Future research is necessary to examine the generalizability of these findings in other settings. Research might be targeted to better understand the impact of surrogate measures in well-defined population groups. It is hoped that by generating a fuller picture of the complex relationships that impact on self-reported weight and weight perception, a better understanding of the issues surrounding overweight and obesity may be gained. Conclusions Self-reported measures of body weight in adults are associated with gender-specific biases. The careful and deliberate consideration of their usefulness as surrogates of instrument-derived measures is important, especially when particular subgroups are involved. Competing interests None declared. Pre-publication history The pre-publication history for this paper can be accessed here:
Background Widespread scepticism persists on the use of the Under-Privileged Area (UPA8) score of Jarman in distributing supplementary resources to so-attributed 'deprived' UK general practices. The search for better 'needs' markers continues. Having already shown that Council Tax Valuation Band (CTVB) is a predictor of UK GP workload, we compare, here, CTVB of residence of a random sample of patients with their respective 'Jarman' scores. Methods Correlation coefficient is calculated between (i) the CTVB of residence of a randomised sample of patients from an English general practice and (ii) the UPA8 scores of the relevant enumeration districts in which they live. Results There is a highly significant correlation between the two measures despite modest study size of 478 patients (85% response). Conclusions The proposal that CTVB is a marker of deprivation and of clinical demand should be examined in more detail: it correlates with 'Jarman', which is already used in NHS resource allocation. But unlike 'Jarman', CTVB is simple, objective, and free of the problems of Census data. CTVB, being household-based, can be aggregated at will. Background The 'Under-Privileged Area 8' (UPA8) score, commonly known as the 'Jarman Index' was developed in the early 1980s [1,2]. It is built on eight socio-economic factors, available from UK Census returns, that were reported by a poll of British general practitioners as, in their opinion, creating high workload. The degree to which it succeeds as a predictor of the clinical burdens of UK general practices and as a marker of socio-economic status remains under scrutiny [3]. In any case it was not designed as a marker of deprivation [1] and using it for this purpose is seen as seriously flawed [4-9]. However, 'Jarman' is still used, now at enumeration district level, to justify supplementary resource allocation ('deprivation payments') to some UK general practices [10]. The 1992 Local Government Finance Act [11] introduced into the UK the 'Council Tax'. Under its provisions Local Authorities demand annual payments ' in respect of any chargeable dwelling in their jurisdiction.' Payments are scaled according to banded assessments (made independently), of the market value of the dwellings in which citizens live. Like the UPA8 score, the 'Council Tax' was never intended as a basis for marking deprivation. Nevertheless we recently proposed the Council Tax Valuation Band (CTVB) of an individual's residence as a proxy marker both of socio-economic position and of clinical demand. We based our assertions on the results of our epidemiological study based in general practice [12] which showed that there is a strong correlation between CTVB and other markers of deprivation such as Townsend score [13] and that the CTVB of a patient's residence is also a systematic predictor of UK general practitioner workload. Ours is the only such analysis that we have been able to find in the literature, despite a very assiduous search, except for an ecological study of Council Tax Benefit recipients published in 1995 [14]. Unlike UPA8 score, CTVB is not Census-based and is applied at household level. But comparing these two 'ugly ducklings' is perfectly possible: this analysis tests the hypothesis that CTVB and UPA8 score are correlated. Materials and Methods A 5% age and sex-stratified study sample was randomly selected from the 11,300 patients of a semi-rural English general practice. Demographic data (age, sex, address, postcode) extracted from the practice database were linked to socio-economic data (marital and employment status, owner-occupancy, rooms and persons per house, access to motor vehicles) obtained from postal questionnaires sent to each of the 565 study individuals [12]. Non-responders were re-mailed or contacted in person as necessary. The residence CTVB of each respondent was obtained from the official 'Lists' of the relevant Local Authorities (North Wiltshire and Kennet). The UPA8 scores for the relevant enumeration districts (via published postcode/ED indices) were obtained from the Department of Primary Care and General Practice of Imperial College of Medicine [2]. Statistics CTVBs were correlated (Spearman) with UPA8 scores using SPSS (version 10.0). Results and Discussion 478 questionnaires were finally returned and suitable for socio-economic analysis, a response of 84.6%. There were no significant socio-demographic differences between responders and non-responders. After omitting residents of nursing and residential homes (who pay no council tax) there were 465 subjects for full analysis. There are 30 enumeration districts in the 6 Calne electoral wards, UPA8 scores between -29.00 and +34.90. Study UPA8 scores were divided into equal bands for tabulation purposes and these and the mean aggregated UPA8 scores for each study CTVB are shown in Table 1. Table 1 Study individuals (n.465) cross-tabulated according to UPA8 score (banded equally) and CTVB showing the mean (with 95%confidence intervals) UPA8 score for each CTVB. UPA8 scores Total Mean UPA8 95% conf int -35 to -20 -20 to -5.0 -5.0 to+10 +10 to+25 +25 to +40 CTVB A 3 10 32 0 6 51 +1.64 -2.47, +5.75 B 3 31 89 19 3 145 -0.23 -1.98,+1.52 C 7 57 49 6 0 119 -5.26 -6.92,-3.60 D 18 39 22 2 0 81 -10.68 -13.25,-8.12 E 15 19 7 0 0 41 -13.95 -16.34,-10.84 F 5 10 1 0 0 16 -13.45 -16.92,-9.98 G 4 5 3 0 0 12 -10.89 -16.33,-5.46 H 0 0 0 0 0 0 - - Total 55 171 203 27 9 465 Correlation coefficient between CTVB and UPA8 is significant -0.423 (95% confidence intervals: -0.32 to -0.49), CTVB increasing as UPA8 reduces (both moving, thereby, in the direction of reducing deprivation), In other words CTVB correlates with UPA8 and the study hypothesis is supported. But although we have shown that CTVB and UPA8 appear to measure, in parallel, the socio-economic status of individuals, the two markers are fundamentally different in their derivation. CTVB does not suffer from several inherent drawbacks that affect 'Jarman': (i) CTVB is an official and objective statistic independent of the views and possible prejudices of clinical personnel [15]; (ii) CTVB is not Census-based; (iii) CTVB is not, therefore, subject to under-enumeration [16]; (iv) CTVB does not go out of date - new and extended homes are assessed for CTVB at 1991 prices [11] (v) CTVB is free of the ecological fallacy [17] since it relates to individual households and not geographical locations that may be socio-economically heterogeneous; (vi) CTVB will not, therefore, be prone to the underestimation of deprivation influence as alleged for ecological measures [18]; (vii) CTVB requires no tortuous mathematical assumptions and modifications [19]. In other words, CTVB not only correlates with 'Jarman' but could have many inherent advantages. And computerised linkage between all patients' residential addresses and the Council Tax Valuation Band lists is perfectly feasible - the latter are archived electronically and are already publicly available on demand from Local Authority Housing Departments. Further, they will be posted on the internet from early 2002 (Brown, T - CTVB lists manager, Wessex. personal communication). It is also perfectly possible to weight CTVBs to match the differing spectra of house prices in the UK regions, available from regularly updated official statistics [20], so that they became a universal attribute, valid when comparing between UK regions as well as within small areas. The authors recognise, however, that this is a small study and that the data are from a single English general practice. Many questions arise, reminding us that this work should be seen as preliminary. Conclusions We therefore suggest that the CTVB of residence of individual UK general practice patients could replace the 'Jarman' score derived for those individuals in determining (i) the socio-economic footprint and (ii) the clinical demands likely to be made of each and every UK general practice, irrespective of the scatter of patients' homes. Unlike 'Jarman', CTVB could also be aggregated to match any geographical boundary, such as that of a Primary Care Trust or a District Hospital catchment area. CTVB might be the basis, therefore, of more equitable resource allocation to primary care teams and secondary health services and is worthy of more study. Funding NHS Executive, R& D Support Grant. Competing interests None declared. Pre-publication history The pre-publication history for this paper can be accessed here:
Background Gaps in disease surveillance capacity, particularly for emerging infections and bioterrorist attack, highlight a need for efficient, real time identification of diseases. Methods We studied automated records from 1996 through 1999 of approximately 250,000 health plan members in greater Boston. Results We identified 152,435 lower respiratory infection illness visits, comprising 106,670 episodes during 1,143,208 person-years. Three diagnoses, cough (ICD9CM 786.2), pneumonia not otherwise specified (ICD9CM 486) and acute bronchitis (ICD9CM 466.0) accounted for 91% of these visits, with expected age and sex distributions. Variation of weekly occurrences corresponded closely to national pneumonia and influenza mortality data. There was substantial variation in geographic location of the cases. Conclusion This information complements existing surveillance programs by assessing the large majority of episodes of illness for which no etiologic agents are identified. Additional advantages include: a) sensitivity, uniformity and efficiency, since detection of events does not depend on clinicians' to actively report diagnoses, b) timeliness, the data are available within a day of the clinical event; and c) ease of integration into automated surveillance systems. These features facilitate early detection of conditions of public health importance, including regularly occurring events like seasonal respiratory illness, as well as unusual occurrences, such as a bioterrorist attack that first manifests as respiratory symptoms. These methods should also be applicable to other infectious and non-infectious conditions. Knowledge of disease patterns in real time may also help clinicians to manage patients, and assist health plan administrators in allocating resources efficiently. Introduction Public health agencies, medical care delivery systems, and clinicians all depend on accurate and timely information about disease occurrence to guide planning, resource allocation, and case management. In the public health arena, the current United States infectious disease surveillance infrastructure was recently the subject of an extensive review by the US Department of Health and Human Services [1]. That review identified substantial weaknesses in existing capacity to detect four major threats to public health  emerging infections, antimicrobial resistance, bioterrorism, and pandemic influenza. A major recommendation of that report was to engage with the health care delivery system as a partner in surveillance. Specifically, it made a "level 1" recommendation for funding of efficient, easy-to-use, and rapid automated reporting systems based on national standards. Such automated reporting systems can take two basic forms  they can facilitate clinicians' active reporting of individual cases of diseases of interest, or they can use some of the extensive information that is already collected in automated form in the process of medical care delivery or in administration of medical care benefits. We focus here on an example of the latter approach, assessing automated information about diagnoses assigned by clinicians during ambulatory care visits. To our knowledge, automated medical record information has not been used for rapid surveillance purposes. Such an approach has several potential advantages. These include the facts that it imposes no reporting burden on clinicians and avoids the reporting biases inherent in spontaneous reports, and the data can be available promptly. In addition, they can assess the very large number of cases of illness for which no etiologic diagnosis exists, and which are therefore typically not reportable. These illness syndromes can be of interest because they might provide several days' advance warning of a serious problem, such as anthrax, and also because they make use of the large body of information in encounters for which no diagnostic testing is performed. Currently, this information is typically unavailable for epidemiologic purposes, although the tracking of influenza like illness in sentinel practices is an example of such syndromic surveillance. The automated medical records of health plans and large group practices are an especially useful source of surveillance data because they serve a well-defined population of members, they have responsibility for most aspects of health care, and they routinely collect clinical, demographic and accounting data. This paper describes some of the technical and methodological issues encountered in developing a surveillance system for lower respiratory infection based on automated ambulatory care electronic encounter records from a large HMO and multi-specialty group practice. Methods We studied information in the automated medical records, demographic, and eligibility records of individuals cared for by Harvard Vanguard Medical Associates (HVMA), a large multi-specialty group practice in eastern Massachusetts. At the beginning of the study period, HVMA was a staff model component of Harvard Pilgrim Health Care (HPHC), a not-for-profit HMO, and all individuals studied were members of Harvard Pilgrim. Ambulatory care, including scheduled, same day, and urgent care visits, is delivered in fourteen health centers in greater Boston. Urgent care visits include many of the encounters that are cared for in hospital emergency rooms in other practice settings. All individuals included in this study were HMO members who had a strong financial incentive to receive their care at one of these health centers. Care in the health centers is delivered using automated medical records. At present, Epicare, a commercially available system is used. Clinicians assign diagnoses for each visit from lists that correspond to ICD9CM codes [2]. The system also contains vital signs and all providers' full text notes. An earlier automated medical record system in use during the first part of the study period used COSTAR[3] diagnosis codes, which we mapped to ICD9CM codes. Individual patient identifiers were replaced with study-specific, encrypted unique identifiers before the data were made available for analysis. The study was approved by the HPHC Institutional Review Board. A total of 7,265,523 encounter records, including routine, scheduled, and urgent care visits, plus telephone calls that clinicians chose to record, were collected during the four years between the beginning of 1996 and the end of 1999. Each encounter was coded by the physician at the time of consultation with up to 11 diagnosis codes. The proportion of visits assigned multiple ICD9CM codes decreased steeply after the first two of these, ranging from 10.2% (744,290) of visits with three codes and 7.4% (540,049) of visits with four codes, to 0% (0) with eleven. Since we were interested in a primary diagnosis of lower respiratory infection, we considered only the first two diagnostic codes. Emergency room visits were excluded for two primary reasons: they are not uniformly captured, and they enter the system after some delay, thereby greatly diminishing their utility for any real time surveillance system. Membership data for the same period were extracted from administrative data systems. The number of days of membership over the four years of the study for each individual member was determined. These were summed by age and gender and used as the denominators for calculating crude and age/gender specific incidence rates. ICD9CM codes were grouped into syndromes for the purposes of the main analysis. A provisional set of ICD9CM codes for Lower Respiratory Infection (LRI) currently in use (personal communication, J. Pavlin; Appendix 1) by the US Department of Defense ESSENCE project [4] were used. Any encounter record with one of the relevant codes in either of the first two ICD9CM codes assigned during an encounter was selected for analysis. After selecting all LRI records, encounters for each individual were grouped into episodes of illness on the assumption that for any individual patient, a subsequent visit for LRI within six weeks of a preceding LRI visit would be likely to represent follow-up for the same infection, whereas widely separated LRI visits probably represented separate events. The data supporting the six week cut off is shown below. After these analyses were completed, the health plan discovered that some clinical data were missing in master files from which the analysis datasets were created. Membership information was not affected by this problem. Corrected data were not expected to be available for several months. Analysis of a random sample of 500 individuals' data showed that the results reported here undercount individuals with LRI, LRI episodes, and LRI encounters by less than 10%, with no identified pattern to the missing information. The data are therefore presented with the understanding that complete figures would include an additional 10% of counts; this difference should not affect the substantive conclusions. Results There were 501,323 individuals who were members for at least part of the four year study period and thus eligible to present for ambulatory care. The median duration of membership was 2.2 years (mean = 2.3 years, s.d. = 1.5 years), yielding 1,143,208 person-years of observation. The age and sex distribution of observation time is shown in Table 1. Table 1 Age and sex distribution of lower respiratory infection encounters and person-time. Age Episodes Years Rate Rate 95% CI ratio group (M/F) F M F M F M F M <1 3304 4252 32060.5 33032.2 0.1031 0.1287 1.25 1.191.31 14 6572 7523 25051.3 26424.2 0.2623 0.2847 1.09 1.051.12 514 9860 10517 76412.1 79600.7 0.1290 0.1321 1.02 1.001.05 1524 4658 3495 57973.5 44746.9 0.0804 0.0781 0.97 0.931.01 2534 7029 3841 135954.1 113350.0 0.0517 0.0339 0.66 0.630.68 3544 8896 5690 120094.0 110367.8 0.0741 0.0516 0.70 0.670.72 4554 7540 5162 79264.2 74378.0 0.0951 0.0694 0.73 0.700.76 5564 4544 3105 34273.8 32041.1 0.1326 0.0969 0.73 0.700.76 6574 8654 7138 26123.8 20967.0 0.1409 0.1154 0.82 0.790.85 7584 2205 1387 10840.1 7312.5 0.2034 0.1897 0.93 0.871.00 >85 596 393 2023.0 917.0 0.2946 0.4286 1.45 1.281.65 During this time, 152,435 ambulatory care encounters were assigned one or more of the codes in the LRI syndrome definition (see Appendix 1). Although 119 different ICD9CM diagnoses contribute to the lower respiratory syndrome category, three codes accounted for more than 90% of all encounters (Table 2). These codes were cough (ICD9CM 786.2), pneumonia not otherwise specified (ICD9CM 486), and acute bronchitis (ICD9CM 466.0). Notably, these are codes that clinicians can assign without performing laboratory testing. Table 2 Most common ICD9CM diagnoses in lower respiratory infection encounters. Individual episodes can have two of these diagnoses. ICD9CM Description Count Percentage of Total 786.2 Cough 62634 52.8 486 Pneumonia, organism not otherwise 27681 23.4 specified 466.0 Acute bronchitis 18286 15.4 466.1 Acute bronchiolitis 5594 4.7 487.1 Influenza with respiratory 1902 1.6 manifestations, not elsewhere classified The 152,435 LRI encounters were attributable to 75,747 individual members who experienced a mean of 2.0 encounters each (minimum = 1, maximum = 65). More than half of these individuals (n = 43,404) had exactly one LRI encounter. For the 32,343 individuals with more than one LRI encounter, the distribution of intervals between LRI encounters is shown in Figure 1. In more than half of individuals with multiple LRI encounters, a second encounter occurred less than three weeks after the first. Approximately 8,000 members had a second encounter more than one year after the first. To avoid double counting of ambulatory care encounters that were really part of the same episode of infection, we used a criterion based on clinical experience together with the distribution shown in Figure 1. If any individual member had more than one LRI encounter, we required 6 weeks free of any LRI diagnosis encounter before attributing a new episode of LRI illness to that individual. Using this criterion, the 152,435 encounters for LRI were reduced to 106,670 distinct episodes of lower respiratory infection, giving an overall annual incidence rate of episodes of LRI coming to medical attention of 93/1,000 person-years. Figure 1 Frequency distribution of intervals between multiple LRI encounters for individual members with more than one encounter over the study period. Annual LRI episode incidence rates are shown in Table 1 and Figure 2 by age group and gender. These incidence rates are highest among pre-school children after the first year of life and among those 85 years and older. In those categories, the rates for boys and men exceeded those for girls and women, by 25% and 45%, respectively. In contrast, among adults from 25 through 74 years, the rates among women were 18% to 34% higher than those in men, with the largest excess in the younger age groups. The confidence intervals for these rate ratios excluded the null in each of the age categories, except 524 and 7584 years. Figure 2 Incidence rate (events/person-year) of lower respiratory infection episodes diagnosed in ambulatory care records. As expected, there were many more LRI episodes during the winter. The weekly counts of these episodes are shown in Figure 3. The seasonal variation in these counts is strikingly similar to the variation in deaths from pneumonia and influenza in 122 cities, reported by the CDC [5], also shown in Figure 3. For most winters, there is a suggestion that the LRI episode counts rise shortly before the peak in deaths. Figure 3 Weekly counts of pneumonia and influenza deaths reported to CDC from U.S. cities, including greater Boston (dashed black line, left hand vertical axis) and lower respiratory infection episodes diagnosed in greater Boston ambulatory settings (solid red line, right hand vertical axis). Because an important surveillance goal is to detect certain events, such as an anthrax exposure, as rapidly as possible, we show daily, rather than weekly, counts in Figure 4. The left hand panel, with over 1,400 data points, also makes evident the overall seasonal variation in disease incidence, though with considerably more scatter. This figure also shows that diagnoses are much less likely to be assigned on weekend days. The right hand panel, which shows daily counts for a single month demonstrates more clearly the day to day variation in occurrence of LRI episodes. The lower weekend counts are seen, usually followed by the week's highest counts on Mondays. However, the overall day to day variation in counts is relatively large, making it difficult to discern important overall trends in disease incidence. This difficulty is evident even in this best case situation in which all of the events in greater Boston are aggregated. Figure 4 Daily counts of lower respiratory infection episodes. The left hand panel shows all years' data. Counts for Monday through Friday are shown in black; counts for Saturdays and Sundays are shown in red. The right hand panel shows the same data for a single month. In Figure 5 we show disease patterns in space as well as time. Episodes are assigned to the census block group in which the individual resides. The four panels in the figure show intervals ranging from the entire four years to a single day. For intervals as small as a week, the overall geographic clustering of episodes, corresponding to the concentration of residents who receive care from Harvard Vanguard, is reasonably well preserved. The small number of events in any locale is well demonstrated, however. A similar display of rates, rather than counts would show much less variation. Figure 5 Distribution of lower respiratory infection episodes in time and space. Each episode is mapped to the census block group in which the individual resided. Four durations are shown: 4 years (upper left), one month (upper right), one week (lower left), and a single day (lower right). Note that scale of the vertical axis is different for the upper left panel, compared to the other three. The x-axis is longitude, and the y-axis is latitude. The area with no episodes on the right border of each plot is water. Counts are shown rather than rates to correspond to the data shown in Figures 3 and 4. Variations between census tracts are principally due to the distribution of health plan members' residence locations. Discussion Routinely collected medical record information has several important advantages for surveillance purposes, and particularly for the surveillance of infection syndromes. Among the most important is the ability to assess the large majority of episodes of illness for which no etiologic agents are identified, either because good medical practice does not require that clinicians perform diagnostic tests, or because an unusual agent may fail to be detected by standard laboratory tests. Syndromic surveillance should thus be useful both for tracking the activity of infectious agents that are common in communities, and also for identification of new, emerging infections or bioterrorist attack. The detection of an increased frequency of events would typically trigger more intensive assessment, including more diagnostic testing than would ordinarily be indicated. Syndromic surveillance also allows the earliest possible identification of increased disease frequency, presumably days before laboratory test results become available. This early indication of a problem may be important in detecting and responding to a bioterrorist attack, for instance the release of anthrax in a community. Other advantages of automated diagnosis data for surveillance include uniformity and increased sensitivity of detection, since clinicians are not required to recognize a condition as being of interest. These data also circumvent the need for providers to initiate reporting, an important consideration in light of the time pressures that constrain existing reporting. For some purposes, automated methods may augment or replace resource intensive sentinel surveillance programs, for example those described by Armstrong [6] and de Wit [7], that have been created to collect information that isn't captured by standard reporting systems. Finally, data from automated medical record systems lend themselves well to incorporation in automated detection systems with little or no added cost, because the data are available in electronic form, avoiding the additional costs and errors due to data entry. For a data source to be useful for disease surveillance, it must be timely, accurate, complete, and capable of distinguishing events of interest from background occurrences, i.e., an acceptable signal to noise ratio. The interval between a patient being seen and data being available for analysis must be short, particularly in any bioterrorist threat situation. Diagnostic and demographic accuracy are needed in order to enable reliable evaluation of geographic clustering of specific emerging infections or syndromes. Complete data or at least reasonably complete sampling is essential if events of relatively small scale are to be detected. In terms of timeliness, this ambulatory record information can be available very quickly. In practice, it is efficient extract each day's visits of interest during the succeeding night, thus making the information available within 24 hours of the patient encounter. In terms of accuracy, the data are probably of acceptable reliability for patient demographics and encounter dates, since this information comes from the administrative database used for reimbursement. The validity or reliability of physician diagnosis in terms of ICD9 codes is neither known nor readily amenable to measurement. In other settings, ICD9 codes have been shown to have substantial discrepancies, when they are compared to the information in the full text medical record [8]. We expect that the diagnoses of interest here also have substantial errors that reduce both sensitivity and specificity. It is likely that these errors are relatively stable over the time periods of interest for surveillance of acute disease syndromes, and so this problem may not interfere appreciably with day to day comparisons. However, there could be important differences attributable to coding practices between groups of clinicians, or in different medical record systems, for instance if the automated systems guide clinicians to choose certain codes over others. The lack of uniformity in the use of ICD9 codes, for instance in assigning a diagnosis of pneumonia, may be ameliorated by grouping diagnoses into broader syndromic surveillance categories, as was done for this report. It is notable that the relatively non-specific diagnosis of "cough" accounted for more than half of LRI encounters studied here. At this time, we have no simple way to measure directly the accuracy of coding or of directly assessing the effect of syndromic groupings. No standard syndromic grouping is yet in wide use for surveillance purposes. The provisional grouping used in this report was developed by the U.S. Department of Defense for its own specific needs but appears to have worked reasonably well with the ambulatory care data described here. Because a small number of relatively non-specific codes (cough, pneumonia, bronchitis) account for the large majority of episodes of lower respiratory infection, it is likely that most syndrome groupings will yield very similar results. Although the inclusion of symptoms like cough clearly reduces specificity, we believe this is outweighed by the gain in sensitivity. This tradeoff is discussed in more detail below. In terms of completeness, an automated system like the one we describe here is typically as complete for ambulatory encounters as the records that clinicians maintain. Our belated discovery that some encounters of interest were missing from the analysis dataset adds an important cautionary note, however, about the potential problems in adapting data designed for one purpose to another one. We believe the omission of emergency room visits has a minor impact on the total number of events, since their number is small in relation to the total number of ambulatory visits. However, surveillance based in emergency rooms is also of great value. We see the system described here as being complementary to emergency room based systems. A potential advantage of assessment of office visits is the possibility that it will provide an earlier signal than will an emergency room based detection system if the condition of interest begins with symptoms that don't warrant emergency room care. The use of data from an automated medical record system in a health care environment linked to individuals' insurance coverage provides an additional reason to believe the data are reasonably complete, since individuals have a strong financial incentive to receive their care from providers whose clinical encounter information is reported to the insurer for reimbursement purposes. It may also be possible to make similar use of diagnoses contained in automated billing data, rather than automated medical records. Although most current administrative systems include time lags that diminish their utility, the development of on-line transaction processing between clinicians and payers may reduce or eliminate that deficiency. All medical records or claims based systems depend, of course, on individuals' bringing the event to clinicians' attention. Such systems provide no information about the large number of illnesses that resolve without formal contact with the medical system. Even large health plans typically include only a portion of individuals in a community. Although the number of individuals may be sufficient for surveillance purposes, it will be important to assess the degree to which the covered population resembles the entire population. Insured populations are likely to be adequate for many conditions of interest, especially if one adjusts for major determinants of illness. An additional advantage of using automated data from health plans is the ability to know the exact size, composition, and residence location of the source population. Although we limited our characterization of the population to age and sex, it is also possible to use more detailed information about disease history, for instance to characterize the burden of illness among individuals with specific chronic diseases. Locating clusters of illness should have great utility for identifying and remedying localized disease sources; these might be locations, such as day care facilities, where infection is transmitted person to person, or areas in which there are environmental sources of infection, such as a contaminated water supply. The geographical information available to health plan is primarily useful, of course, for conditions whose source is near individuals' homes. Primary medical records will include multiple encounters within a single episode of infection for some patients. The decision to define a new episode on the basis of six weeks free from any LRI coded encounter was based on a combination of clinical experience and the pattern of repeated encounters. For surveillance systems to be comparable, the classification of encounters into episodes of illness is an important methodological problem that deserves additional attention. It is possible, for instance, that the pattern of repeat visits might change during a cluster of illness. Although the distribution of LRI visit intervals supports a six week disease free interval to become eligible for a second episode, other cutoffs might also have been chosen. It seems reasonable to assume that if similar patterns of illness in time and space are observed in other systems (such as specific disease surveillance systems, hospital discharge records, or other large ambulatory care record systems), then this provides some degree of criterion validity for the data presented here. The striking similarity in seasonal variation between our LRI episodes and the national experience with pneumonia and influenza deaths provides one measure of assurance that our system identified relevant events. We also compared the CDC's pneumonia and influenza death data for Boston to our experience, but the number of reported deaths was too small for meaningful seasonal patterns to emerge. Additional support for the utility of medical record surveillance information comes from comparison of our data to that collected by the National Ambulatory Medical Care Survey (NAMCS) [9], which uses multistage sampling of ambulatory care physicians, requiring participants to report a random sample of patients seen in a randomly assigned week. For the period 1990 to 1996, the estimated rate for lower respiratory infection office visits was 74.2/1000 population per year, based on about 40,000 sampled records per year nationally. This estimate is reasonably close to our observed rate of 93/1000. The difference between the two rates may be due in part to sampling variation (principally arising from the smaller NAMCS sample), differences in the age/comorbidity profiles of the populations, greater sensitivity of the HVMA sample because it included telephone encounters, and lack of specificity of the cough diagnosis. In any event, the difference, even if real, should not seriously interfere with the utility of this syndromic surveillance system to identify overall disease trends or provide early warning of illness clusters. Similarly, the age and sex distribution of these cases is consistent with our knowledge of the epidemiology of lower respiratory tract infection. Others, using data from an office practice, have shown an early childhood peak at approximately one year, also with higher rates in males of approximately 25% [10]. The increasing rate with age among adults has been widely recognized, along with an overall male predominance [11-13]. Some studies have reported either a smaller difference between men and women among younger adults [12], or an excess among younger adult women [11], as observed in our population. Our data do not distinguish between actual differences in disease incidence by age and sex, and differential ascertainment, either because of differences in likelihood of seeking health care or differences in the way clinicians code encounters for men and women. In order to distinguish signals of interest from background occurrences, we believe it will be necessary to develop statistical methods to identify unusual clusters that deserve further attention. The volume of data acquired is so large that it is impractical to perform manual daily inspection of data from a large geographically dispersed population. This is especially important since there were only twelve lower respiratory infection syndrome clusters each year of more than approximately five events occurring in a single day among health plan members residing in a single census tract (authors' unpublished data). The specific number of events required to be included in the twelve most extreme clusters depended on the number of health plan members in a census tract, as well as the month of the year, and the day of the week. The fact that relatively few events occurred on any particular day in any census tract supports our inclusion of the "cough" diagnosis in the syndrome definition, in order to improve the sensitivity of our case finding. Although this is a non-specific diagnosis, and it accounted for a majority of all events, the total number of these was not so large that it compromised the utility of this particular surveillance system. An enhancement that may be useful in automated medical record systems, but not in claims based systems, is to require fever (measured value, not ICD9 code) to be part of the definition of a lower respiratory infection. This would presumably preserve sensitivity for conditions like anthrax, and also reduce the number of false positive clinical events. To the extent this surveillance method proves useful, it will be worthwhile to extend it to other conditions that cluster in the areas of residence of affected individuals. Within infectious diseases, these might include diseases spread by airborne dissemination in residential areas, by contaminated foods or water distributed to residents of a neighborhood, by insect or other animal vectors, or by person-to-person transmission in households (secondary spread). Specific infection syndromes of interest, in addition to lower respiratory infection, include upper respiratory infection, gastrointestinal disease, neurologic disease, and rash. It may also be useful for other conditions that may be clustered in time or space, such as injuries. We conclude that as automated ambulatory care record systems become more widely available, they can assume an important, currently unfilled, role in disease surveillance. Such systems are less prone to undercounting than traditional public health reporting systems, and they are less resource intensive than traditional sentinel surveillance systems. These data can serve several different purposes, including informing clinicians of conditions that are prevalent in their communities, providing detailed and timely information to health plans that need to allocate scarce resources, and to public health programs to allow early recognition and response to changing disease patterns. Suitably de-identified electronic data could be provided to public health systems in a format consistent with the emerging National Electronic Disease Surveillance System (NEDSS) standards [14]. Inclusion of such reporting capability, under clinicians' and health systems' control, in commercial medical record systems is likely to be an inexpensive way to provide the required data in the most usable form. The timely use of automated diagnosis information, especially with cluster detection algorithms, may be a valuable resource for supplementing current infectious disease surveillance systems. List of abbreviations HMO: Health maintenance organization HPHC: Harvard Pilgrim Health Care HVMA: Harvard Vanguard Medical Associates ICD9CM: International classification of disease, 9th version, clinical modification. LRI: lower respiratory tract infection NAMCS: National Ambulatory Medical Care Survey NEDSS: National Electronic Disease Surveillance System Competing interests None declared Pre-publication history The pre-publication history for this paper can be accessed here: Supplementary Material Appendix 1 List of ICD9 codes contributing to the lower respiratory infection syndrome group. Click here for file
Background The aim of the study was to measure knowledge about the symptoms, prevalence and natural history of stroke; the level of concern about having a stroke; understanding of the possibilities for preventing stroke, and the relationship between age, sex, country of origin, educational level, income, self-reported risk factors, and the above factors. Methods A random sample of households was selected from an electronic telephone directory in Newcastle and Lake Macquarie area of New South Wales, Australia, between 10 September and 13 October 1999. Within each household the person who was between 18 and 80 years of age and who had the next birthday was eligible to participate in the study (1325 households were eligible). The response rate was 62%. Results The most common symptoms of stroke listed by respondents were "Sudden difficulty of speaking, understanding or reading" identified by 60.1% of the respondents, and "paralysis on one side of body" identified by 42.0% of the respondents. The level of knowledge of the prevalence of a stroke, full recovery after the stroke, and death from stroke was low and generally overestimated. 69.9% of the respondents considered strokes as being either moderately or totally preventable. There were few predictors of knowledge. Conclusion The study suggests that educational strategies may be required to improve knowledge about a wide range of issues concerning stroke in the community, as a prelude to developing preventive programmes. Background Stroke remains a leading cause of death, long-term disability, and health care expenditure, but opportunities exist for reducing its cost to the community [1]. Recent advances in the treatment of acute ischemic stroke offer hope in reducing its devastating effects [2,3]. Primary prevention is also an important approach to substantially reducing the prevalence, recurrence, disability, and mortality of stroke [4-7]. Previous studies have shown that the knowledge of stroke among patients and in the community is poor. Pancioli and colleagues, [8] in a population based survey, found that 57% of respondents identified at least one of five established warning signs of stroke, 28 % correctly listed two or more, and only 8% correctly identified three established warning signs of stroke. Our previous study [9] showed that only 73.4% of respondents from a random population sample identified the brain as the organ affected by stroke. While 76% of the respondents could list at least one risk factor for stroke, only 50% could identify one of the warning signs for stroke, 26% could correctly list two, and only 9% of respondents could correctly identify three or more warning signs. There have been considerable efforts to increase knowledge about stroke and early presentation at the hospital when stroke symptoms occur. Alberts et al.[10] reported a highly significant improvement in presentation times for patients with cerebral infarction after the implementation of a community education program that used a multimedia approach. Before the program, 37% of patients with cerebral infarction presented to either a hospital or were referred from a general practitioner within 24 hours. After the program, the proportion of patients rose to 86%. Stem et al.[11] also reported 10.9% increase in stroke awareness and knowledge between the pre-education group and the post-education group in their community education programs. Adequate planning of community interventions to prevent people from having their first stroke and to present at the hospital early after stroke symptoms occur will require a comprehensive multidisciplinary strategy. In the past, however, most researchers have focused on identifying the poor level of knowledge about stroke risk factors, symptoms, and treatment in hospital patients and in the general population, and there are no published data on the understanding of perceptions and beliefs about stroke in the community. This information is also a good resource for developing a public education program to reduce the occurrence of stroke and appropriate responses to the stroke symptoms. This study aimed to measure in a random sample of the population: knowledge about stroke symptoms, prevalence, and natural history level of concern about having a stroke understanding of the possibilities for stroke prevention the relationship between age, sex, country of origin, educational level, income, self-reported risk factor, and the above factors. Methods Sample and setting A random sample of 1773 households in Newcastle and Lake Macquarie, Australia, a regional city of approximately 250,000 people was randomly selected from an electronic telephone directory between 10 September and 13 October 1999. Within each household the person who was between 18 and 80 years of age and who had the next birthday was eligible to participate in the study. Procedure One week after mailing an information letter, a trained telephone interviewer contacted each selected household to conduct a telephone interview. If the eligible household member was not available to complete the interview, arrangements were made to call back at a later time. Three attempts were made to contact each eligible household member during the survey period. Measurement A literature review of previous studies concerning the chance of having a stroke and the knowledge of stroke symptoms, prevalence, mortality, and full recovery identified potential items for the survey instrument [6,7,12-15]. From this review a draft instrument was developed that addressed the knowledge of stroke symptoms, prevalence, mortality and full recovery. The final survey instrument contained 22 items divided into 3 sections: Knowledge and perception about stroke. These items addressed knowledge and perception regarding stroke symptoms, prevalence, mortality, and full recovery. Response options for each of these items were closed-ended questions. Respondents' demographic details (age, gender, marital status, country of origin, education, income), Prevalence of self-reported risk factors for stroke among the respondents (high blood pressure, angina, heart attack, previous stroke, diabetes, high cholesterol, smoking, and family history of stroke). This study was approved by the Newcastle University and Hunter Area Research Ethics Committees. Statistical Analysis Descriptive and comparative statistical analyses were performed using the statistical program SAS version 6.12. Chi-squared tests were used to assess the relationships between components of stroke-related knowledge, demographic characteristics and self-reported risk factors. The effects of demographics and the presence of risk factors on the participants' knowledge of stroke were evaluated separately using logistic regression analyses. For each model, response options for the dependent variable were categorised as either 'know' or 'do not know'. P-values from Wald statistics were used to assess the significance of predictor variables. Missing data were excluded. Two-tailed significance tests were used, and a probability value of less than 0.05 was considered statistically significant in both univariate and multivariate analyses. Results Sample A total of 1773 telephone calls were made and of those, 1325 households were eligible. The remainder of the calls were ineligible (due to fax number, business phone number, disconnected, or no answer). A total of 822 participants completed the questionnaire giving a response rate of 62%. Demographic characteristics of the respondents are presented in Table 1. Respondents were asked whether they had been told by health care professionals that they had a risk factor for stroke. Of the respondents, 258 (31.4%) reported a high blood pressure (hypertension), 48 (5.8%) diabetes, 24 (2.9%) a previous history of stroke, 35 (4.3%) angina, 206 (25.1%) a high blood cholesterol, 180 (21.9%) current smokers, and 356 (43.3%) a family history of stroke. Table 1 Demographic characteristics of the respondents (n = 822) Age (mean  SD) 48.9  16.1 Sex> Male 335 (48.0%) Female 487 (52.0%) Country of origin Australia 715 (87.0%) Overseas 70 (13.0%) Education Primary 19 (2.3%) Secondary 514 (62.6%) Tertiary 279 (33.9%) Income (A$) Less than $20,000 252 (30.7%) $20,000 to $39,999 189 (24.8%) $40,000 to $59,999 142 (15.3%) More than $60,000 132 (12.1%) Don't wish to answer 107 (13.0%) Knowledge of stroke symptoms Table 2 shows that the most common symptoms of stroke listed by respondents were "Sudden difficulty of speaking, understanding or reading" identified by 494 respondents (60.1%), and "paralysis on one side of body" identified by 393 respondents (42.0%). Six hundred and ninety four (85.5%) respondents correctly listed at least one stroke symptom, 489 (60.2%) correctly listed 2 symptoms, and 253 (31.1%) correctly listed 3 or more symptoms of stroke. In the final logistic regression model, only higher educational level was a significant predictor of knowledge of stroke symptoms (p = 0.006, OR; 1.3, 95%CI; 0.670.93). No statistically significant interactions were found between demographic factors and self-reported risk factors or the knowledge of stroke symptoms. Table 2 Respondent's knowledge of stroke symptoms (n = 822) Responses N (%) Sudden difficulty of speaking, understanding or reading 494 (60.1%) Paralysis any part of body 229 (36.4%) one side of body 393 (42.0%) Blurred or double vision, loss of vision in an eye or both 243 (29.6%) Incoordination or imbalance 243 (29.6%) Loss of memory 125 (15.2%) Brain malfunction 76 (9.0%) Numbness, tingling sensation, dead sensation any part of body 55 (6.7%) one side of body 63 (7.7%) Weakness any part of body 53 (6.4%) one side of body 74 (5.1%) Difficulty in swallowing 33 (4.0%) Dizziness (vertigo) 24 (2.9%) Headache / migraine 20 (2.4%) Chest pain, chest tightness, or murmur 12 (1.5%) Don't know 54 (6.5%) Knowledge of stroke prevalence, full recovery, mortality The level of knowledge of the prevalence of stroke in the community, the chance of full recovery after a stroke, and death from a stroke was generally low (Table 3). To the question "out of 100 Australians, how many do you think will have a stroke in their life?" only a few respondents (20.7%) were within 10% of the correct Australian rate. [16] Approximately half of the respondents over-estimated the prevalence of stroke in Australia by at least 50%. The level of knowledge of stroke prevalence in this study is lower than similar study of telephone survey regarding knowledge of breast cancer in Australian women. This study showed that that one-third of respondents were able to make an approximately correct estimate of the incidence of breast cancer [17]. In the univariate analysis respondents with higher educational level (p = 0.001) and males (P = 0.02) were more likely to estimate the likelihood of stroke within 10% of the correct answer. Table 3 Respondents perception about the likelihood of developing a stroke, full recovery, and death after the stroke. Response categories to question on likelihood (persons / 100) Australia rate (persons / 100)* 010 1130 3150 51100 Developing a stroke (n = 729) 1 20.7% 30.6% 29.2% 19.5% Full recovery after the stroke (n = 731) 33 22.9% 19.6% 34.2% 23.4% Death from the stroke (n = 719) 9 26.7% 40.8% 23.4% 9.2% * : Australian Institute of Health and Welfare (1999). When asked: "out of 100 Australians who have had a stroke, how many of them do you think will die as a result of stroke?" Approximately one third of respondents were within 10% of the Australian rate [16]. More than a half of the respondents over-estimated the risk of death after having a stroke. In the univariate analysis, respondents who had completed tertiary education were more likely to estimate within 10% of the correct answer than those who had not completed tertiary education (p = 0.004). Respondents with a history of high blood pressure were more likely to over-estimate the risk of death after a stroke than those without (p = 0.003). However, in the final logistic regression model, there was no association between demographic factors or self-reported risk factors and knowledge of stroke prevalence and mortality in Australia. The beliefs of respondents about the likelihood of having another stroke after the first stroke varied widely. 72 respondents (8.8%) rated the likelihood the same as for a person who has not had a stroke, 399 respondents (48.5%) rated the likelihood at 2 to 3 times, and 281 respondents (34.2%) at more than 4 times more likely. Sixty-seven (8.2%) respondents considered strokes as not preventable at all, 125 (15.2%) respondents as slightly preventable, 506 (61.6%) respondents as moderately preventable, 68 (8.3%) respondents as totally preventable, while 56 (6.8%) respondents did not know. Younger respondents (aged 1839, p = 0.001) and those who had completed tertiary education (p = 0.04) estimated more correctly than those aged over 40 years and those who had not completed tertiary education. Concern about the possibility of having a stroke When asked about the chance of having a stroke during their lifetime ("What do you think are your chances of having a stroke in your life time?"), 651 respondents (91.1%) reported no or only a low chance of their developing a stroke. Twenty-two respondents (3.1%) reported a high lifetime chance of developing a stroke. More men reported moderate or high chances than did women (p = 0.04). Respondents in each of these groups were more likely to indicate a low lifetime risk of stroke: Older Age (aged 6080; p = 0.001). Lower Educational (completed primary school; p = 0.001), and Born Overseas (p = 0.01). Respondents who had been told by a health care professional that they have high blood pressure (p = 0.008), heart disease (p = 0.03), or previous stroke history (p < 0.001), and having a family history of stroke (first & second degree relatives; p = 0.001), were all more likely to think they had high chance of having a stroke in their lifetime than were respondents in other groups. To the question "In the last 12 months have you been at all concerned about the possibility that you might have a stroke?", more than half of respondents reported that they had not been concerned about their chance of having a stroke during the previous 12 months (Table 4). Younger respondents were more likely than older respondents to report that they were 'never' concerned about having a stroke in the last 12 months (p = 0.003). Respondents who had been told by a health care professional that they had high blood pressure (p = 0.001), diabetes (p = 0.004), or a high cholesterol level (p < 0.001) were more concerned about having a stroke than those who had not been so told. Respondents born in Australia were more likely to express concern about stroke than those born overseas (p = 0.03). Respondents with previous history of stroke (p < 0.001) or family history of stroke (p = 0.001) had experienced concern more often during the previous 12 months than did those without either personal or family stroke history. Table 4 Concern about the possibility of developing a stroke (n = 822) Frequency N (%) Never 599 (68.0%) Occasionally 164 (19.9%) Sometimes 52 (6.3%) Always 35 (4.2%) Don't know 12 (1.5%) Discussion This community-based study demonstrates aspects of public knowledge and perception about stroke. Of 822 respondents, 694 (85.5%) were able to name at least one established stroke symptom. Respondents, in general, overestimated both stroke prevalence in Australia and the chance of full recovery after the stroke. Respondents generally considered the possibility of their having a stroke during their life as being not a matter of serious concern. In a multiple logistic regression model, only one group  those with a higher level education  had better knowledge of established stroke symptoms. "Sudden difficulty of speaking, understanding or reading" was listed as the most common stroke symptom and more than 80% of respondents could list at least one established stroke symptom. In a hospital-based prospective study Kothari et al.[18] reported that unilateral weakness and numbness were the stroke symptoms most frequently noted by patients. Two-fifths of patients could not identify a single symptom of stroke. In the final logistic regression model, there was no significant association between self-reported stroke risk factors and knowledge of stroke symptoms. These results suggest that respondents who recognise in themselves an established stroke symptom still need to be educated by their doctor or through community educational programs. Although those with stroke risk factors (high blood pressure, diabetes, heart disease, previous stroke history etc) correctly identified themselves as having a higher risk of stroke [12], the older respondents incorrectly identified themselves as having a lower risk of stroke. Providing the older members of the community with information about how to recognise stroke symptoms and how to access emergency system in the event of their experiencing stroke symptoms may be beneficial. Furthermore, it would seem advisable to evaluate the role of denial in health behaviour and to develop and test educational strategies to address this denial. Prevention remains the most important approach to substantially reducing the prevalence, recurrence, disability, and mortality of stroke. Respondents showed a positive attitude to the possibility of stroke prevention. Gorelick[19] discussed reduction of stroke risk. In his review, lifelong behaviours controlling risk factors are best achieved during pre-adolescence or adolescence. Randall et al., and Kingsley et al. [20,21] emphasised the necessity of finding methods to educate and motivate the population to reduce risk factors, and the importance of educating patients about their risk factors after examining them in the primary care setting. A comparatively low proportion of respondents expressed concern about their lifetime chance of having a stroke. Respondents informed by health care professionals that they had a stroke risk factor were more concerned about the possibility of having a stroke than those who had not been told. However, those with stroke risk factors had no better knowledge of stroke symptoms than those without risk factors. Even if overall attitude towards stroke prevention was optimistic in this study, this result may indicate that some deny risk and that some accept and at the same time practically ignore their own personal risks. Research that could determine educational methods of bridging the knowledge-behaviour gap might focus on people's apparent reluctance to change life style towards prevention of stroke and on ways of bringing people to accept calling the emergency medical system as a natural and proper response when they experience stroke symptoms. Since this is the first study of its kind in Australia, the results might not be generalisable across Australia, (or to other countries). Conclusions In conclusion, the knowledge of stroke symptoms in general was high, but the knowledge about stroke prevalence, mortality, and recovery after the stroke was poor. The lifetime chance of having a stroke was in general underestimated. This study suggests that education strategies may be required to improve knowledge about a wide range of issues concerning stroke in the community, as a prelude to developing preventive programmes. Competing interests None declared Pre-publication history The pre-publication history for this paper can be accessed here:
Background In most endemic parts of the world, onchocerciasis (river blindness) control relies, or will soon rely, exclusively on mass treatment with the microfilaricide ivermectin. Worldwide eradication of the parasite by means of this drug is unlikely. Macrofilaricidal drugs are currently being developed for human use. Methods We used ONCHOSIM, a microsimulation mathematical model of the dynamics of onchocerciasis transmission, to explore the potentials of a hypothetical macrofilaricidal drug for the elimination of onchocerciasis under different epidemiological conditions, as characterized by previous intervention strategies, vectorial capacity and levels of coverage. Results With a high vector biting rate and poor coverage, a very effective macrofilaricide would appear to have a substantially higher potential for achieving elimination of the parasite than does ivermectin. Conclusions Macrofilaricides have a substantially higher potential for achieving onchocerciasis elimination than ivermectin, but high coverage levels are still key. When these drugs become available, onchocerciasis elimination strategies should be reconsidered. In view of the impact of control efforts preceding the introduction of macrofilaricides on the success of elimination, it is important to sustain current control efforts. Background Onchocerciasis, or river blindness, is caused by infection with the filarial parasite Onchocerca volvulus. The parasite is transmitted by Simulium species (blackflies) that breed in fast flowing streams [1,2]. Until recently the blindness and skin pathology caused by heavy infections, constituted a major public health problem in many parts of tropical Africa, Yemen, and Latin America. This consideration led to the establishment of the Onchocerciasis Control Programme (OCP) [3] in West Africa, the Onchocerciasis Elimination Program in the Americas (OEPA) [4], and the African Program for Onchocerciasis Control (APOC) [5,6]. All three programmes have come to rely on the regular (OEPA semi-annually, OCP both annually and semi-annually, and APOC annually) distribution of ivermectin (Mectizan) to lower the microfilarial load in affected individuals and thereby reduce transmission and mitigate the clinical manifestations of the infection [7]. In addition, since 1975, OCP has made intensive use of vector control by means of aerial larviciding. This has led to the virtual elimination of the parasite from many formerly endemic areas. However, as the OCP will come to an end in 2002, potential recrudescence of the infection, resulting from residual foci or immigration of infected humans and flies, remains a serious threat unless total elimination can be achieved [8]. In Africa, after the cessation of larviciding, control of the infection will rely on decentralized annual ivermectin distribution, which has been made available by Merck and Co. for as long as it will be needed. This has several disadvantages. First, in view of imperfect geographical and therapeutic coverage, and density dependence in the microfilarial uptake by flies [9,10], low level transmission may continue. Second, resistance to ivermectin might develop and spread [11], as it already has in some nematode parasites of veterinary importance [12-15]. Third, in man, the average life-span of adult worms is approximately 10 years, and while repeated treatments of ivermectin seem to have some permanent effect on the fertility of adult worms, this effect manifests itself only slowly after years of treatment [16]. Unfortunately, the number of safe effective alternative treatments is limited. Diethylcarbamazime, also a microfilaricide, causes severe side effects in onchocerciasis[17]. Suramin, the only currently available highly effective macrofilaricide has even more serious side effects [18]. Large scale nodulectomy, which has been attempted in Latin America, is impractical and will never succeed in eliminating all adult worms [19,20]. Some success was obtained with amocarzine, although its macrofilaricidal properties were not optimal [21]. Several other compounds are currently under consideration. One of these is doxycycline which following long-term treatment was shown to sterilize adult worms in treated humans, an effect that was correlated with the disappearance of the filarial endosymbiont (Wolbachia spp) [22,23]. Another promising drug, widely used in veterinary practice, is moxidectin [24,25]. When used as a single dose it either kills or sterilizes the adult worm depending on the animal model and parasite used. The half-life of this drug in animals is approximately 10 times that of ivermectin, thereby reducing the probability of successful re-infection. However, the effect of this compound in humans remains to be evaluated. The advantages of macrofilaricidal drugs are obvious. With a 100% effective macrofilaricide and 100% coverage, elimination could be achieved almost instantaneously. By contrast, with ivermectin, even with 100% coverage  impossible under current exclusion criteria  elimination of the infection from the community would take over a decade. Even if not all adult worms were killed due to lower drug efficacy or incomplete coverage, a macrofilaricide would still be more effective than ivermectin alone. Taking in consideration the above issues we will address, by using the microsimulation ONCHOSIM model, the possible effects of incorporating a macrofilaricidal drug on the control of O. volvulus and provide suggestions on how it should be best used. Methods Both the life cycle of the parasite and the effects of an intervention are highly complex. While mathematical models can still be formulated explicitly, model equations are impossible to solve analytically [26-28]. Stochastic microsimulation was therefore used as a numerical technique to explore the potential of macrofilaricides. The ONCHOSIM model and computer program, developed by Plaisier et al[29,30] was adapted to incorporate the effect of such a macrofilaricide. We optimistically assumed that a single dose of the drug (or drug combination) under consideration would kill or sterilize 95% of all adult worms. We further assumed that it would have the same effects as ivermectin on the remaining worms. This could be achieved, if necessary, by combining the macrofilaricide with ivermectin. We did not make any assumption about a protective prophylactic effect on re-infection. Parameters, such as the parasite uptake curve, validated to be appropriate for use in areas where the savannah type (blinding) vector-parasite complex dominates, were available as default settings for this programme [30]. In all cases a 10-year programme of annual macrofilariciding was considered. While longer periods may have higher success rates, in practice it may be unrealistic to expect that control programmes can be sustained uninterruptedly for many decades. The outcome of interest is the probability that this 10 years programme would lead to elimination of the parasite from a closed community (village) of approximately 400 people. Three scenarios were considered for simulation. 1. A scenario in which the introduction of the macrofilaricide follows a period of 12 years of intensive control with larviciding (achieving a conservative 65% reduction in biting rates) and ivermectin distribution. Prevalence and transmission have been reduced to a very low level. This pattern is characteristic of many areas covered by OCP. The impact of the macrofilaricide is contrasted with a continued distribution of ivermectin (for 10 years), both in the absence of continued larviciding (i.e. the situation as it will prevail in the OCP countries after the OCP has been phased out). 2. A scenario in which control has been limited to 5 years of annual ivermectin distribution. By the time macrofilaricides become available this will be characteristic of many areas covered by the African Programme for Onchocerciasis Control (APOC). Following these 5 years of ivermectin distribution continued ivermectin distribution for 10 years is compared to its replacement with the macrofilaricide (or a combination of the two drugs). 3. A scenario in which there has not been any control measures for many years. This is typical for areas not covered by either OCP or APOC. This scenario also characterizes areas, such as parts of the north of Sierra Leone, in which the programme has been suspended for many years. The macrofilaricide is compared to the introduction of ivermectin distribution. To adjust the level of endemicity we used the entomological parameter "biting rate". At Asubende, a highly endemic area with a community microfilarial load (CMFL) [31] of approximately 6070 mf per skin snip, a mean monthly biting rate of 2400 s. damnosum bites for an adult human male has been observed. For each of these scenarios above we considered sub-scenarios with biting rates equal to the Asubende level, equal to 50% above (i.e. a monthly biting rate of 3600) and equal to 50% below (i.e. a biting rate of 1200) this level. Biting rates for other types of individuals (women, children) were assumed proportional to the biting rate in adult males. For all scenarios we only considered mass treatment once annually. Another parameter that we varied was the coverage. Both a "low" coverage of 50% and a high coverage of 75% of the total population were considered. Each configuration was simulated a 100 times. In all cases we assumed that groups not eligible for ivermectin treatment (pregnant women, children under 5 years of age) were neither eligible for macrofilaricide treatment. Compliance was not considered to be fully random, but to depend on an individual's propensity to adhere to treatment [32]. This implies that individuals with a low propensity to comply may act as a reservoir for the parasite and this scenario is thus more pessimistic (especially for macrofilaricides) than completely random adherence. We ignored all forms of drug resistance. We did not consider the effects of immigration of infected individuals or the influx of infected flies. This means that conclusions regarding elimination of infection are only valid when interventions are targeted at sufficiently large areas simultaneously and the probability of elimination in individual communities is high. Results The results are shown in Table 1. Table 1 Simulation results, using ONCHOSIM. Cells are based on 100 simulations each. Each cell shows number of eliminations obtained with a macrofilaricide/number of eliminations obtained with ivermectin. Coverage (percentage of individuals actual getting treatment during mass treatment) Monthly Biting Rate Number of successful eliminations macrofilaricide/ivermectin per 100 simulations Scenario 1 ('OCP') Scenario2 ('APOC') Scenario3 (no previous intervention) 50% 1200 100/100 97/42 64/0 50% 2400 78/23 0/0 0/0 50% 3600 1/0 0/0 0/0 75% 1200 100/100 100/100 100/97 75% 2400 100/100 98/0 68/0 75% 3600 100/98 28/0 9/0 Discussion Low levels of infection and transmission of onchocerciasis, as indicated by a low CMFL, have little public health impact, and mild infections can go almost unnoticed. An intervention programme that would succeed in keeping epidemiological indices at low levels would therefore have a public health impact similar to actual elimination of the parasite. In the absence of the development of resistance, annual ivermectin distribution with a reasonably high coverage is probably totally adequate for this. Even under conditions of continued high transmission, ivermectin based control programs could prevent or mitigate most of the morbidity of onchocerciasis infections. Yet, we have chosen elimination as our target. The reason for this is that in the absence of elimination, control measures would have to be sustained and should retain their effectiveness. Failing these conditions, e.g. due to the development of resistance, recrudescence to pre-intervention levels may only be a matter of time. We explored the potential of macrofilaricides to bring about elimination. Coverage is key, as non-compliant individuals may act as reservoirs for the infection and thereby perpetuate transmission. Obviously, macrofilariciding if applied to 100% of the population  currently unattainable  must quickly lead to elimination of the parasite. Still, if only few individuals escape treatment, the reservoir of infection will ultimately be depleted by the natural mortality of the parasite, assuming that immigration of new L3 larvae has been interrupted. Whether a specific coverage level under specific circumstances would achieve elimination can best be theoretically explored using microsimulation. In this study, such simulations were used to explore whether elimination would be feasible within 10 years. We made several assumptions that we believe reflect the limitations that actual control programs could encounter. First, similar to ivermectin, children under 5 were assumed to be excluded from treatment. Second, we assumed that treatment adherence was imperfect and that some individuals have a greater propensity to escape treatment than others. We considered several scenarios. Our main objective was twofold, viz. estimating the 'absolute' probability of elimination of the parasite from the community, and to compare this probability to that of (continued) use of ivermectin alone. In all cases we found that a macrofilaricide had a substantially higher potential for achieving elimination than ivermectin. Yet, even with a macrofilaricide, elimination would rarely be achieved within 10 years when treatment coverage is low and/or biting rates are high. Control efforts prior to the introduction of the macrofilaricide also appeared to be important: in "OCP" areas, with highly favourable epidemiological indices at the outset of macrofilariciding, continued use of ivermectin alone would also often lead to elimination. As we only evaluated the success rates of entire programmes (i.e. including all preceding control measures), in some instances elimination might have been achieved even in the absence of those extra 10 years of treatment. Outside "OCP" areas (scenarios 2 and 3) elimination would never be achieved by ivermectin alone within 10 (additional) years, unless biting rates were low. By contrast, even under difficult conditions of high biting rates or low coverage, macrofilaricide based programmes would occasionally be successful within 10 years. If so, provided programmes are continued for a sufficiently longer period, ultimate success would be likely. We wish to stress that our conclusions are based on microsimulations and are therefore contingent upon the validity of the model and the assumed properties of the drug. Real macrofilaricides may have properties that differ from those assumed in the model. For example, the drug may prevent re-infection for some time following treatment (thereby enhancing its effect) or it may sterilize female worms but be ineffective against male adult worms (thereby reducing its effect). Arguably, even the most effective antihelminths (e.g. albendazole against ascariasis) barely have our assumed 95% killing rate. Whenever a macrofilariciding drug becomes available for human use, extensive simulations should be carried out to explore its potential. Nevertheless, only community trials can demonstrate whether modelling results can be replicated in the field. Conclusions Macrofilaricides have a substantially higher potential for achieving onchocerciasis elimination than ivermectin, but high coverage levels are still key. When these drugs become available for human use, onchocerciasis elimination strategies should be reconsidered. In view of the impact of control efforts preceding the introduction of macrofilaricides on the success of elimination, it is important to sustain current control efforts. Competing interests None declared. Pre-publication history The pre-publication history for this paper can be accessed here:
Background The purpose of this study was to investigate the impact of a 20-year process of de-industrialization in the British Columbia (BC) sawmill industry on labour force trajectories, unemployment history, and physical and psychosocial work conditions as these are important determinants of health in workforces. Methods The study is based on a sample of 1,885 respondents all of whom were sawmill workers in 1979, a year prior to commencement of de-industrialization and who were followed up and interviewed approximately 20 years later. Results Forty percent of workers, 64 years and under, were employed outside the sawmill sector at time of interview. Approximately one third of workers, aged 64 and under, experienced 25 months of more of unemployment during the study period. Only, 1.5% of workers were identified as a "hard core" group of long-term unemployed. Workers re-employed outside the sawmill sector experienced improved physical and psychosocial work conditions relative to those employed in sawmills during the study period. This benefit was greatest for workers originally in unskilled and semi-skilled jobs in sawmills. Conclusions This study shows that future health studies should pay particular attention to long-term employees in manufacturing who may have gone through de-industrialization resulting in exposures to a combination of sustained job insecurity, cyclical unemployment, and adverse physical and psychosocial work conditions. Background The primary purpose of this study is to further our understanding of the dynamics of de-industrialization on unemployment and on physical and psychosocial work conditions as these are key determinants of health in workplaces. We present here a case study of a single industry, sawmilling, in the province of British Columbia (BC) in Western Canada over a 20-year period between 1979 and 1999. This investigation is based on a sample of workers employed in 14 BC sawmills in 1979 just prior to a major recession. These workers were followed up and interviewed approximately 20 years later in order to determine their unemployment history, current employment (sector and occupation), and current physical and psychosocial work conditions. Canada's resource manufacturing sector was particularly hard hit by a recession which began in 1980 and lasted until 1985 [1-3]. In BC, unemployment in the forest products industry rose from 6.4% in 1979 to 19.2% in 1982 [4]. In the sawmill sub-sector unemployment was 44% in 1982 and, 39% of those downsized during the recession were without work 7 years later [5]. The recession was followed by a sustained period of restructuring in many of these sawmills. Canada's resource manufacturing sector was not unique in experiencing major employment losses in the 1980s and 1990s. Average unemployment rates in G-7 nations for the decade 19641973 were 3.1% compared to 7.8 % for the decade from 1983 to 1992 [6]. Reductions in employment were uneven across sectors. From 1973 to 1990, the annual growth of manufacturing employment per capita for the United States and for OECD nations was -0.7 and -1.6, respectively, compared to per capita growth of service sector employment of +1.5 in the United States and +1.3 the OECD [7]. Canada was particularly hard hit as it experienced the largest decrease in manufacturing employment (32%) compared to a G-7 average of 24 percent between 1971 and 1991 [8]. Given that de-industrialization in developed nations is widespread and, because it has affected and continues to affect millions of workers, it is important to investigate the long term consequences of this process. Workers affected by de-industrialization will fall into three very broad, and not necessarily exclusive categories: the long term unemployed, those unemployed following downsizing but re-employed in the long term, and, "survivors" who remain employed in industries that will experience differing intensities of re-structuring. According to a large body of research, the first class of potential "losers" in the de-industrialization process, will likely be those workers with long term exposure to unemployment since it has adverse effects on general mortality and morbidity [9,10]. If de-industrialization produces a group of workers who are not re-employed or re-employable over the long term, this research indicates that such workers will be at high risk for ill health. In terms of the second class of workers  those who are downsized from an industry but find re-employment in the long term  the research is equivocal. Most of the research on unemployment conducted in the era from 1945 to the early 1970s focused on workers downsized because of fluctuations in the business cycle. However, according to Bartley the population of unemployed workers produced by de-industrialization should be called "redundants" [11]. These differ from the population of unemployed in the immediate post-war era because their status is due to permanent rather than cyclical shifts in the labour markets of developed economies. The implicit assumptions are first, that structural changes in the labour market, associated with de-industrialization, will make it more difficult than in the past for unemployed workers to find re-employment and, second, that re-employment of these redundants will ameliorate or reverse the ill effects of unemployment. Most longitudinal studies of the impact of unemployment followed by re-employment have focused on emotional and psychological outcomes. Some of these studies showed that adverse psychological impacts of unemployment continued unabated after re-employment [12,13]. Others showed psychological recovery following re-employment, but with the extent of recovery depending on whether the new job was better than the old [14,15]. Interestingly, studies which demonstrated the ameliorative effects of re-employment following unemployment were conducted in situations where workers found jobs that were superior to their old jobs [16-19]. As far as is known, besides the general observation that many of the re-employment jobs were "better" than the workers' old jobs, these investigators did not compare occupational category or psychosocial and physical work conditions of old with re-employment jobs in any detail. The research on survivors is even less complete. Most studies of survivors focus on short term (3 months or less) psychological or behavioural outcomes [20-25] and have been conceptualized within a "survivor guilt" model [26] in which adverse effects observed among survivors are ascribed mainly to the loss of co-workers and organizational stability because of downsizing. Several studies have shown that survivors experienced lowered job satisfaction, organizational commitment, and greater stress [21-23,27,28]. Two of these studies determined that a downsizing process which was perceived as "fair" had a positive impact on survivors attitude to their job and commitment to their employer [23,27]. Two other studies showed that blue-collar workers and technicians were more likely to perceive the downsizing process as unfair compared to supervisors and managers [20,29]. As in the case of re-employment research, most survivor studies assessed outcomes within a few weeks or months of downsizing so that long-term conclusions about the impacts on survivors are difficult to determine. The survivor guilt framework of these studies does not allow for the possibility that adverse health impacts among survivors could also have been due to the new job conditions they encountered as their industries restructured. As far as is known, only one long term study has been undertaken with survivors of downsizing [30]. This study investigated the effects of a well planned 'strategic' downsizing  conducted in conjunction with an "empowerment" program among 139 employees in a British chemical processing plant over four years. This downsizing was planned and implemented mainly through early retirement and "natural wastage" so that less than 5% of downsized workers were laid-off. The study observed statistically significant increases in task-level demand, control, worker participation, as well as in job satisfaction over 4 years. The authors concluded that detrimental effects on employee well-being due to increased demand may have been moderated by increased task-level control and participation in the downsizing and that the increased demands were largely due to surviving workers having to cope with the same amount work but with fewer co-workers. However, usually the downsizing process involves active restructuring [31], with complex alterations to existing technology, jobs, and work conditions without implementation of "empowerment" programs [32]. Such restructuring has been shown to adversely impact task-level control, social support and demand [33-35]. Also several studies have shown that restructuring may involve the introduction of new forms of work organization such as total quality management (TQM) and of new production methods, in particular lean production, all of which may impact physical and psychosocial work conditions profoundly [36,37]. In a systematic review of 20 studies on the effects of industrial restructuring involving lean production techniques, Landsbergis showed that most of these workplaces were characterized by increased work pace and limited job autonomy [38]. In other words, the restructuring  at least in as much as it involved moves to lean production  may produce work conditions which are detrimental to workers health. Any long term investigation of survivors of de-industrialization must take into account the impact of restructuring on health via its influence on persistent threats to employment and by way of changed physical and psychosocial work conditions. This paper addresses several questions. What was the demographic impact of the de-industrialization process? Did de-industrialization, occurring over a 20-year period, in BC's sawmill sector, produce a core of long-term unemployed workers? The next group of questions concerns the second and third categories of workers affected by de-industrialization; those who were re-employed, over the long term outside the sawmill sector, and the survivors who remained employed in sawmills? How do these two groups of workers differ socio-demographically and in terms of their unemployment histories? And, how do the physical and psychosocial work conditions differ for these two groups approximately 20 years after the recession? Methods This investigation is based on a sample of 3,000 sawmill workers drawn randomly from a cohort that was originally gathered to study the impact of chlorophenol anti-sapstain chemicals on BC sawmill workers [39]. Selection of sawmills and workers for the original study Fourteen medium to large sized sawmills, located mainly in Southwest BC, participated in a retrospective cohort study which was conducted between 1987 and 1998. Mills were selected on the basis of a long-term history of chlorophenol use and availability of intact personnel records. A total of 28,794 workers were enrolled in the cohort, representing approximately 20 percent of all BC sawmill workers. To be eligible, a worker had to be employed at a study mill for at least one year between January 1, 1950 and December 31, 1998. The cohort contains job history data on all cohort members from 1950 to 1998. Because a recession and major restructuring of sawmills began in 1980, the year 1979 was chosen as the pre-recession/restructuring "baseline" year. All workers enrolled in the cohort during 1979 were included in this baseline sub-cohort. A sample of 3,000 workers was randomly selected from the 9,806 workers working in a study sawmill in 1979. Locating interviewees In order to locate interviewees the 1979 sub-cohort was linked to the British Columbia Linked Health Database (BCLHDB). Through the BCLHDB we had access to the first 3-digits of the 6-digit postal codes allowing us to identify the community where cohort members lived, so that we could then locate individuals through local public information sources. The 9,806 workers employed at a study mill in 1979 were linked probabilistically to the BCLHDB. Linkage efficiency was 94.7% such that 3-digit postal codes were obtained for 9,282 of the 9,806 workers in the sub-cohort including 2,920 (97.3%) of the 3000 sampled workers. Searches of union pension plans, electronic telephone databases, and telephone books (by hand) were undertaken to obtain full addresses for the 3,000 workers. For the 80 unlinked workers in the sample, address searches were undertaken using names only. Administering the interviews Face-to-face interviews were conducted between November 1997 and March 1999. Subjects living in remote regions of the province were interviewed by telephone. A short version of the questionnaire (requiring about 20 minutes compared to one hour) was administered by telephone when a respondent was only willing to conduct a brief interview or when proxy interviews were conducted for deceased and incapacitated interviewees. However, because work-related variables were incompletely determined with the short version of the questionnaire, only the long version of the questionnaire was used in the analysis described here. The instrument The instrument was developed after a thorough review of the literature on technological change, restructuring, unemployment, and health and work. Two focus groups were conducted with experienced sawmill workers to finalize the questionnaire; it was then pilot tested on 29 retired sawmill workers. Socio-demographics characteristics were measured. To ascertain their labour market experience, the history of cross-sectoral and occupational mobility and the history of unemployment, measured by the number of episodes and duration, was determined from 1979 to time of interview. Task-level work characteristics were measured using a shortened version [40,41] (See Additional file) of the demand/control instrument [42]. The questions in this instrument measure decision lattitude (control), psychological and physical demand, and co-worker and supervisor social support for each job title held by a respondent. Psychosocial work conditions were determined in job held at time of interview for those still employed. Data analysis In order to measure cross-sectoral mobility industries were coded into the following sectors: sawmill, other forest products manufacturing (including pulp and paper, plywood, shingle and shake etc.); fishing and farming; construction/renovation; non-forest products manufacturing and mining; the service sector; and transportation. In order to measure cross-occupational mobility, all sawmill job titles obtained in the interviews were re-coded to one of 86 basic sawmill job titles [40]. All jobs were also coded using the Standard Occupational Classification [43] and then translated into the Pineo16 Occupational Status Scale [44]. This 16-category scale was collapsed into 4 basic categories; professional/managerial, trades, semi-skilled, and unskilled. Employment trajectories and sociodemographic characteristics were determined for workers who remain employed in the sawmill sector, for workers who were re-employed in other sectors, and for unemployed workers. In the first set of descriptive analyses, the labour force participation status and sector of last or current employment was determined for all respondents (See File 2:figure 1.doc). In the second set of analyses, socio-demographic characteristics for labour force participants 64 years and under at time of interview were compared between the unemployed, those employed in sawmills and workers employed outside the sawmill sector. Chi square statistics were calculated for all possible comparisons among the three groups of workers (Table 2). Figure 1 Labour force participation of workers in 1979 and at time of interview. The number of labour force participants, aged 64 and under, was 1,239 (69 unemployed + 570 Non-Sawmill workers + 600 Sawmill workers). Table 2 Sociodemographic characteristics by sector for labour force participants 64 years of age or under (percent). SOCIO-dEMGRAPHICS Sawmill (1) N = 600 Other sector(2) N = 570 Unemploy (3) N = 69 Chi square 1*2*3 Chi square 1*2 Chi square 1*3 Chi square 2*3 Age category 157.1*** 153.5*** 5.5 21.3** 3539 48 (8.0) 151 (26.5) 8 (11.9) 4044 128 (21.3) 192 (33.6) 19 (26.9) 4549 131 (21.9) 106 (18.6) 19 (26.9) 5054 143 (23.8) 71 (12.5) 10 (14.9) 5559 108 (17.9) 20 (3.5) 9 (13.4) 6064 42 (7.0) 30 (5.3) 4 (6.0) Marital status 0.47 0.46 0.06 0.01 % Not married 91 (15.2) 95 (16.7) 11 (16.4) Place of birth 39.3*** 39.3*** 0.80 4.4* Non-Canadian born 212 (35.3) 108 (19.0) 21 (29.9) Highest education 78.9*** 68.8*** 3.9 25.5*** University 48 (8.0) 94 (16.5) 2 (3.0) Community college 68 (11.4) 126 (22.1) 10 (14.9) Apprentice 113 (18.8) 93 (16.3) 11 (16.4) Secondary 170 (28.3) 160 (28.1) 18 (25.4) Elementary or less 201 (33.5) 97 (17.0) 28 (40.3) Income in 1998 66.0*** 42.4*** 43.3*** 18.9** < $39,999 49 (8.2) 111 (19.5) 10 (15.0) $40,000$79,999 421 (70.1) 312 (54.8) 42 (60.0) >$80,000 130 (21.6) 147 (25.8) 17 (25.0) Home ownership 15.3** 15.3*** 1.3 0.5 % own home 547 (91.1) 475 (83.3) 60 (86.6) ***p > 0.00; **p = 0.0010.01; *p = 0.050.01. In the third set of analyses, unemployment history was compared and contrasted between the unemployed, those employed in sawmills and workers employed outside the sawmill sector. Chi square statistics were calculated for all possible comparisons among the three groups of workers (Table 3). As well, in order to determine the size and characteristics of workers most affected by de-industrialization, length and duration of unemployment were calculated (for the three groups of workers) for workers experiencing 25 months or more unemployment during the study period (Table 4). Table 3 Unemployment history by sctor for labour force participants 64 years of age or under (percent). SOCIO-dEMGRAPHICS Sawmill (1) N = 600 Other sector(2) N = 570 Unemploy (3) N = 69 Chi square 1*2*3 Chi square 1*2 Chi square 1*3 Chi square 2*3 Ever/never 26.5*** Ever unemployed 220 (36.7) 295 (51.7) 69 (100.0) # of episodes 42.9** 1.2 37.2*** 32.0*** 1 159 (72.1) 200 (67.7) 24 (34.3) 2 45 (20.5) 69 (23.5) 25 (35.8) 3 or more 16 (7.3) 26 (8.8) 20 (29.9) Duration 24.8** 4.5 10.9** 24.3*** 112 months 76 (35.2) 78 (26.4) 38 (56.1) 13 to 24 months 80 (36.5) 130 (44.1) 13 (18.2) >25 months 62 (28.3) 87 (29.5) 18 (25.8) ***p > 0.00; **p = 0.0010.01; *p = 0.050.01. Table 4 Length and duration of unemployment for those workers experiencing 25 or more months of unemployment by sector for labour force participants 64 years of age or under. Labour force participation status N Duration in months* Mean # of episodes Sawmill 62 38.3 (2587)** 1.9 (15)*** Non-sawmill 87 36.4 (2590) 2.0 (18) Unemployed 18 50.4 (2589) 4.1 (211) *Average cumulative duration. **Numbers in brackets =Range in months. ***Numbers in brackets=Range in the number of episodes of unemployment. Next, one-way analysis of variance was used to compare the mean scores for psychosocial and physical work conditions at time of interview between those still employed in the sawmill sector with those workers employed outside the sawmill sector (Table 5). A main effects model was constructed controlling for age, place of birth, and education. Separate models were run within each occupational category at time of interview so that the F statistic represents a test of significance for the impact of sector on physical and psychosocial work conditions. Table 5 Analysis of variance* for physical and psychosocial work scores at time of interview for workers employed outside the sawmill sector and in a sawmill by occupational category at time of interview Control Social support Psychological demand Physical demand Noise Job category Sm* Non** Sm Non Sm Non Sm Non Sm Non Managers 27.5 26.9(0.60) 6.0 6.7(0.71) 12.8 13.7(0.25) 2.4 2.1(0.72) 2.6 2.1(0.00) Trades 24.2 24.8(0.72) 5.8 5.8(0.58) 13.2 12.6(0.01) 3.0 2.9(0.01) 3.4 2.9(0.00) Semi-skilled 22.2 23.3(0.05) 5.4 5.9(0.005) 13.2 12.9(0.35) 2.7 2.7(0.60) 3.2 2.8(0.00) Unskilled 20.7 22.4(0.01) 5.2 6.0(0.00) 13.3 12.4(0.01) 3.0 2.7(0.22) 3.3 2.4(0.00) Numbers in parentheses indicate p values, for the F statistic, after controlling for occupational category at time of interview, income, education, age, and place of birth. *Sm = Sawmill sector **Non = Non-sawmill sector Range in adjusted noise, social support, and physical demand scores was from 1 to 4. Range in adjusted psychological demand scores was from 8 to 20. Range in adjusted control scores was from 18 to 32. Finally, one-way analysis of variance was used to compare the mean scores for psychosocial and physical work conditions at time of interview between those still employed in the sawmill sector with those workers employed outside the sawmill sector (Table 6). A main effects models was constructed controlling for age, place of birth, and education. Separate models were run within each occupational category held at baseline in 1979 so that the F statistic represents a test of significance for the impact of sector on physical and psychosocial work conditions with occupational categories held a baseline. Table 6 Analysis of variance* for physical and psychosocial work scores at time of interview for workers employed outside the sawmill sector and in a sawmill by occupational category in 1979 Control Social support Psychological demand Physical demand Noise Job category SM* Non ** Sm Non Sm Non Sm Non Sm Non Managers 23.9 24.3(0.08) 5.9 5.9(0.42) 13.3 13.7(0.90) 2.6 2.3(0.72) 2.9 2.3(0.17) Trades 23.8 24.7(0.17) 6.0 5.9(0.96) 13.0 12.4(0.01) 2.9 2.6(0.004) 3.3 2.8(0.000) Semi-skilled 22.8 24.2(0.09) 5.6 6.1(0.009) 13.1 12.8(0.18) 2.6 2.7(0.57) 3.1 2.5(0.000) Unskilled 22.4 25.0(0.000) 5.4 6.2(0.000) 13.4 13.2(0.16) 2.9 2.6(0.05) 3.3 2.4(0.000) Numbers in parentheses indicate p values, for the F statistic, after controlling for occupational category in 1979 (baseline), income, education, age, and place of birth. *Sm = Sawmill sector **Non = Non-sawmill sector Range in adjusted noise, social support, and physical demand scores was from 1 to 4. Range in adjusted psychological demand scores was from 8 to 20. Range in adjusted control scores was from 18 to 32 Results Survey response Table 1 shows that 62.9 percent of respondents completed the long questionnaire and 9.1 percent completed the short questionnaire for an overall survey response rate of 72 percent. The refusal rate was 4.2 percent, and 19 percent of respondents were not located. The proportion of workers "not found" was highest among those who had worked in isolated "mill towns". Although refusal rates did not vary by age category, the "not found" rate was highest in younger age groups and workers with the lowest duration of work in a study sawmill. The analysis is based on the 1,885 respondents (62.9%) who answered the long questionnaire. Table 1 Interview status Interview status Number Percent Long questionnaire (face-to-face) 1885 62.9 Short questionnaire* 270 9.1 Questionnaire sub-total 2155 72.0 Refusals 126 4.2 Deceased 18 0.6 Needs translator 8 0.3 Not located 582 19.0 Unresolved 111 3.8 Total 3000 100.0 *32 short questionnaire interviews were with the relatives of deceased workers. The total number of deceased workers in the sample was therefore 50. Cross sectoral mobility of the labour force What were the labour force circumstances for workers at time of interview? In 1999, 464 (24.6%) were aged 65 and over. Of the remaining 1,421 (75.4%) respondents aged 64 or under, 600 (42.2%) were still employed in a sawmill, 570 (40.1%) were employed outside the sawmill sector, 131 (9.2%) had taken early retirement, 69 (4.9%) were unemployed, 40 (2.8%) were disabled, and 11 (0.8%) were either working as volunteers, looking after children at home, or attending educational institutions. Among the 570 workers age 64 and under who were employed outside the sawmill sector 212 (37.2%) were in the service sector, 167 (29.3%) were employed in non-sawmill forest products manufacturing  such as pulp mills, paper mills or logging operations, 73 (12.8%) were in construction or renovation, 56 (9.8%) were in transportation, 49 (8.6%) were in non-forest products manufacturing and 13 (2.3%) were employed in fishing or farming. Socio-demographic characteristics of labour force participants 64 years old and under There were no significant differences in marital status among the 3 groups (i.e. the unemployed, employed in the sawmill sector, and employed outside the sawmill sector) (Table 2). In comparing sawmill sector workers with the unemployed, no statistically significant differences were observed except for current income as 23 unemployed workers (33.4%) earned less than $49,000 in the year preceding interview compared to 49 (8.2%) sawmill workers (Chi square 43.3; p < 0.00)). The greatest differences in socio-demographic characteristics were found between groups of the currently employed. Approximately 50% of workers employed outside the sawmill sector were under age 45 compared to 29.3% of sawmill workers. As well, non-sawmill workers were approximately twice as likely to have a college or university education and to be Canadian born than sawmill workers. For both employed groups, approximately 25% earned more than $80,000 in the year before interview. However, 19.5% of workers employed outside the sawmill sector, and 8.2% of sawmill workers were in the lowest income category (less than $39,000). Home ownership was significantly greater for sawmill workers (91.1%) compared with non-sawmill workers (83.3%). Interestingly, more unemployed workers (86.6%) owned homes than workers employed outside the sawmill sector. Unemployment history of labour force participants 64 years old and under Statistically significant differences for the "ever" unemployed were observed as 51.7% of non-sawmill sector workers had experienced unemployment compared to 36.7% of sawmill workers (Table 3). As well, workers unemployed at time of interview were approximately 4 times as likely to have experienced 3 or more episodes of unemployment compared to workers employed at time of interview although long durations of unemployment (>25 months) were similar across the 3 groups. A total of 167 (13.2%) of labour force participants 64 years of age or under experienced an average cumulative duration of unemployment of 25 months or more during the study period (Table 4). The survivor group experienced an average of 38.3 months and 1.9 episodes of unemployment, the group re-employed outside the sawmill sector experienced an average of 36.4 months and 2.0 episodes of unemployment, and the group unemployed at time of interview experienced 50.4 months and 4.1 episodes of unemployment. Work conditions in re-structured sawmills compared to work conditions for those re-employed outside the sawmill sector Table 5 shows mean scores for control, social support, psychological demand, physical demand and noise for workers in the sawmill and non-sawmill sectors, within occupational categories at time of interview, after controlling confounders. Control and social support scores decreased moving down the occupational hierarchy in both sawmill and non-sawmill sectors. In contrast, demand and noise scores increased moving down the occupational hierarchy except for physical demand and noise among tradesmen and the semi-skilled. Control and social support were greater among non-sawmill workers, except in the case of control for managers which was greater for workers employed in a sawmill. Scores for demand variables, with the exception of psychological demand for managers, were greater for sawmill compared to non-sawmill workers. Statistically significant differences between the sawmill and non-sawmill sectors were observed for noise within all occupational categories. Noise scores were always higher in the sawmill sector with differences ranging from a low of 12.5% within the semi-skilled category to a high of 27.2% within the unskilled category. For physical demand, differences between the sectors were not statistically significant except for trades, where it was slightly greater for sawmill workers. Tradesmen and unskilled workers employed in sawmills experienced 4.5% and 6.8%, respectively, greater psychological demand (statistically significant) than their colleagues employed outside the sawmill sector. Semi-skilled and unskilled workers employed in sawmills experienced 9.2% and 15.4% less social support (statistically significant) than their colleagues employed outside the sawmill sector. And, semi-skilled and unskilled workers employed in sawmills experienced 5.0% and 7.6%, respectively less control, (statistically significant) than their colleagues employed outside the sawmill sector. Table 6 compares work conditions at time of interview according to occupational category in 1979 for sawmill workers and those who left the industry. This analysis assesses the impact of moving away from employment in sawmills for workers starting in the same occupational category at baseline. No statistically significant differences in work conditions were observed for workers who were managers at baseline and who had obtained re-employment outside the sawmill sector 20 years later. Workers who left the sawmill sector for re-employment elsewhere had reduced noise scores relative the survivors who stayed employed in sawmills for all occupational categories except managers. Reductions in noise scores were greatest for those who were originally unskilled workers in sawmills (27.3%). For workers who were tradesmen in a sawmill at baseline and moved to re-employment outside the sawmill sector at time of interview, statistically significant reductions were observed for psychological demand (4.6% decrease) and physical demand (10.3%). For workers who were in semi-skilled occupations in a sawmill at baseline and moved to re-employment outside the sawmill sector at time of interview, a statistically significant increase of 8.9% was observed for social support. For workers who were in unskilled occupations in a sawmill at baseline and moved to re-employment outside the sawmill statistically significant increases in control (2.7%), social support (14.8%), and statistically significant decreases in physical demand (10.3%) were observed. Discussion De-industrialization has been widespread in manufacturing workforces in developed nations over the past quarter of a century. This trend is likely to continue with technological innovation in manufacturing and, furthermore, is likely to continue in conjunction with sustained restructuring of manufacturing industries. The long-term impacts on threat of unemployment, unemployment, and working conditions have been under investigated in spite of the fact that this process is widespread in the industrialized world, is likely to have major impacts on health, and has affected and continues to affect many workers. The purpose this study was to better understand the dynamics of de-industrialization on intermediate workplace determinants of health in a sample of BC sawmill workers. The first question, addressed in the study was, what was the demographic impact of the de-industrialization process? Of those workers 64 years and under, and employed at time of interview, approximately half were employed outside the sawmill sector indicating that the non-sawmill sector was vibrant enough during the study period to provide employment opportunities for workers who exited the sawmill sector. Workers who exited the sawmill sector were slightly younger than those who remained employed in mills and also had significantly lower incomes, were better educated, and more likely to be Canadian born. This demographic profile may be partially explained because lay-offs in the industry proceeded strictly on a seniority basis. With the onset of recession and restructuring in 1980, younger workers were laid off first many of whom took further education leading to subsequent employment within the expanding non-sawmill sector. Availability of education for these young adults in conjunction with economic expansion in BC's non-sawmill segments of the economy partially explains these observations. Did de-industrialization, occurring over a 20-year period, in BC's sawmill sector, produce a core of long-term unemployed workers? The impact of de-industrialization can be gauged by the scope and depth of unemployment among workers in this sample. For example, approximately 40% of workers aged 64 and under at time of interview had experienced unemployment at least once during the study period. Within this group of workers, approximately one third experienced unemployment for an average cumulative duration of 36 or more months. And, within the group of workers who were unemployed at time of interview, one quarter experienced over 4 years of unemployment. In other words, among the large number of workers in this sample who experienced unemployment, those workers who were employed at time of interview were out of work 15 % of the study period and those workers unemployed at time of interview were out of work for over 20% of the study period. Although a hard core of long-term unemployed workers was not evident in this study, it is clear that in both groups of workers many were exposed to unemployment for long periods of time. This, means that the length of time, during the study period, that workers were exposed to a combination of the threat and experience of unemployment was also likely very high. How did the physical and psychosocial work conditions differ for sawmill survivors and exiters approximately 20 years after the recession? In general, physical and psychosocial work conditions experienced by workers employed, in similar occupations, outside the sawmill sector were better than for workers employed at sawmills. And, the work conditions benefits of re-employment in a similar job category outside the sawmill sector relative to continued work in the sawmills were greater for those employed in unskilled or semi-skilled occupations and trades at time of interview. In particular, statistically significant improvements in social support and control were observed for unskilled and semi-skilled workers and statistically significant improvements were observed in psychological demand for unskilled workers. Statistically significant improvements were also noted for psychological and physical demand among tradesmen who were re-employed outside the sawmill sector. By conducting the same analysis within occupational category at baseline, we were able to compare physical and psychosocial work conditions in 1999, for workers who started from the "the same place" in 1979. As in the previous analysis, workers who moved to re-employment outside the sawmill sector, in general, showed improvement in work conditions relative to workers who remained in sawmills. And again, as in the previous analysis the benefits of improved work conditions were most pronounced for the unskilled, semi-skilled, and tradesmen. In evaluating the balance of change in control and demand conditions, unskilled workers appear to have benefited most from modest improvements in physical and psychosocial working conditions in restructured sawmills. In contrast, managers may have gained the least benefit from restructuring (they were the only group to show a decline in control scores in combination with an increase in psychological demand, indicating that job strain for managers may be higher outside than inside the sawmill sector). There are several limitations to this study. First, the sawmill cohort, by selecting workers who worked for a minimum of one year excluded workers with the least seniority. This investigation, therefore likely underestimated unemployment relative to the entire BC sawmill workforce. Second, this bias will be reinforced because the workers "not found" in the sample of 3000 workers tended to be younger with less seniority than interview respondents. The "not found" likely consisted of young workers with low seniority who left the province during the initial recession between 1980 and 1985. Because downsizing in the early 1980s proceeded strictly on the basis of seniority, those most likely to be laid off in the early 1980s were also those with the lowest duration of employment. This group is over-represented among non-respondents. By 1999, members of this group would have likely been located if they were employed at a study sawmill so they also represent those workers in 1999 who were either living outside BC, or if employed, were working outside the sawmill sector in the province. Another limitation of this investigation pertains to its generalizability. As noted in the introduction, the way in which workers experience de-industrialization will depend on the extent of the process, the occupational mobility of downsized workers and their ability to obtain education, and the availability of alternative labour markets. The results of this study are based on a particular situation in the resource sector in BC in the 1980s and 1990s. However, the general trend to de-industrialization of blue-collar manufacturing was widespread in the industrial world during this time and is continuing. The particular finding in this study that "survivors" of this process may be at some risk for exposure to both unemployment and adverse physical and psychosocial work conditions highlights the possibility that de-industrialization in other industries may pose risks for survivors. Conclusions This potential for adverse exposures among survivors of de-industrialization has been noted, as far as is known, in one other study [45]. Studies related to de-industrialization usually focus on those who are downsized as the workers who retain jobs within these industries are usually considered the "winners" in the situation. This study points out that in the context of de-industrialization, involving both downsizing and re-structuring and technological change, the histories of unemployment as well as work conditions for those surviving who remain attached to the industry may also be worthy of study for their potential impacts on health. Competing Interests None declared. Pre-publication history The pre-publication history for this paper can be accessed here: Supplementary Material Additional file 13-item shortened demand/control questionnaire. This file contains the questions used to calculate control, social support, psychological and physical demand, and noise scores. Labour force participation of workers in 1979 and at time of interview Click here for file
Background So far the prevalence of viral hepatitis infection in hospitalized patients has not been extensively studied. Therefore we conducted the present five-year observational study to evaluate the prevalence of HBV and HCV infection in high-risk hospitalized patients of Crete, the largest Greek island, Due to the homogeneous population, epidemiological studies can be accurately done. Methods The study was carried out in two out of four District General Hospitals, and in the University Hospital of the island. Markers for HBV and HCV were studied and statistically evaluated according to age, sex and geographical area, in a well-defined hospitalized population. Results The total prevalence of HBsAg and anti-HCV in the three prefectures during the five-year study is 2.66% and 4.75% respectively. Overall the relative risks were higher in males than females for each hepatitis marker (p < 0.001). Higher prevalence of HBcAb was found in the 4160 years age group for both sexes (males 36.17%, females 27.38%). Peak HBsAg prevalence was found in the age group of 2140 and 4160 years for males (5.4%) and females (3.09%) respectively. Anti-HCV prevalence increases with age reaching the highest prevalence in the age group of 4160 years for males (7.19%) and in the 6190 years age group for females (7.16%). For both sexes significant differences between the three locations were identified. For HBsAg a higher prevalence in Heraklion (3.96%) compared to Chania (2.30%, males: p < 0.0001, females: p < 0.05) and Rethymnon (1.45%, males: p < 0.01, females: p < 0.0001) was detected. For HCV a significantly higher prevalence in Heraklion (6.54%) compared to Chania (2.39%, males: p < 0.001, females: p < 0.001) but not in Rethymnon (5.15%, NS). A lower prevalence rate of HBcAb in Heraklion compared to Chania (20.07% versus 23.05%, males: p < 0.001, females: p < 0.001) was found. Conclusions These results were possibly overestimated, but nevertheless reflect the situation of the general population within the island as shown by our previous publications in other study groups. Moreover they contribute to the mapping of viral hepatitis prevalence in a geographical area of Southern Europe and may be helpful in planning public health interventional strategies. Background The problem of viral hepatitis in hospital populations around the world has not been adequately studied, although hospitalized patients overall and especially certain high risk groups among them, represent a possible source for viral hepatitis infection of medical, nursing and auxiliary personnel caring for them as well as for their relatives at home. Epidemiological studies concerning the prevalence of HBV and HCV markers in hospitalized patients have been published, in isolated groups of high-risk patients [1-4] or hospital workers [5] worldwide. Crete, the third largest island of the Mediterranean Sea, has a very homogeneous population of 540.054 inhabitants (1991 census). This allows for epidemiological studies to be conducted with accuracy. Clinical studies have emphasized many differences between the island and mainland Greece, in both liver cell carcinoma characteristics and prevalence of viral markers [6]. A recent study in blood donors in northwest Greece reported an HBsAg prevalence of 0.85% [7] while a study of blood donors in Crete has shown a significantly lower prevalence of 0.40% for the same viral marker [8]. A survey of blood donors in mainland Greece reported a 0.4% prevalence of anti-HCV using a second generation enzyme linked immuno-assay (ELISA 2) [9] not different from the overall prevalence found with the same screening test in Crete (0.38%), but with marked differences among the prefectures of the island (Heraklion 0.52%, Rethymnon 0.52%, Chania 0.23%) [8]. Accepting that, high risk hospitalized patients are not the ideal population for epidemiological studies, overestimating the problem, nonetheless they contribute in the epidemiological mapping of a serious public health problem like viral hepatitis, in a certain geographical area. The aim of the present study was to investigate the prevalence of hepatitis B and C markers in high-risk patients admitted in two out of four District General Hospitals and in the University Hospital of the island, during a five-year period, in an attempt to assess the situation in the hospitals of the island. Methods We retrospectively analyzed the results of virological examinations for HBV and HCV, done on patients admitted in the District General Hospitals of Chania and Rethymnon and in the University Hospital of Heraklion during a five-year period (19921996). All criteria for inclusion and exclusion of a patient in the study had been agreed upon, before starting the study in all three hospitals. Chania and Rethymnon District General Hospitals serve the total population of the corresponding two western prefectures, while the University Hospital is the main hospital, serving the two east prefectures of the island and it is also the referral center of Crete for liver diseases. Virological tests were done in the Department of Virology, University Hospital and the Blood Banks of the other two Hospitals. HBsAg and anti-HCV were studied in all three hospitals while IgG-HBcAb was available only in Heraklion and Chania. Serum samples were tested by the microparticle capture enzyme immunoassay according to the manufacturer's instructions, using kits from Abbott Laboratories (North Chicago, IL); IMx HBsAg (hepatitis B surface antigen), IMx CORE for IgG HBcAb and second and third generation enzymed linked immunosorbent assays (ELISA 2 and ELISA 3) for the detection of anti-HCV were used. 281 184 admissions (138 850 males) were recorded in the three hospitals during the study period of five years. Patients from day care hospital admissions were also included. In all three hospitals patients were considered as high-risk and included in the study if certain criteria applied upon admission were met. The criteria for screening those patients for hepatitis B and C were: Greek nationality, alcoholism, altered liver function tests or exposure of the patient to any risk factor; Risk factors for HBV infection were: multiple sexual contacts, family history, professional risk, major or minor surgical or dental operations, iv drug abuse, tattooing or piercing, previous transfusion, chronic renal failure under dialysis and previous hospitalization over 5 days. Risk factors for HCV infection were: homosexual contacts, family history, professional risk, major or minor surgical or dental operations, iv drug abuse, tattooing or piercing, previous transfusion, previous hospitalization over 5 days and chronic renal failure under dialysis. Exclusion criteria for HBV testing was previous vaccination for hepatitis B with a recent proof of immunization. A total of 46 901 patients (22 779 males) fulfilled the above criteria and were tested for HBsAg. In 21 981 of them (10 291 males), IgG-HBcAb was also studied. Anti-HCV was tested in 34 155 patients (16 919 males). Double tests were eliminated. Statistical analysis For the statistical analysis the relative risks (RR) were calculated from the corresponding proportions. Simple binomial-based tests were used to test for differences in proportions between the sexes at different time periods and locations. Direct standardization was used in the comparison of positivity rates at the three locations for each of the sexes separately. Weighting was by the numbers investigated in each year under the assumption that different subjects were tested each year. The standard population used, was the population calculated so that the proportionate distribution was one third of the way between the three populations under study (Heraklion, Rethymnon and Chania). The Rethymnon and Chania populations were contrasted with the Heraklion population using a version of Cochran s test [10]. The rates calculated in Cochran s test were drawn only from the two years in which data were available for all three hospitals. Results The crude prevalence rates of the markers during the time period considered are presented in Table 1. Table 1 Prevalence rates (%) and relative risks (RR) of HBsAg, HBcAb and HCV in hospitalized patients over the 19921996 time period by sex. Marker Location Year Males (m) tested % Females (f) tested % % (m+f) RR (m vs f) HBsAg Heraklion 19924 2979 6,11 3553 2,81 4,32 2,17 ** 1995 1874 4,59 2534 2,57 3,43 1,79 ** 1996 1741 5,11 2710 3,25 3,98 1,57 * Overall 5,41 2,88 3,96 1,88 ** Hania 1992 1502 3,13 1381 1,38 2,29 2,27 * 1993 3016 2,39 2634 1,29 1,88 1,85 * 1994 2782 3,16 2497 1,88 2,56 1,68 * 1995 2156 2,41 1710 2,11 2,28 1,15 1996 1847 3,14 1760 2,10 2,63 1,49 Overall 2,80 1,73 2,30 1,62 ** Rethymnon 1992 697 3,01 915 0,77 1,74 3,94 * 1993 1369 1,39 912 0,66 1,10 2,11 1994 871 2,41 1154 1,21 1,73 1,99 1995 970 1,75 1115 0,81 1,25 2,17 1996 975 2,15 1247 1,12 1,58 1,92 Overall 2,03 0,94 1,45 2,17 ** HBcAbn Heraklion 19924 2979 25.26 3553 17.69 21,14 1995 1874 24,76 2534 14,56 18,90 1,70 ** 1996 1741 26,77 2710 15,09 19,66 1,77 ** Overall 25.51 15.99 20,07 1,73 ** Hania 1992 843 23,61 587 22,66 23,22 1,04 1993 919 22,63 712 24,16 23,30 0,94 1994 590 23,39 710 11,41 16,85 2,05 ** 1995 816 34,80 424 28,30 32,58 1,23 1996 529 23,06 460 13,48 18,60 1,71 ** Overall 25,72 19,63 23,05 1,31 ** HCV Heraklion 19924 3319 8,44 3959 7,35 7,85 1,15 1995 1817 6,27 2361 5,12 5,62 1,22 1996 1713 6,25 2656 4,63 5,26 1,35 Overall 7,31 5,96 6,54 6,55 ** Hania 1992 739 1,49 903 0,78 1,10 1,92 1993 1323 2,34 901 2,11 2,25 1,11 1994 1928 4,15 1540 3,05 3,66 1,36 1995 1542 1,82 1075 2,23 1,99 0,81 1996 1794 2,79 1215 1,15 2,13 2,42 ** Overall 2,73 1,97 2,39 1,39 * Rethymnon 1992 237 16,03 250 10,00 12,94 1,60 1993 763 3,93 452 7,52 5,27 0,52 1994 576 4,34 499 6,21 5,21 0,70 1995 573 2,27 618 3,72 3,02 0,61 1996 595 3,87 807 4,34 4,14 0,89 Overall 4,70 5,64 5,15 0,83 *Difference between sexes in the prevalence of the marker is significant at the 1% level, ** Difference between sexes in the prevalence of the marker is significant at the 0.1% level, nNot measured in Rethymnon Because of missing information in prevalence rates near the start of the study, information has been lost for statistical analysis. Data were not available yearly for Heraklion early in the study-period; therefore the rates calculated in Cochran s test, were drawn only from the two last years. On the other hand, because of these drawbacks in analysis, the extremely high and unexplained Rethymnon rates for anti-HCV, which were registered in 1992, were not included in the comparison between the three locations. However, certain conclusions can be drawn. Comparison between sexes and age groups HBsAg The proportion of males positive was overall significantly higher, than that of females in all three hospitals. Relative risks were 88%, >100% and 62% higher for males versus females in Heraklion, Rethymnon and Chania respectively, (p < 0.001 for the test of differences in prevalence rates between males and females at each location). Considering each time period separately, there was no year in which the relative risk was significantly lower for males. The prevalence of HBsAg in high-risk hospitalized patients of Crete, according to gender and age is shown in figure 1. The higher prevalence for males 5.4%, was detected in the 2140 years age group, whether for females the peak HBsAg carrier rate (3.09%) was detected in the 4160 years age group. The lower prevalence rate of HBsAg infection (1.1%) was found at the age group of 020 years for both sexes. This might be due to the fact that in Greece under the Guidelines of the Hellenic Society of Pediatricians, the HBV vaccination program of neonates has been going on since 1992, although it is only one year that this program became obligatory. It is therefore expected that the HBV infection rate in young people will further decline in the future. Figure 1 Prevalence (%) of HBsAg in male and female high-risk hospitalized patients of Crete according to age. HBcAb the overall relative risks were 73% and 31% higher for males than females in Heraklion and Chania respectively (again p < 0.001 in both Heraklion and Chania). The prevalence of HBcAb in high-risk hospitalized patients of Heraklion and Chania, according to gender and age is shown in figure 2. The peak exposure rates for both males (36.17%) and females (27.38%) detected in the 4160 years age groups are much higher than the ones reported for blood donors (between 8% and 9.5% in different prefectures) and probably are due to the fact that in general, blood donors are younger than hospitalized patients. When blood donors of over 40 years of age are considered, the exposure rates are similar (23.32%) [8]. Figure 2 Prevalence (%) of HBcAb in male and female high-risk hospitalized patients of Heraklion and Chania according to age. anti-HCV there were no overall differences between sexes in Rethymnon, but in Heraklion and Chania the relative risks were 23% and 39% higher for males as compared to females (p < 0.001 and p < 0.01 respectively). The prevalence of anti-HCV in high-risk hospitalized patients of Crete, according to gender and age is shown in figure 3. In accordance with our previous report on the general population [11] anti-HCV prevalence increases with age reaching the higher prevalence for males (7.19%) in the age group of 4160 years and for females (7,16%) in the 6190 years age group. Figure 3 Prevalence (%) of anti-HCV in male and female high-risk hospitalized patients of Crete according to age. Comparison between the locations For both sexes there was evidence of significant differences between the three locations, this evidence being more apparent in males than females (Table 1). More specifically, for both sexes, evidence was provided by Cochran s test, of significant differences in the three viral markers. HBsAg Higher prevalence in Heraklion (3.96%) than Chania (2.30%) (males: p < 0.0001, females: p < 0.05), and Rethymnon (1.45%) (males: p < 0.01, females: p < 0.0001). anti-HCV Higher prevalence in Heraklion (6.54%) than Chania (2.39%) (males: p < 0.001, females: p < 0.001), but no significant difference between Heraklion and Rethymnon (5.15%) in males or females. It is interesting to note that, the Rethymnon prefecture with the lowest percentage of HBsAg, has a high prevalence of anti-HCV. A similar situation has been reported in volunteer blood donors on the island, where in the same prefecture with the lowest rate of HBsAg (0.27%) the highest rate of anti-HCV (0.52%) was found [8]. Moreover pockets of very high prevalence of anti-HCV have been discovered in the Rethymnon area [11]. These results are also in accordance with the higher prevalence of hepatocellular carcinoma due to HCV in the Rethymnon area [6]. HBcAb Lower prevalence in Heraklion (20.07%) than Chania (23.05%) (males: p < 0.001, females: p < 0.001). For similar exposure rates to HBV, there was a significant decrease in carrier rate in Chania. This was true for practically every year and for both sexes. It was very clear in the year of 1995, when although the exposure rates were 32.58% in Chania versus 18.90% in Heraklion, carrier rates were 2.28% versus 3.43% respectively. Even on similar exposure rates, as during 1996, (Chania: 18.6% versus Heraklion: 19.66%) fewer people became carriers in Chania than in Heraklion (2.63% versus 3.96%). Since the population of these two prefectures is genetically homogeneous and the age groups of hospitalized patients were similar we suspect that the lower carrier rate in Chania might be related to an, as yet unidentified environmental factor. In general the RR were higher in males for each hepatitis marker. This observation agrees with the earlier report in blood donors [8]. When results are analyzed according to sex and location, the male predominance is universal with one notable exception for anti-HCV in Rethymnon, which we call the Rethymnon female paradox. Females in Rethymnon have a higher HCV infection rate than males, a feature unique in the island, which we already reported in a study of general population in a rural area of Rethymnon (males: 2.4%, females: 3.7%) [11], and in the study of the volunteer blood donors (males: 0.37%, females: 0.49%) [8]. The reason for this discrepancy is not clear. Intravenous drugs cannot be the explanation since the same phenomenon appeared in an isolated community where no drugs exist [11]. Moreover, almost 90% of our HCV cases are of the sporadic type with no obvious source of infection. Medical and particularly midwifery malpractice of the recent past in the Rethymnon area, appears to be a rational explanation. The University hospital of Heraklion is a referral center, with a much larger catchment area than the regional hospitals of Rethymnon and Chania that could be directly compared. Therefore the last two were compared in crude prevalence rates of the markers over the five-year period for males and females separately, using the binomial test for proportions. As can be seen in Table 2, for both the male and female populations, evidence was provided of a significantly: Table 2 A comparison of crude prevalence rates (%) for the markers HBsAg and anti-HCV in Rethymnon and Hania over the 19926 period. Rethymnon % Hania % Difference in prevalence 95%CI Males HBsAg 2.03 2.80 -0.008 (-0.013, -0.003)* anti-HCV 4.70 2.73 0.020 (0.011, 0.028)** Females HBsAg 0.94 1.73 -0.008 (-0.012, 0.004)** Anti-HCV 5.64 1.97 0.037 (0.027, 0.046)** *p < 0.001, **p < 0.0001, CI : Confidence Interval Lower prevalence in Rethymnon than Chania for HBsAg (p < 0.001 and p < 0.0001 for males and females respectively). Higher prevalence of anti-HCV in Rethymnon than Chania (p < 0.0001 for both males and females). The prevalence of HBV and HCV markers of patients, in the English publications during the past 10 years refers to isolated high-risk groups. Thus, a published survey of HBV and HCV infection markers from a referral center of Northern Greece among alcoholics with chronic liver disease, alcoholics without chronic liver disease, hospitalized non-alcoholic patients and healthy controls, reported an HBsAg prevalence rate of 10.8%, 7.4%, 1.4% and 2.1%, an HBcAb prevalence rate of 39.2%, 36.4%, 14.3% and 19.5% and an anti-HCV prevalence rate of 1.2%, 1.4% 0% and 0.6% respectively [12]. The prevalence rates of HBV viral markers in the non-alcoholic hospitalized patients are lower than ours while our figures come closer to the ones reported in their healthy control group. Furthermore our anti-HCV prevalence rates exceed theirs. The small number of patients included in the hospitalized non-alcoholic patients (70 patients) may be a problem for a direct comparison of the two studies but still we believe that these differences do reflect the different situation between Crete and mainland Greece. In another Greek study, the prevalence of anti-HCV of patients in haemodialysis was found to be 17.6% [13] while Romanian patients on dialysis since 1996 have an anti-HCV prevalence of 28.6% and an HBsAg prevalence of 21.6% [2]. High prevalence rates for HBsAg (15.3%) and anti-HCV (9.7%) were reported among non-intravenous-drug-using patients [1] attending clinics for sexually transmitted diseases that probably underline the importance of the sexual transmission of the viruses [14]. A recent report of Greek HIV-positive patients, reported prevalence rates of HBV extremely elevated at 67.4% rising to 90.9% in the blood transfusion recipients, while anti-HCV were 13.8% rising to 45.5% in blood transfusion recipients [15]. Similarly in Italy HIV-positive patients have HBV infection markers prevalence rate of 82% and anti-HCV prevalence rate of 72% if they are drug addicts while the prevalence rate is 77% for HBV markers and 7% for anti-HCV if HIV-positive patients are homosexuals [4]. These extremely elevated figures apply to an even more selected high risk-group of patients, compared to our study. In a survey from mainland Greece HCV infection was responsible for 25% of chronic liver disease patients [16]. Higher figures than ours (18% anti-HCV, and 5% HBsAg) have been published concerning patients in an inner-city emergency department in a study that lasted for only six weeks [3]. It is obvious that due to the lack of homogeneity of the study groups mentioned before, direct comparisons of the present study with reports either from Greece or from other parts of the world may be illusive. Conclusions In the present study we investigated the prevalence rates of HBV and HCV markers in high-risk hospitalized patients of Crete in a study period of five years. We consider important that similar studies are extended over a period of many years, since infection rates are not similar each year and a limited study period will not yield representative results. HBsAg had an overall prevalence rate of 2.66% with a significant difference between sexes (3.39% males, 1.97% females). A lower carrier rate was detected under the age of 20 years and peak carrier rates in middle age groups. HBcAb results showed an overall exposure rate to HBV virus of 20.96%, with the higher exposure rate for both sexes detected in the 4160 years age group. anti-HCV had an overall prevalence rate of 4.75% with no significant differences between sexes (males: 4.90%, females: 4.60%), increasing with age and peaking in the 4160 years age group for males and in the 6190 years age group for females. This finding confirms our previous field studies reporting high anti-HCV prevalence (4.8%) in outpatients of primary health care centers in Crete [17]. In conclusion, we believe that, viral hepatitis markers prevalence rates of the high-risk hospitalized patients in Crete, overestimate but reflect the situation in the general population of the island. Our results also underline the differences between the island and mainland Greece, which is a country of intermediate prevalence of HBV and HCV infection. Additionally these data contribute to the mapping of viral hepatitis prevalence in this geographical area of Southern Europe and therefore may be helpful in planning public health interventional strategies. Abbreviations HBV: Hepatitis B virus HCV: Hepatitis C virus HBsAg: Hepatitis B surface antigen HBcAg: Hepatitis B core antigen HBcAb: Hepatitis B core antibody anti-HCV: antibody to hepatitis C virus Competing interests None declared. Pre-publication history The pre-publication history for this paper can be accessed here:
Background Invasive meningococcal disease is a significant cause of mortality and morbidity in the UK. Administration of chemoprophylaxis to close contacts reduces the risk of a secondary case. However, unnecessary chemoprophylaxis may be associated with adverse reactions, increased antibiotic resistance and removal of organisms, such as Neisseria lactamica, which help to protect against meningococcal disease. Limited evidence exists to suggest that overuse of chemoprophylaxis may occur. This study aimed to evaluate prescribing of chemoprophylaxis for contacts of meningococcal disease by general practitioners and hospital staff. Methods Retrospective case note review of cases of meningococcal disease was conducted in one health district from 1st September 1997 to 31st August 1999. Routine hospital and general practitioner prescribing data was searched for chemoprophylactic prescriptions of rifampicin and ciprofloxacin. A questionnaire of general practitioners was undertaken to obtain more detailed information. Results Prescribing by hospital doctors was in line with recommendations by the Consultant for Communicable Disease Control. General practitioners prescribed 118% more chemoprophylaxis than was recommended. Size of practice and training status did not affect the level of additional prescribing, but there were significant differences by geographical area. The highest levels of prescribing occurred in areas with high disease rates and associated publicity. However, some true close contacts did not appear to receive prophylaxis. Conclusions Receipt of chemoprophylaxis is affected by a series of patient, doctor and community interactions. High publicity appears to increase demand for prophylaxis. Some true contacts do not receive appropriate chemoprophylaxis and are left at an unnecessarily increased risk. Background Invasive meningococcal disease is a significant cause of morbidity and mortality in the United Kingdom and the commonest infectious cause of death under the age of 20[1]. In 1999 almost 3000 cases were notified with an overall case fatality rate of around 8%[2]. There is an increased risk of a secondary case of meningococcal disease amongst household contacts, which is between 450 and 1650 times that of the general population [3-6]. This is in part explained by the fact that household and kissing contacts frequently carry the same pathogenic strain[7]. Chemoprophylaxis is given to close contacts of cases to eliminate naso-pharyngeal carriage of meningococci. Prophylaxis reduces, but does not eliminate, the risk of secondary cases[8]. If prophylaxis is not given to appropriate contacts then preventable secondary cases may occur. Unnecessary use of prophylaxis is associated with increased antibiotic resistance, drug side effects, and removal of non-virulent meningococci and N. lactamica; both organisms induce immunity and provide a competitive flora against colonisation with virulent meningococcal strains [9-11]. UK guidelines identify who should receive prophylaxis [12], and in this study we evaluate the prescribing of prophylaxis by hospital staff and general practitioners against these criteria. Methods All confirmed and clinical cases [13] of invasive meningococcal disease amongst residents of Southern Derbyshire Health Authority between 1st September 1997 and 31st August 1999 were identified from the Notifications of Infectious Diseases database and data from the enhanced surveillance of meningococcal infections undertaken by the Communicable Disease Surveillance Centre, Trent. Data on contacts identified at the time were obtained from the Consultant for Communicable Disease Control's (CCDC) records and were assessed against the current UK guidelines [12]. Data were recorded regarding the method of contact tracing (face to face or by telephone, and by whom if face to face contact had taken place), whether the case was confirmed by laboratory investigations, the serogroup of identified organisms and the number of contacts identified. General practitioner prescribing data from Prescribing Analysis and Cost (PACT) for 1st September 1997 to 31st August 1999 were examined to identify possible chemoprophylactic prescriptions for rifampicin, ciprofloxacin and ceftriaxone. Hospital dispensing data for rifampicin (the only drug used for chemoprophylaxis in the hospital protocol during this period) were examined for the period 1st March 1999 to 31st August 1999. Computerised data were not available before 1st March 1999. All 2-day courses of rifampicin were assumed to be for eradication of meningococcal carriage[14]. Ciprofloxacin is widely used in general practice, but the only indications for single dose treatment in the British National Formulary are gonorrhoea and chemoprophylaxis for meningococcal disease[15]. All prescriptions for single dose ciprofloxacin were assumed to be for prophylaxis. The same assumption was made for single 250 mg doses of ceftriaxone. As PACT data do not identify individual patients a questionnaire was sent to all GP practices in Southern Derbyshire. This covered the use of rifampicin, ciprofloxacin and ceftriaxone for prophylaxis during the study period. The questionnaire also requested the initials of the contact, the initials of the index case for the contact, the drug prescribed and the date of the prescription. Practices were free to obtain the information by whatever method they felt was most effective in the context of their own practice. This information was linked with the database of cases and contacts to identify which contacts had been prescribed prophylaxis. Practices were also given the option to indicate if they were unable to retrieve the relevant data. For those who had received a prescription, an assessment was made and they were classified into one of the following groups: known to the CCDC and prophylaxis recommended known to the CCDC, related to a known case of meningococcal disease, but prophylaxis not recommended not known to the CCDC but related in time and place to a known case, and known to the CCDC and not related to a known case of meningococcal disease in the district. Statistics Student's t tests on log transformed data were used to compare the mean number of contacts per case by serogroup, whether confirmed or clinical case and method of contact tracing. The Mann Whitney U test was used to compare the level of additional prescribing per GP for each practice by response status to questionnaire and training status of the practice. The relationship between the size of the practice and the number of additional prescriptions per GP was explored by using Spearman rank correlation. Mann Whitney U test was used to determine differences between the levels of additional prescribing at local authority level. Linear regression was used to explore any possible relationships between the level of additional prescribing at Local Authority level and the Towsnend deprivation score and rate of invasive meningococcal disease. Results During the study period 134 cases (66 male, 68 female) of meningococcal disease were notified. Of these 88 (66%) were confirmed by laboratory diagnosis and 46 (34%) were clinical cases. Of the 75 that were groupable, 50 (67%) were serogroup B, 24 (32%) were serogroup C and 1 (1%) was serogroup Y. The population estimate for 1998 for Southern Derbyshire was 567,457. The rate of confirmed meningococcal disease was 7.8 per 100,000 per annum. The rate of clinical and confirmed cases[13] was 11.8 per 100,000 per annum compared to the England and Wales rate in 1998 of 6.1/100,000 (rate ratio 1.9, 95% CI 1.52.5, p < 0.0001) Contact tracing In 34 (25%) cases the patient or other key informants were interviewed in person by the CCDC, in 24 (18%) by another public health physician and in 51 (38%) cases contact tracing was performed by telephone. In 25 (18%) of cases it was impossible to determine the method of contact tracing. 952 close contacts were identified for whom prophylaxis had been recommended by a public health physician. The mean number of contacts per case was 7.2 and the median 6.0. The mean number of contacts for each case visited by a public health physician was 6.4 and for each case where contact tracing was done by telephone was 8.3 (Students t test on log transformed data, p = 0.03). There were no significant differences in the mean number of contacts per case by serogroup, by whether face to face contact tracing was performed by the CCDC or a public health physician in training, nor by whether the case was confirmed by laboratory investigations or not. The degree of contact with the index case was determined for 697 (73.2%) of the contacts as shown in Table 1. Table 1 Nature of contact. Nature of contact Number (%) Household 442 (63) Overnight stay in past 7 days 71 (10) Childminding 20 (3) Kissing (saliva exchange) 19(3) Resuscitation 1(0) Other contact (>8 hours) 40 (6) Other contact (<8 hours) 104(15) TOTAL 697(100) Prescribing For 568 (60%) contacts chemoprophylaxis was prescribed by hospital staff and for 296 (31%) the general practitioner (GP) was asked to prescribe. For 88 (9%) contacts the prescriber was unspecified. During the six month period for which hospital prescribing data were available, 69 prescriptions were identified from the dispensing records. Of these 11 were for the elimination of carriage in cases. A further five were contacts where chemoprophylaxis was not recommended and in one instance the prescription might have related to one of three recent cases, but the contact had not been identified by the CCDC. For six identified contacts no record could be found that the prescription had been dispensed, although for two of these the GP had prescribed. Of the 296 contacts for whom GPs were asked to prescribe, 277 were patients of GPs in Southern Derbyshire. 604 prescriptions for chemoprophylaxis were identified from the PACT data, 327 (118%) more than recommended by the CCDC. The rates of disease and number of additional prescriptions per GP for each local authority area are shown in Table 2. No association could be demonstrated by linear regression between the mean number of additional prescriptions per GP for each local authority area and the rate of invasive disease (p = 0.30) or Townsend deprivation score (p = 0.72). The two areas with high rates of disease (including clusters), and subsequent publicity both had significantly higher prescribing The other large authority with high rates of disease, but little publicity, had a significantly lower level of additional prescribing. Table 2 Rates of meningococcal disease and additional prescriptions per GP (from PACT data) by local authority area Local Authority 1* 2 3 4* 5 Cases of meningococcal disease 34 62 1 28 9 Population 125727 258919 37091 103735 67655 Townsend deprivation score -1.56 2.09 -2.97 -0.79 -3.01 Rate of IMD per 100,000 13.5 12.0 1.3 13.5 6.7 Number of practices 16 38 4 14 9 Mean number of additional prescriptions per GP (SD) 2.2(3.1) 0.2(1.4) 0.5 (0.5) 1.9(1.1) 0.9(0.8) Median number of additional prescriptions per GP (interquartile range) 1.2 (0.53.3) 0.0 (0.00.8) 0.5 (0.00.9) 1.91 (0.92.6) 0.7 (0.31.6) Difference in mean number of additional prescriptions per GP for each Local Authority compared to all others. P (two tailed) 0.02 <0.001 0.6 0.002 0.8 *represents high publicity area At a practice level, there were no significant differences in estimated additional prescribing by response status to questionnaire, training status or size of practice. GP Questionnaires Fifty-seven out of 80 practices (71%) replied to the questionnaire. Of these, 17 (21% of all practices) were unable to supply data. Data was therefore obtained from 40 (50%). Chemoprophylaxis was recommended for 142 identified contacts who were patients of these practices whilst the practices identified 179 chemoprophylaxis prescriptions. Figure 1 shows whether or not a record of prescribing existed for the contacts who had been recommended to have prophylaxis. Figure 2 shows how many of the recorded prescriptions for chemoprophylaxis had been recommended. Figure 1 Outcome of recommendations for chemoprophylaxis from GP questionnaires Figure 2 Analysis of prescriptions written by GP practices from GP questionnaires In these practices, PACT identified a total of 305 courses of chemoprophylaxis and GP practices identified 179. The number of prescriptions for rifampicin, ciprofloxacin and ceftriaxone are shown in Table 3. There is no difference between the ratio of prescriptions recorded by GPs between rifampicin and ciprofloxacin. Table 3 Comparison of PACT and GP questionnaire data Antibiotic PACT total GP total Ratio of no of prescriptions from PACT to GP data Rifampicin 221 127 1.7 Ciprofloxacin 83 52 1.6 Ceftriaxone 1 0 Total 305 179 1.7 Discussion This study demonstrated that after a case of invasive meningococcal disease, more prescriptions for chemoprophylaxis are dispensed than would be expected from a strict interpretation of the United Kingdom guidelines [12]. However, some people who are at increased risk appear not to receive prophylaxis. No practice characteristics examined accounted for differences in additional prescribing between practices, nor did the rate of invasive meningococcal disease or the level of social deprivation in the local authority areas. However, it is plausible that significant levels of publicity in the two areas with highest levels of additional prescribing may have increased requests to GPs to prescribe prophylaxis. There are a number of possible limitations of this study. Firstly, contact ascertainment may be incomplete. Not all recommendations for prophylaxis may be recorded and it was not always possible to ascertain the degree of contact from the records. Secondly, questionnaire data from practices were incomplete. These practices may not be representative. However, the fact that the numbers of additional prescriptions per GP were similar for responders and non-responders suggests that this has not affected the results. The mean number of contacts per case of meningococcal disease in our study was similar to that found in other studies in the UK[14,16]. Significantly less close contacts were identified when a public health physician conducted a face to face interview with the key informants. This suggests that contact tracing is more appropriate with less unnecessary prophylaxis given when informants are interviewed personally. It is, however, possible that there may be a confounding effect between the use of telephone interviews and experience at contact tracing. However, where face to face interviews took place there was no significant difference between the number of contacts identified by the CCDC and public health doctors in training. There were significant discrepancies between the numbers of prescriptions recorded by PACT and those identified by the practices. PACT is an accurate record of prescriptions dispensed by community pharmacies. Short courses of rifampicin have no other indication, so it is likely that these are for chemoprophylaxis[14]. By contrast, single dose courses of ciprofloxacin are indicated for the treatment of gonorrhoea. However, less than 10 isolates of Neisseria gonorrhoeae come from general practice in the district each year. [D Bullock, personal communication] Most of these will be referred to the genito-urinary medicine service. Even if they were all treated in general practice the difference this would make to the overall results presented here would be small. If significant amounts of single dose ciprofloxacin were being used for indications other than chemoprophylaxis, then the ratio of PACT prescriptions to those recorded by GPs would be higher for ciprofloxacin than for rifampicin. As this was not the case, it is likely that most single dose courses were for chemoprophylaxis of meningococcal disease. The data provided by general practices may have underestimated the prescribing of chemoprophylaxis. Prescriptions may not be recorded in the records, may not be entered on the computer system or may not be retrieved during a search. This may be a particular problem if the patient is attended by an out of hours service. Although these prescriptions will be attributed to the practice the patient is registered with on PACT, the correspondence from the out of hours service may not find its way into the main patient record or may not be computerised. It is therefore likely that the data from the GP questionnaires underestimated the true level of prescribing. Hospital prescribing was in line with the recommendations of the CCDC. However, we found that GPs had prescribed twice as many courses of prophylaxis (from PACT data) as recommended. The additional prescribing must be for one of the following reasons: for true close contacts who have been missed by the CCDC, which, although possible is unlikely for contacts of cases in other districts. In this study only 5% of recommendations were to GPs in other districts. It is likely that the reverse is also true, so this could account for only a small proportion of additional prescriptions. for people whose degree of contact does not warrant prophylaxis for contacts of patients who do not have meningococcal disease (e.g. contacts of people who are perceived by the public to have meningococcal disease, but in fact have another disease). For this to occur the GP would be required to prescribe prophylaxis solely on the word of the patient. Many GPs would consult the Public Health Department in this situation, which would lead to the recognition of cases of meningitis or reassurance that it was not meningococcal disease. It is impossible from the data available to further assess the nature of this additional prescribing, but it is probable that it results from a combination of the suggested possibilities. A UK study in 1995 [14] showed over-prescribing by a factor of three, although this only used PACT data and did not include hospital data or obtain further information from GPs. This approach may overestimate prescribing and almost certainly include some appropriate prescriptions. An audit from Denmark[17] also found that unnecessary prophylaxis was prescribed. The mean "over-treatment" in the Danish study was 0.9 person/case (in our study 2.4 persons/case). The Danish study interviewed an adult associated with each case and also identified 0.4 missed contacts per case. Our methodology did not allow this comparison to be performed. Conversely, their methods were likely to underestimate the level of additional prescribing because the informant may not know about prescriptions supplied outside the immediate household. Over-prescribing varied by local authority area and was significantly higher in two areas. Practices in local authority areas 1 and 4 wrote significantly more additional prescriptions than average. Both these areas had high levels of disease with local publicity surrounding clusters and individual cases. In the other area (2) with a similarly high rate of disease levels of publicity were much lower. No evidence of an association between over prescribing and rates of invasive meningococcal disease or social deprivation could be found. We speculate that the high publicity levels resulted in higher levels of demand for chemoprophylaxis from people who were associated with the cases, but not true close contacts. This is supported by a lower rate of prescribing in the other high disease rate area. This area (2) has no discrete communities in which clusters of disease have been identified and the public did not react in the same way as in the other two, more rural, areas. This over-prescribing is likely to be patient driven, as GPs do not actively seek inappropriate contacts to treat. On almost 50% of occasions that GPs were asked to prescribe, there is no record within the practice that the prescription was written. There are a number of possible explanations for this. Firstly, prescriptions may not have been written, leaving some people at an unnecessarily increased risk of disease. This is supported by the fact that 10 out of 80 practices prescribed less according to PACT data than the number of courses recommended. Secondly, prescriptions may have been issued but no record kept which has implications for clinical governance. If the patient is attended by the out of hours service, the prescription may have been written but the information not transferred to the main general practice record or not entered on the practice computer system. Even if the GP has written a prescription the contact may still not have received prophylaxis. It is possible that some contacts did not come forward to receive their prescription or did not present it to a pharmacy. The prescription charge may have acted as a deterrent. Other contacts may have found that rifampicin was not immediately available at the pharmacy and consequently did not return to collect their antibiotics. Further work is necessary to elucidate the extent to which these barriers may operate. Conclusions Receipt of chemoprophylaxis is affected by a series of patient, doctor and community interactions. Additional prescribing occurs at all stages in the process. High publicity appears to increase demand, although a significant number of contacts appear never to receive a cost-effective treatment. Our study also raises issues about the quality of documentation on the identification and subsequent supply of antibiotics to contacts. Further research is required to elucidate the reasons why some contacts seem not to receive prophylaxis. A number of steps could be taken to ensure that use of chemoprophylaxis is as appropriate as possible. Face to face interviews with key informants by public health practitioners may help to prevent overprescribing. Further research is necessary to clarify this issue. Overprescribing may also be avoided by ensuring that general practitioners are aware of the availability of public health advice to help make decisions about prophylaxis. When publicity occurs it is important to use the media to ensure that reliable information on the level of the risk of secondary cases is given to the public. Further work is also necessary to investigate to what extent potential barriers to contacts obtaining prophylaxis operate. Finally the use of structured recording forms will facilitate future audit of this important area of public health practice. Competing interests None declared. Pre-publication history The pre-publication history for this paper can be accessed here:
Background Race is commonly described in epidemiological surveys based on phenotypic characteristics. Training of interviewers to identify race is time-consuming and self identification of race might be difficult to interpret. The aim of this study was to determine the agreement between race definition based on the number of ascendants with black skin colour, with the self-assessment and observer's assessment of the skin colour. Methods In a cross-sectional study of 50 women aged 14 years or older, from an outpatient clinic of an University affiliated hospital, race was assessed through observation and the self-assignment of the colour of skin and by the number of black ascendants including parents and grandparents. Reliability was measured through Kappa coefficient. Results Agreement beyond chance between self-assigned and observed skin colour was excellent for white (0.75 95% CI 0.720.78) and black women (0.89 95% CI 0.710.79), but only good for participants with mixed colour (0.61 95% CI 0.580.64), resulting in a global kappa of 0.75 (95% CI 0.710.79). However, only a good agreement for mixed women was obtained. The presence of 3 or more black ascendants was highly associated with observed and self-assessed black skin colour. Most women self-assigned or observed as white had no black ascendants. Conclusions The assessment of race based on the race of ascendants showed reasonable agreement with the ascertainment done by trained interviewers and with the self-report of race. This method may be considered for evaluation of race in epidemiological surveys, since it is less time-consuming than the evaluation by interviewers. Background The characterisation of race or ethnicity has been frequently required to describe populations in epidemiological surveys [1]. Race is commonly investigated as a risk factor [2,3] or as a potential confounding in the explanations of health outcomes [4] in studies of risk factors for hypertension or osteoporosis, conditions known to affect differently white and black individuals. For instance, Black hypertensive patients in sub-Saharan Africa are prone to cerebral haemorrhage, malignant hypertension, uraemia, and congestive heart failure, whereas they are relatively protected against coronary artery disease [5]. Race is also highly associated with bone mineral density of the lumbar spine, trochanter, femoral neck, and midradius [6], and black individuals are generally considered protected against osteoporosis The race concept is also important in the investigation of inheritance, since it is related to methods of treatment and prevention. Many diseases are heritable, and many of these diseases are more or less prevalent within certain populations. Diseases such as sickle-cell anaemia, cystic fibrosis, and phenylketonuria are caused when an individual carries a double recessive gene(s) that causes the disease [7]. Many genetic diseases can be controlled if detected early enough (e.g. phenylketonuria can be controlled with a proper diet), and other diseases are more frequent in some ancestral lines than others (e.g. osteoporosis is more frequent in white women) [6]. Race is often classified based on phenotypic characteristics such as colour of the skin, but other characteristics such as hair colour and type are rarely described. Race refers to differences in biology, while ethnicity involve differences of shared characteristics, including ascendants, geographical origins, cultural traditions, and language [8]. It has been suggested that more than one source of information should be used to describe race, including self-assigned ethnicity (using nationally agreed guidelines enabling comparability with census data) and the ethnicity assigned by the observer, based on national census categorisation or logical categories proposed by the researchers [9]. Evaluation of race according to skin colour, hair type, conformation of the nose and lips, and jaw position has been used to identify black, white and mixed individuals [10], although the criteria to classify them are scarcely reported [11,12]. Consequently, reliability of data on race or ethnicity is difficult to achieve and the classification used may not capture the diversity of inhabitants of different countries or regions [4,13]. Conceptual and methodological problems in the investigation of ethnicity have been recognised [14]. Our study was designed to determine the reliability between three methods of establishing race, one based on the number of black ascendants, the self-assessment of skin colour, and race assigned by observers. Methods A cross-sectional study was conducted on a sample of 50 women aged 14 years or older, which were systematically selected from an outpatient clinics of a University affiliated hospital in Porto Alegre, southern Brazil. Participants included in the study answered to a pre-tested and structured questionnaire, which collected information on the number of black ascendants (parents and grandparents), school attendance, and included a self-assignment of the colour of skin as well as the observer assessment of skin colour. In a preliminary phase, a training was provided to the observers to standardise the identification of the skin colour and in the details of several phenotypic characteristics employed in Brazil before [10] such as the colour of hair, lines and hands' palm surface. Following the training, the principal investigator and the research assistants observed 28 women and compared their findings of the physical features. The research team reached full agreement for skin colour (white, mixture or black), hair colour (blonde, light brown, medium brown, dark brown or black), lines and hands' palm surface (pink palm and colourless lines, pink palm and red lines or white palm and dark lines) for the last 15 women observed. We also investigated the race of ascendants, through the question: "Which are the race of your ascendants: parents and grandparents?". A total of six research assistants were certified for the study. During the study, after the informed consent was obtained, one interviewer applied the questionnaire asking questions to the participants and the research team independently registered the information on physical characteristics, observing the women under sunlight. All interviewers were blinded to each other answers. Skin colour was described by the observers as white, mixed or black, the self-assigned skin colour used white, black, mixed, and local words meaning light mulatto and dark mulatto. The race of the parents and grandparents was investigate using a heredogram, which incorporated two generations to the assess the inheritance. Even though information could be reported for a maximum of six ascendants some women did not know the father or grandparents. Therefore, we collapsed the categories with more than 3 ascendants of black origin in the category of at least three black ascendants. There was investigated a sample of 50 women, which did not include the 28 women at the training phase. This sample size was sufficient to detect an agreement of at least 85%, with an error of 10%, and a confidence interval of 95%. In order to calculate the kappa coefficients, self-reported mixed skin colour was collapsed with light mulatto and dark mulatto. Analysis were carried out through Chi-square for contingency tables and kappa statistics to calculate to what extent the observers agreed beyond what we would expect by chance alone [15]. Kappa coefficients were calculated from observation of six interviewers and the skin colour self-assigned by the participant. The Kappa statistic was calculated for each two categories (white vs. non white; black vs. non black and mixed vs. non mixed) and a global Kappa with 95% confidence interval for all three categories. Kappa greater than 0.75 was taken as an excellent agreement, between 0.75 and 0.40 intermediate to good agreement, and below 0.40, poor agreement. The reliability of self-assigned black, mixed, or white skin colour with the number of black ascendants was obtained by weighted kappa. Weights were giving to the frequencies in each cell of the table according to their distance from the diagonal that indicates agreement [16]. The study was approved by the Ethics Committee of our Institution and all participants gave their informed consent to participate. Results Table 1 presents the characteristics of the 50 women included in the study. Most participants were white, had black or dark hair and 56% of them had no black ascendants. Table 1 Distribution of demographic, socio-economic and phenotypic characteristics of studied population Mean ( SD) or N (%) Age (years) 34,7 ( 12,6) School attendance (years) 8,6 ( 4,2) Self-assigned skin colour White 20 (40%) Light mulatto 6 (12%) Mixed 6 (12%) Dark mulatto 8(16%) Black 10 (20%) Observed skin colour* White 27 (54%) Mixed 13 (26%) Black 10 (20%) Hair colour* Black 18 (36%) Dark brown 16 (32%) Medium brown 7 (14%) Light brown 7 (14%) Blond 2 (4%) Hair style * Straight 13 (26%) Wavy 17 (34%) Curly 13 (26%) Afro 7 (14%) Colour of lines and hand palm surface * Pink palm and colourless lines 25 (50%) Pink palm and red lines 13 (26%) White palm and dark lines 12(24%) Number of black ascendants 0 28 (56%) 1 7 (14%) 2 5 (10%) 3 or more 10 (20%) * According to the observation of the majority of interviewers Table 2 shows that the agreement between self-assigned and observed skin colour was excellent for white and black women, but only good for mixed participants, resulting in a global agreement beyond chance of 0.75. Table 2 Agreement between self-assigned and observed skin colour of the participants Skin colour Overall agreement Kappa (95% Cl) White 95% 0.75 (0.720.78) Mixed 55% 0.61 (0.580.64) Black 90% 0.89 (0.860.92) Global 78% 0.75 (0.710.79) The presence of three or more black ascendants was highly associated with observed and self-assessed black skin colour, as well as the absence of black ascendants was associated with those considered whites (table 3). Approximately half of women self-assigned or observed as having mixed skin colour reported none black ascendant. Table 3 Association between number of black ascendants and observed or self-assigned skin colour Number of black ascendants Skin colour 0 1 2 3 p Self-assigned <0.0001 White 90% 5% 5% 0% Light mulatto 67% 33% 0% 0% Mixed 50% 17% 17% 16% Dark mulatto 38% 38% 12% 12% Black 0% 0% 20% 80% Observed <0.0001 White 81% 15% 4% 0% Mixed 46% 15% 15% 24% Black 0% 10% 20% 70% Table 4 presents the inter-observer agreement for phenotypic characteristics of the participants. Observed skin colour reached excellent agreement, while colour of lines and hands' palm surface, and hair colour had good agreement. Hairstyle presented an intermediate agreement, but the confidence interval was wide. Self-assigned skin colour was also associated with the colour of the lines and of the hands' palm surface (p < 0.0001). Pink and colourless palms' lines were observed in 95% of white women, 50% of light mulatto, 17% of mixed and 25% of dark mulatto. On the other hand, 90% of black women had dark lines and white hands' palm. Table 4 Interobserver agreement for participants' phenotypic characteristics Kappa (95% CI) Observed skin colour 0,79 (0,680,91) Colour of lines and hand palm surface 0,66 (0,550,78) Hair colour 0,60 (0,480,71) Hair style 0,48 (0,050,91) Discussion Race can be a useful term in the information about susceptibility to certain heritable conditions. In this use of the concept of race, it is possible to describe those who are susceptible according to their biological ancestry [7]. Thus, the race concept (populations showing some discrete trait) is a proper unit for such objective, and would include specific ancestral populations at risk and ethnic groups that may also be at risk if such a population was in the individual's genealogy. Epidemiological studies conducted in populations with racial diversity have investigated pathways of diseases in the social environment and its mediation by economic resources. For conceptual and practical reasons, the current preference to define ethnicity in such studies is for self-assessment [17]. However, ethnicity incorporates a concept that is not easy to measure with accuracy or validity and people may change their assessment over time [17]. Any method of characterisation of race is imprecise, but we are compelled in having some in order to improve caring for people. In Brazil, the self-definition of race include unsounded and hardly interpretable words which makes difficult the comparison between national and international studies. We collapsed the categories light mulatto and dark mulatto within a mixed category since they are difficult to interpret as white or black. In Brazil mulatto is another word for mixed race. In this study, both observed and self-assigned skin colour achieved excellent global reliability. The agreement suggests that health researchers may adopt any of these methods to collect information on race if some drawbacks were overcame. First, the observer's assessment of skin colour depends on training and a clear definition of the categories [11] and to reassure the level of training, inter-observer agreement should be determined. The agreement between the researchers was poor at the starting of the training and improved to an excellent agreement at the end of the training. A similar performance in inter-observer agreement was described in a previous study conducted in the same population of origin of the present sample [18]. In this study, the assessment of mixed skin colour reached a modest overall agreement, despite the intensity of training. Although the observation was carried out under the sunlight on less exposed surface of the arm, the perceived colour varies among people. It may also be affected by the skin colour of the observers, an aspect that could not be explored because all of them were white. Southern Brazil is a tri-racial mixture of Caucasians, Negroes and Indians [12]. Most of the population is Caucasian, predominantly Portugueses, Germans and Italians. Indians represent a very small fraction of the population. Mulattos were more numerous than negroes. Accordingly, the assessment of race was based on self-identification is not clear-cut. The disadvantage was that reported skin colour was expressed by local terms that needed to be decoded, such as light mulatto or dark mulatto [10]. These terms needed to be translated into conventional categories, as white, mixed or black. In this study we grouped all non-white or black women into mixed colour [10]. The self-assigned skin colour method may be biased in Caucasians, that over-estimate their skin pigmentation, and by individuals who do not consider themselves fair [19]. Our data show that the definition of race through the number of black ascendants may be a very good option. Regardless of training to observe skin colour or grouping local terms for self-assigned skin colour, the association of number of black ascendants with skin colour was highly significant. Most of white women had no black ascendants as well as black women had three or more. The agreement among the research assistants on colour of lines and of hands' palm surface, hair colour and style attained good kappa coefficients, but the confidence intervals were wide. Although the intensive training in order to standardise data collection of phenotypic characteristics, those physical attributes were not assessed independently of skin colour [10]. Conclusions In conclusion, our study showed that the assessment of race based on the race of the ascendants informed by the individual has a reasonable agreement with the ascertainment done by trained interviewers and with the self-report of race. This approach may save time on training of interviewers and may overcome potential measurement biases of other methods of race definition. Competing Interests None declared. Pre-publication history The pre-publication history for this paper can be accessed here:
Background Because both public health surveillance and action are crucial, the authors initiated meetings at regional and national levels to assess and reform surveillance and action systems. These meetings emphasized improved epidemic preparedness, epidemic response, and highlighted standardized assessment and reform. Methods To standardize assessments, the authors designed a conceptual framework for surveillance and action that categorized the framework into eight core and four support activities, measured with indicators. Results In application, country-level reformers measure both the presence and performance of the six core activities comprising public health surveillance (detection, registration, reporting, confirmation, analyses, and feedback) and acute (epidemic-type) and planned (management-type) responses composing the two core activities of public health action. Four support activities  communications, supervision, training, and resource provision  enable these eight core processes. National, multiple systems can then be concurrently assessed at each level for effectiveness, technical efficiency, and cost. Conclusions This approach permits a cost analysis, highlights areas amenable to integration, and provides focused intervention. The final public health model becomes a district-focused, action-oriented integration of core and support activities with enhanced effectiveness, technical efficiency, and cost savings. This reform approach leads to sustained capacity development by an empowerment strategy defined as facilitated, process-oriented action steps transforming staff and the system. Background Because public health surveillance and action are crucial to effective public health practice, the World Health Organization (WHO) has initiated consensus meetings at the regional and national level to review and reform surveillance and action systems [1-4]. These meetings emphasized improved epidemic preparedness and epidemic response. They also highlight the need to facilitate and standardize surveillance and action assessments and to include integration strategies in the reform process. In response, the WHO Regional Office for Africa (WHO/AFRO) has recently initiated the Integrated Disease Surveillance (IDS) project [5,6]. This effort uses the conceptual framework described in this report, which was effectively pilot tested and used to develop a 5-year plan of action (PoA) during the implementation of IDS in Tanzania ([7] and Nsubuga P, Eseko N, Wuhib T, Chungong S, Ndayimrije N, and McNabb SJN; Centers for Disease Control and Prevention, Tanzanian Ministry of Health, and WHO; in press). WHO/AFRO have adapted, further piloted, and subsequently adopted this framework for public health surveillance assessments (Phase I of IDS) in Africa. The importance of surveillance and action reform is fundamental to reducing national and international threats of infectious diseases [8]. Renewed threats to health posed by emerging and re-emerging infectious diseases (IDs); worldwide efforts to eradicate polio and eradicate dracunculialus and leishmaniasis; and the evolving drug-resistance of strains of tuberculosis, malaria, cholera, and Streptococcus pneumoniae have prompted an evaluation of the performance of national-level systems of public health surveillance and action [9-11]. In many countries, IDs continue to be substantial causes of mortality, morbidity, and rising health-care costs and must be carefully monitored and controlled ([12-14] and Wuhib T, Chorba TL, Davidiants V, MacKenzie M, and McNabb SJN; Centers for Disease Control and Prevention and Armenian Ministry of Health; unpublished manuscript). Any gaps, inaccuracies, or delays in surveillance and ineffective or inefficient public health actions are revealed both by these renewed ID threats and closer evaluations. Many countries recognize internal problems with poor performance  or lack  of public health surveillance and action [1]. At the national level, the use of duplicative, independent, vertical public health surveillance systems (e.g., one system for tuberculosis, another system for malaria), while keeping surveillance close to action may result in the redundant use of personnel, excessive costs, and ineffective or inefficient actions. Further, some high priority diseases may receive less attention because both technical and financial support for vertical surveillance systems may come from outside a country's borders. Developing countries' interests may not always be a top priority for duplicative, independent, vertical surveillance systems. Such a situation results in the use of differing surveillance terminology, methods  including analyses and reporting procedures  and actions that overload health workers. This may lead to discouragement and poor performance. To facilitate and standardize national-level assessments and to create a user-friendly method of national-level reform, we designed a conceptual framework in which surveillance and action reside as interdependent processes (Figure 1). This intuitive framework is not only an easy-to-use process of assessing surveillance and action, but also can be used as a pattern for reform. The two processes compose an open system, defined as the continual input of new case-patients [15]. We categorized this framework into eight core and four support activities that can be measured easily by using well-defined, country-specific indicators. Figure 1 Conceptual framework of public health surveillance and action Methods In the early years of modern public health (1940  1960), the term surveillance was applied to the collection, analysis, interpretation, and dissemination of (health outcome-specific) data to those who need to know[16]. Later, public health surveillance was defined as the ongoing systematic collection, analysis, and interpretation of outcome-specific data for use in the planning, implementation, and evaluation of public health practice[17]. Rephrasing the latter meaning, surveillance data are collected at the health facility  the first level of contact of the patient with the health system  then analyzed, interpreted, and used for action. Surveillance, per se, does not include the public health action(s) resulting from the interpretation of the data. Few envisaged the inherent responsibility of surveillance practitioners (i.e., those public health officials responsible for interpreting the data collected) for prevention and control actions. However, as early as 1963, the international public health community through the WHO recognized the importance of linking public health surveillance to public health action [18]. In this conceptual framework, public health surveillance comprises six core activities: 1. detection 2. registration 3. confirmation (both epidemiologic and laboratory) 4. reporting 5. analyses 6. feedback Two core activities comprise public health action: 1. acute (epidemic-type) responses 2. planned (management-type) responses All public health surveillance and action core activities are enabled by the four support activities of 1. communication 2. training 3. supervision 4. resource-provision Case-patient detection (core activity #1) is the first step of this framework (Table 1). Defined as the public health circumstance or event that identifies a (presumptive) case-patient as such by the public health system, case-patient detection usually occurs at the health-facility level. Although usually effected by a health-care provider (e.g., a private physician, nurse, community public health worker, volunteer, or a paid MoH practitioner), the laboratory may also play a role in detection (see also core activity #3 below). Table 1 Idealized Distribution of Public Health Surveillance and Action Core and Support Activities in a Country with a District-Oriented 1 Public Health Structure, by Organizational Level. Organization Level Surveillance Core Activities Action Core Activities Support Activities Detection Registration Confirmation (Epidemiologic and Laboratory) Reporting Analyses Feed-back Acute (Epidemic-Type) Response Planned (Management-Type) Response Communication Training Supervision Resource-Provision Health X X X X X X X X X X Facility Health X X X X X X X Facility Lab District X X X X X X X X X District Lab X X X X X X X X Regional X X X X X X X X Regional Lab X X X X X X National X X X X X National Lab X X X X X 1 serving 250,000500,000 persons Specified descriptive variables are then entered or registered into a public health record (core activity #2). Case-patients may be detected but not be registered. Once registered, case-patients either remain unconfirmed or become confirmed (core activity #3). Confirmation occurs through the evaluation of epidemiologic criteria and/or laboratory test results. Epidemiologic confirmation involves intensive case-patient investigation in the field (e.g. household or workplace). Laboratory tests help confirm or rule out diagnoses of registered case-patients. By detecting new, previously unreported case-patients through routine laboratory testing, the laboratory itself can serve as a secondary or primary surveillance system by detecting, registering, and reporting case-patients. Some laboratories provide ongoing reporting of new health outcomes (e.g., antibiotic resistance). Reporting (core activity #4) of case-patient data involves the movement of public health surveillance data collected from lower levels of the health system (e.g. health facilities) to higher ones (e.g., district or national offices). Once data have been received at the appropriate health level, they are analyzed (core activity #5). Analyses should be done as close to the primary reporting level as possible so there is minimal delay in implementing the appropriate interventions to prevent disease. Interventions to improve data analyses might include training in analytic methods, including data presentation (standardized charts and graphs) and the establishment of indicator targets designed for public health action. Often, reported data are numbers. Analyses of numbers produce results. Results of analyses of surveillance data are the end points of public health surveillance. However, for collected data to lead to action, they must be interpreted or transformed into public health information and then to public health messages that are used for public health action. Feedback (core activity #6) is the flow of information and messages back to lower levels from higher ones. Targeted interventions to correct poor feedback might include such efforts as providing timely and regular messages from the national level to the health-facility level on the basis of locally provided data. Even though public health surveillance and action are interdependent processes (Figure 1), they relate through inflow and outflow of data-information-message (i.e., interpretation) (Table 2). Table 2 Example of Data-Information-Message Data: There are 100 case-patients and five fatalities of measles in Region X in one month, with an incidence of 90/10,000 persons. Information: This is a 50% increase in measles over last year at this time. Message1: There is an epidemic of measles in Region X that requires immediate public health action. Every infant in Region X can be vaccinated for less money than it takes to treat the 100 case-patients and five fatalities. 1 Messages are subjective statements about information. They enable the target audience to put the information into the proper context for action. This transformation of data to information to messages can been defined as a process, which is a series of actions or operations, always in motion, directed toward a particular goal [19]. The output of public health data-information-messages (interpretation) yielded from surveillance should never be separated from public health action. Public health action continuously influences public health surveillance by providing public health interpretation to guide any modifications in the content or scope of surveillance. Two core public health actions of acute (epidemic-type) (core activity #7) and planned (management-type) (core activity #8) responses rely upon messages derived from surveillance. Acute (epidemic-type) responses occur directly, reactively, and generally include immediate public health actions (e.g., epidemic investigation, contact follow-up, or targeted interventions designed to stop the ongoing transmission of disease). Planned (management-type) responses occur with periodicity over time and require a vision of future needs. Examples of such responses include community public health education, purchasing next year's immunization supplies, ordering tuberculosis medication in anticipation of future needs, or reallocating public health personnel and resources in response to changing trends of disease. Public health actions, in turn, must be measured, evaluated, and the results used to not only measure and modify the control and prevention measures taken, but also to guide future modifications in public health surveillance. Four support activities promote or improve the core activities by enhancing their performance through more efficient and effective functioning (Table 1). Core activities can and do occur with or without support activities. Generally, the more support, the better the performance. Communication (support activity #1) usually proceeds from public health authorities. By definition, communication is a process that involves at least two people in an effort to convey, receive, interpret, and agree upon the meaning of data, information, or messages[20]. Communication includes the provision of public health messages through public health bulletins within a country, or to other countries, and also to supra-national organizations (e.g., WHO, United Nations International Children's Emergency Fund, or the World Bank). Training (support activity #2) and supervision (support activity #3) facilitates day-to-day operations. Interventions might include motivating public health workers through training and supervision and taking appropriate public health actions in a timely manner. Other interventions, such as training in decision-making, management, and communications could tie the development of analytic skills and knowledge to applied broad public health practice competencies. Resource-provision (support activity #4)  e.g., the availability of funds, trained personnel, and materials such as communications infrastructure (i.e., telephone, fax, or computer), electricity, gasoline, or vaccine-promotes or improves all eight core activities. The framework uses the concepts, goals, and objectives of surveillance and action to provide the development of flexible, yet objective, indicators, by each core and support activity and by health-care level, for any country (Table 3). Indicators  used here as measures of performance  can be customized to adapt to surveillance systems throughout the world. Table 3 Sample Indicators Measuring the Performance of Public Health Surveillance and Action Core and Support Activities Public Health Surveillance and Action Activity or Approach Sample Indicator Detection Proportion of sites where the community has reported cases within past the past year Registration Proportion of sites with a currently maintained registry Confirmation (Epidemiologic and Laboratory) Proportion of sites with standardized case definitions for all reportable diseases (having implemented them) Reporting Proportion of sites having submitted all four previously required reports Analyses Proportion of sites with appropriate denominator data Feedback Proportion that received any type of feedback from a higher level Acute (Epidemic-Type) Response Proportion of sites involved in, conducting, or that conducted an epidemic investigation within the past six months Planned (Management-Type) Response Proportion of sites that have implemented community-wide prevention and control measures based on local data within the past year Communication Public health bulletin published and distributed quarterly Training Training received in general epidemiology and public health surveillance Supervision Surveillance activities were supervised during the previous six months Resource-Provision Telephone service available Empowerment Proportion of district-level MoH staff involved in Phase 2; MoH steering and working committees meet on a regular basis Results The authors propose the application of this conceptual framework of public health surveillance and action using a five-phase approach to national-level reform. Using the framework of eight core and four support activities as a road map, public health practitioners can assess the existing surveillance and action system, propose a model for a future system, and develop and implement a plan of action (PoA) to achieve the model through a series of process-oriented action steps. These action steps yield not only measurable outputs and outcomes (by virtue of objectives with indicators), but also develop measurable sustained capacity among MoH staff (by virtue of objectives with indicators). This five-phase approach begins with Phase 1  preparation for the reform effort and ends with the full implementation of the reform model. Phase 5 involves the evaluation and monitoring of the entire process toward achieving the country-level, district-oriented reform model. Each process-oriented phase yields specific and measurable outputs and outcomes, monitored by objectives that guide the next steps. Phase 1  preparation The overall design and implementation of all phases should proceed under the direction of a MoH steering committee (advisory board) that includes high-level decision makers and also representatives from the health facility and district level. The WHO Country Representative can serve as a link between the MoH and other partners. This steering committee should gain consensus and support from other MoH officials, donor organizations, non-governmental organizations (NGOs), and other key players to explain findings, gain understanding and support for the process and to build consensus on reform. The outcome of this phase is a commitment to the entire reform process. The steering committee will provide each organization with copies of reports; discuss major findings, interpretations, and conclusions; identify the specific interests and suggestions for improvement from stake holders; respond to questions; and invite participation in processes and debriefings. Phase 2  assessment of public health surveillance and action To assess the current status of surveillance and action, the MoH uses indicators (Table 3) to measure the existing presence and performance of the multiple, independent, and vertical public health surveillance and action systems using the eight core and four support activities. This assessment involves a structured examination at all health levels: national, regional, district, and health-facility. The indicators are captured by questions (both in interview and observation format) and framed into health-level specific questionnaires. The outputs from Phase 2 are objective findings that guide practitioners in the development of a comprehensive PoA with targeted interventions conceptualized, planned, and budgeted during Phase 3. Phase 2 could proceed under the direction of a working committee appointed by the steering committee. The group would also include representation from outside technical partners (e.g., WHO) and the national, regional, district and health-facility levels. The Phase 2 working committee will complete the initial analyses of the assessment data and prepare a draft report of the findings to be presented to the steering committee. A draft report is left with the MOH and includes recommendations. Finalization of the document is then done by the MoH. Phase 3  development of a Plan of Action (PoA) Following completion of Phase 2, the MoH will review the objective findings and develop a detailed PoA using crosscutting and surveillance themes. Cross-cutting themes may include integration, district-level focus, budget, action-oriented approach, and advocacy. Phase 3 should proceed under the direction of another working committee, appointed by the steering committee. This working committee should involve staff from the Phase 2 working committee and key stakeholders from the various vertical programs involved. The working committee of Phase 3 develops a first-draft PoA based on the comprehensive review of the assessment findings. The working committee is empowered to envision the final reform model, determine the major weaknesses that should be addressed to achieve it, and identify the appropriate action steps. This workshop process is conducted using the conceptual framework of the stepwise surveillance core and support functions as a road map, beginning with case-patient detection and working through all eight core and four support activities, with the reform model in full view. The working committee then identifies the process-oriented action steps to achieve each specific objective, including implementation indicators, time-lines, the organization or group primarily responsible for implementation, required resources, and means of overcoming potential obstacles (Table 4). Table 4 Example of Taking One Finding from Phase 2 and Developing a Detailed Plan of Action Finding Recommendations Objectives Indicators Action Steps Only 3/21 reportable diseases have standard case definitions (SCDs) * All reportable diseases should have SCDs for suspected, probable, and confirmed case-patients. The Ministry of Health (MoH) should select or develop case definitions appropriate for various types of health facilities/providers* SCDs are established for every reportable health condition SCDs are developed for each reportable health condition 1) The MoH should establish a working group 2) Review existing SCDs 3) Select/develop a proposed SCD for each reportable disease* 4) Conduct consensus workshops at district and national levels * This information can be developed in much greater detail, e.g. tiered case definitions (e.g. suspect, probable, and confirmed) can be developed for different diseases and laboratory capabilities can be specified for various levels. Phase 4  implementation Phase 4 focuses on carrying out the process-oriented action steps developed and described in the PoA as necessary to achieve goals and objectives. Phase 4 is the most critical and difficult phase, and often requires financial support from outside agencies. Advocacy both within and beyond the MoH is critical to final approval of the PoA. Once formally approved by the MoH, in many countries, advocacy to donors is critical. These activities are processes and include specific outcomes and indicators that measure progressive change at all levels. They are conducted over the life of the project, beginning with the completion of Phase 3 in the first year. Phase 5  monitoring and evaluation During Phase 5, there is a determination of whether the planned changes are occurring by measuring progress. Monitoring and evaluation provides guidance to not only overall reform progress, but also helps identify problem areas in implementation of the PoA. This phase is conducted over the entire lifespan of the project, beginning with the initiation of Phase 1 in the first year. Discussion The authors describe here a conceptual framework of the interdependent processes of public health surveillance and action. These two processes comprise eight core and four support activities. The presence and performance of these activities can be measured using well-defined indicators that yield information. The information can then guide a comprehensive national-level reform by identifying gaps and opportunities for integration and by targeting interventions designed to improve the technical efficiency and effectiveness of both public health surveillance and action. These efforts will reduce costs. Thacker and Klaucke et al. advanced public health surveillance in 1988 with their strategy for surveillance assessment [21,22]. Their strategy includes an evaluation of the components of public health importance, objectives and usefulness, operation of the system, cost, and system attributes (simplicity, flexibility, acceptability, sensitivity, predictive value positive, representativeness, and timeliness). It involves both qualitative (e.g., simplicity) and quantitative (e.g., predictive value positive) measurements [23,24]. This evaluation strategy is primarily disease specific and may not be practical when applied to national-level public health surveillance and action reform efforts that involve the evaluation and consequential modification of entire country-level, multiple surveillance systems. The Thacker and Klaucke model is used most effectively when a single health indicator (e.g., tuberculosis, hepatitis A, or malaria) is being evaluated. It is less useful to evaluate (concurrently) multiple surveillance systems or when the surveillance systems being assessed monitor multiple health indicators. For example, this model does not allow sensitivity or predictive value positive to be calculated for a surveillance system that captures multiple health indicators simultaneously. It does not identify areas amenable to integration. The authors' goal has been to develop an evaluation tool and provide a road map for national-level reform efforts. The framework proposed here provides specific and objective data to measure the structure (i.e., a description of the number and distribution of regional, district, and health facilities), presence (i.e., system existing at the respective health-care level), and performance (i.e., meeting the minimum standards established by the MoH and required by the indicator chosen at each health-care level) of multiple public health surveillance systems at a national level. The framework is comprehensive, organized, consistent, flexible, diagnostic, action-driven and oriented, and easy for MoH staff to use. Further, it is designed to build sustained capacity and can be used to economically evaluate both the current and reformed efforts. The framework also promotes a "public health action-led" rather than "surveillance (data)-led" model that closely meets the expressed needs of many MoHs [25]. It meets the goal of public health surveillance; namely, to use public health information to guide the system to take the appropriate public health action [26]. The framework is most useful in an applied context. Those countries in the process of political and economic reform or those that do not already have a well-established surveillance infrastructure may more easily use this approach than countries with well-established surveillance systems. It is intuitive for MoH field staff who might not have extensive training in surveillance (Nsubuga P, Eseko N, Wuhib T, Chungong S, Ndayimrije N, McNabb SJN; Centers for Disease Control and Prevention, Tanzanian Ministry of Health, and WHO; in press), and it permits economic analyses of surveillance and action (Carande-Kulis V, Aldrich M, Messionier M, and McNabb SJN; Centers for Disease Control and Prevention; unpublished manuscript). A model of public health surveillance and action reform must adapt to the ongoing political, economic, and social realities of three concurrent movements: 1) decentralization, 2) integration, and 3) primary health-care delivery [27,28]. Funded in large part by international donors, these movements drive the reform model of public health. Decentralization of public health practice accompanies the peripherally shifting movement of other governmental functions [27,28]. During decentralization, political power, authority, responsibility, resources, communications and transportation capacity, person power, and autonomy shift peripherally to a highly focused, more efficient and autonomous district level (one serving 250,000  500,000 persons). Public health surveillance systems have traditionally existed in a top-heavy and disease-specific (or vertical) form. With the governmental shift toward decentralization, the integration of public health-related activities, management responsibilities, and services becomes critical to the efficient performance of public health practice. With the integration of health-care services, information needs and uses and public health responses can change. The movement toward the integration of surveillance and action implies the ultimate focus of reform should move from a program-specific focus (e.g., infectious diseases) toward integration with other health problems (e.g., maternal and infant mortality). Accompanying decentralization and integration is emphasis on developing a primary health-care model by restructuring and combining essential health services, including public health services at the district level. This model has the flexibility to include non-infectious disease health outcomes. What then does the public health reform model envision, and how should it be achieved? Directed through process-oriented action steps, the conceptual framework of public health core and support activities described here provides the underpinning for the practical outworking of reform. Its application envisages the creation, over time, of an integrated, district-focused, and action-oriented system of public health practice, including both reformed public health surveillance and action with enhanced effectiveness, technical efficiency, and cost savings. This vision should be achieved by an empowerment approach that leads to sustained capacity development. If sustained capacity development is desired, empowerment should be the central underlying strategy of public health practice reform. Defined as a series of facilitated process-oriented actions leading to transformation (i.e., improvement in public health judgment and performance and increased competency in various areas of public health practice), empowerment builds sustained capacity. Beginning from phase 1, the respective MoH staff should lead each phase of reform. The active involvement and central role of the respective MoH staff in each phase increases the likelihood of ownership, acceptability, and relevance to local conditions of the final model of reform that is adapted. The processes of public health surveillance reform, which lead to sustained capacity development, are as important as the final reformed model itself. This transformation process is focused on people as well as systems. Indeed, the sustained development of capacity is measured by MoH practitioners who are empowered and trained to think independently; react appropriately to changing public health circumstances; and develop new public health strategies. The application of reform may require additional donor funding, often from external international sources. Technical and financial assistance can come from other external facilitator-partners (e.g. WHO, United States Agency for International Development, United Nations Foundation, and the Centers for Disease Control and Prevention), but their role should be limited to the provision of tools, resources, critiques or other technical guidance. It requires time to implement reform strategies in an objective-based PoA. Abbreviations (IDs) infectious diseases (IDS) Integrated Disease Surveillance (MoH) ministry of health (NGO) non-governmental organizations (PoA) plan of action (SCD) standardized case definitions (WHO) World Health Organization (WHO/AFRO) Regional Office for Africa Competing interests We certify that we have participated sufficiently in the conception and design of this work, as well as its execution and the analyses of the data. Further, we have collaboratively written the manuscript and take public responsibility for it. We believe the manuscript represents valid work. We have reviewed the final version of the submitted manuscript and approve it for publication. Neither this manuscript nor one with substantially similar content under our authorship has been published or is being considered for publication elsewhere. If requested, we shall produce the data upon which the manuscript is based for examination by the editors. We certify that we have no affiliations with or involvement in any organization or entity with a direct financial interest in the subject matter or materials discussed in the manuscript. Drs. McNabb, Wuhib, Nsubuga, and Carande-Kulis were employees of the U.S. federal government when this work was performed and prepared for publication; therefore, it is not protected by the Copyright Act, and there is no copyright of which the ownership can be transferred. Dr. McNabb serves as corresponding author; his address is listed below. Pre-publication history The pre-publication history for this paper can be accessed here:
Background Before 1991, the infectious diseases surveillance systems (IDSS) of the former Soviet Union (FSU) were centrally planned in Moscow. The dissolution of the FSU resulted in economic stresses on public health infrastructure. At the request of seven FSU Ministries of Health, we performed assessments of the IDSS designed to guide reform. The assessment of the Armenian infectious diseases surveillance system (AIDSS) is presented here as a prototype. Discussion We performed qualitative assessments using the Centers for Disease Control and Prevention (CDC) guidelines for evaluating surveillance systems. Until 1996, the AIDSS collected aggregate and case-based data on 64 infectious diseases. It collected information on diseases of low pathogenicity (e.g., pediculosis) and those with no public health intervention (e.g., infectious mononucleosis). The specificity was poor because of the lack of case definitions. Most cases were investigated using a lengthy, non-disease-specific case-report form Armenian public health officials analyzed data descriptively and reported data upward from the local to national level, with little feedback. Information was not shared across vertical programs. Reform should focus on enhancing usefulness, efficiency, and effectiveness by reducing the quantity of data collected and revising reporting procedures and information types; improving the quality, analyses, and use of data at different levels; reducing system operations costs; and improving communications to reporting sources. These recommendations are generalizable to other FSU republics. Summary The AIDSS was complex and sensitive, yet costly and inefficient. The flexibility, representativeness, and timeliness were good because of a comprehensive health-care system and compulsory reporting. Some data were questionable and some had no utility. Background Health information systems (HIS) provide a scientific and technological framework to gather, manage, and interpret data to inform the public, policymakers, administrators, and health-care workers about the distribution and determinants of health conditions. Further, they can (and should) guide and measure the impact of interventions [1]. Public health surveillance  a subset of HIS  has been defined as the ongoing, systematic collection, analysis, and interpretation of outcome-specific data for use in the planning, implementation, and evaluation of public health practice[2]. Public health surveillance can be used to 1) assess the overall health status of a population, 2) describe the natural history of disease, 3) monitor disease trends, 4) detect epidemics, 5) evaluate the effect of prevention and control measures, 6) generate hypotheses, and 7) facilitate epidemiologic and laboratory research [3]. Before 1991, the Soviet Union centrally planned the infectious diseases surveillance systems (IDSS) of its 15 republics. Approximately 300 million persons were covered under the IDSS. Central monetary and technical support for the IDSS ended in 1991. As a result, the republics have struggled to maintain their respective IDSS. The former Soviet Union (FSU)-wide diphtheria outbreak in the 1990s [4] and the re-emergence of malaria in Tadjikistan in 1991 [5] and in Armenia [6] and Azerbaijan [7] in 1994 indicated that financial constraints resulting in the disruption of public health infrastructure and services had increased the risk of the re-emergence of infectious diseases [8]. After 1991, this transition to nationalism, privatization, and social reorganization posed new challenges to each republic of the FSU. The loss of centralized training, public health expertise, and resources especially impacted the surveillance systems in each republic. [9] Further, privatization of the FSU medical systems resulted in under-budgeted public health services and inadequately paid personnel [10]. Combined with these infrastructure problems, the increased population migration  both within the FSU and internationally  contributed to morbidity and mortality through population dislocation [8]. This has increased the risk of exposure to re-emerging microbial and environmental pathogens and limited access to health services and good nutrition [9,10]. The most important of these changes was the financial crises resulting from the severance of economic ties among all republics of the FSU. Public health officials were challenged to transform the bulky, state-sponsored IDSS. With high inflation and unemployment, they also suffered from shortages of vaccines, hospital supplies, and essential drugs. Provision of basic public health services were compromised, including repairing antiquated water and sewer systems, resulting in increased risk for gastroenteritis and infections with hepatitis A [11]; the largest documented outbreak of typhoid in this century occurred in Tajikistan in February 1997 [12]. Practices such as reusing syringes during vaccination and poor sterilization procedures during dentistry have contributed to nosocomial outbreaks of HIV and a high prevalence of infections with hepatitis B [13]. The objective of this work was to assess the current status and functioning of various IDSS, so as to guide reform efforts. At the invitation of seven Ministries of Health (MoHs), we performed assessments in the Russian Federation and in the Republics of Kazakhstan, Tadjikistan, Uzbekistan, Turkmenistan, and Armenia, and in the Kyrgyz Republic. We found striking homogeneity in comparing the IDSS from one republic to another; and, for clarity, we present here representative findings by using the Armenian component of the IDSS (AIDSS) as a prototype. Discussion We used the CDC guidelines to assess the seven IDSS [14]; these guidelines have recently been republished in revised form [15]. This strategy includes an assessment of public health importance, objectives and usefulness, operation of the system, cost, and the seven system attributes (i.e., simplicity, flexibility, acceptability, sensitivity, predictive value positive, representativeness, and timeliness). The assessment involves gathering both qualitative (i.e., simplicity, flexibility, and acceptability) and observations of the quantitative (i.e., sensitivity, predictive value positive, representativeness, and timeliness) attributes. In Armenia, we attempted to gather as much information as possible with respect to the construct and utility of the AIDSS from those most integral to its functioning and application. Therefore, we conducted face-to-face interviews and focus group discussions with approximately 50 epidemiologists at the Ministry of Health, the National Sanitary Epidemiologic Service, the Institute of Epidemiology, and two regional, three districts, and one city Sanitary Epidemiologic Service (SES) office. We also interviewed 23 health-care workers at village health centers, polyclinics, hospitals, and laboratories, which serve as the primary reporting units for the AIDSS. At the time of this assessment, the AIDSS was moving toward reform, and we chose not to use the limited resources to gather quantitative data (e.g., through chart reviews) to assess the quantitative attributes of the AIDSS (i.e., sensitivity, predictive value positive, representativeness, and timeliness). Rather, we relied on qualitative observations gleaned in the course of the interviews. We present here the qualitative observations made of the AIDSS and recommendations for reform. We believe these observations and recommendations reflect the status of the IDSS of the other FSU republics. Description of the AIDSS A republic of 3.3 million, Armenia gained its independence from the FSU on September 21, 1991 (Figure 1). The country is divided into 11 regions, which are subdivided into 37 districts (Figure 2). Responsibility for the AIDSS rests with the Department of Hygiene and Epidemiologic Surveillance  a department of the Armenian MoH (AMoH)  composed of 52 functional units known as the Sanitary Epidemiologic Service (SES). The units or stations of the SES parallel the geopolitical divisions of Armenia. There exists one SES station in each district. Each city has one SES station, except Yerevan, the capital. Yerevan has eight districts, each with one SES station. The national SES is also located in Yerevan. Figure 1 The Caucasus region of the New Independent States (NIS) Figure 2 The Sanitary Epidemiologic Stations (SES) The SES, per se, developed from a model created in the late 1800s in russia and uniformly developed over many years in the republics of the FSU. Its principle functions are to collect and analyze public health surveillance data and to implement and enforce strategies for the prevention and control of infectious diseases. Traditionally, the SES had approximately 10% of the entire medical person-power and budget of the AMoH, and was separate from the curative medical care system [16,17]. The SES was staffed by epidemiologists (physicians), microbiologists, sanitary hygienists, and other health workers (paramedics and physician assistants). The district SES was the basic public health unit that monitored infectious diseases, investigated outbreaks, attended to child and adolescent health, inspected the food-service industry, monitored water purity, and dealt with occupational and environmental health problems throughout Armenia [6]. City-, regional-, and national-level SES administrations were larger, with specialized staff. The SES collected infectious diseases data from all health-care facilities throughout the country. Before 1991, Armenia had a comprehensive and free health-care delivery system accessible to all citizens with health facilities and health-care workers employed under the auspices of the AMoH. Outpatient facilities (village health centers and polyclinics) and hospitals reported to the AIDSS. Each district had 1245 village health centers, two polyclinics, and one hospital. The seven cities in Armenia had variable numbers of polyclinics and hospitals. Altogether, Armenia had 830 village health centers, 228 polyclinics, and 179 hospitals (Figure 3). Data from these health facilities were reported to the 52 districts and city SES and then to the national SES, which forwarded aggregated data to the Department of Hygiene and Epidemiologic Surveillance in the AMoH. Figure 3 The flow of information, infectious diseases surveillance system, Republic of Armenia, 1996 Objectives and Data Collection, Reporting, Analyses, and Response The objectives of the AIDSS were to identify cases of infectious diseases, document outbreaks, and monitor trends in disease occurrence. It collected aggregated and case-based data on new cases of 64 infectious diseases. This list included some diseases with low pathogenicity (e.g., pediculosis and scabies) and some diseases with inadequate or non-existent preventive measures (e.g., infectious mononucleosis and parapertussis). There existed no tiered (e.g., confirmed, probable, and suspected) standardized case definitions. The reporting of confirmed or suspected cases of some infectious diseases required immediate reporting via telephone or in person within 1224 hours. These included epidemic prone diseases (e.g., diphtheria, polio, plague). Data were provided monthly and yearly from the district to national SES. Reports from the national SES were sent monthly to the AMoH and the other 14 republics of the FSU and back to the district SES twice a year. The district and city SES reported immediately to the national SES. At all levels, epidemiologists used descriptive statistics for data analyses. These included the calculation of aggregated case numbers and incidence and prevalence rates based on estimates of the population size provided by the state agency charged with gathering census data; limited stratification by person, time, and place; and the assessment of trends. Epidemiologists did not use analytic methods to assess risk factors for diseases, even though they were collected. Cases and contacts of every disease were investigated by a district or city SES epidemiologist within 24 hours of the receipt of the case report, using a standardized form known as the epid carta. This form was not disease specific, yet lengthy (46 questions, many of which required subjective responses). Little or no feedback was provided to the original reporting sources and no routine or formal sharing of data and information occurred between the district SES and health-care facilities. Table 1 summarizes the public health practice activities in Armenia, stratified by health facility. Table 1 Public health surveillance and action core and support activities, by health level, Republic of Armenia, 1996 Organization Level Public Health Surveillance and Action Core and Support Activity Detection Registration Confirmation (Epidemiologic and Laboratory) Reporting/ Feedback Analyses Acute (Outbreak-Type) Response Planned (Management-Type) Response Communication Training Supervision Resource-Provision Primary Health Facilities X X X X X X X X X X District SES X X X X X X X District Lab X X X X X X X X District Lab X X X X X X X Regional SES X X X X X X X Regional Lab X X X X X National SES X X X X National Lab X X X X Qualitative Attributes Simplicity Referring to both its structure and ease of operation, the AIDSS was complex. Epidemiologists gathered voluminous information on each case; parallel vertical public health programs reported duplicative information; much time was required to collect, register, and report case information; and many staff maintained the system. Flexibility and Acceptability Addressing the extent to which the AIDSS could adapt to changing information needs or operating conditions and reflecting the willingness of participants to provide information and monitor the system, the AMoH, itself, determined both its flexibility and acceptability. Because the AMoH provided salaries for health-care personnel, it could enforce compulsory reporting, through monetary fines and professional demotions. Changes in reportable conditions or criteria were made rapidly because administrative SES employees carried out orders quickly and completely at all levels in the system. Quantitative Attributes While we did not quantitatively measure the attributes of sensitivity, predictive value positive, representativeness, and timeliness, we did make qualitative observations. Sensitivity Assessing the AIDSS's ability to account for all incidents of a disease (i.e., the proportion of cases detected, correctly diagnosed, and reported), we learned that detection of most reportable infectious conditions did come to medical attention. This was enhanced because all citizens received free health care and because primary care physicians were responsible for care (including community outreach) of persons in their assigned territories. However, due to constrained resources, laboratory confirmation of reportable infectious diseases was limited in practice. As such, reporting was based on clinical or epidemiologic, rather than laboratory information. Predictive Value Positive The likelihood that a disease report constituted a true case of that disease was diminished because of the lack of laboratory confirmation and standardized case definitions. Representativeness We felt that, ideally, the AIDSS accurately described the distribution of diseases in the population by person, time, and place because disease reporting was mandatory and failure to report was a punishable offense. And, because of the penalties, all official reporting to the national level did occur on a monthly basis. However, it was common practice for epidemiologists to conceal cases of infectious disease and willfully underreport epidemic morbidity, because outbreaks meant that the epidemiologists were not performing their duties of preventing and controlling infectious diseases. This paradox resulted in epidemiologists managing two sets of information: one officially reported to higher-ups and one unofficially kept (with more accurate numerators). Timeliness Information for action or for long-term planning was available because mandatory reporting to the SES within 1224 hours of diagnosis for most conditions under surveillance allowed rapid implementation of control and prevention measures. Costs Because budgets were relatively non-existent in the FSU, historical data on the costs of operating the AIDSS were not available. In the FSU centrally planned economy, resources were obtained from cost centers (e.g., utilities were not metered, office supplies were requested by quantity and not cost, and salaries were provided from the central budget). However, numerous observations led us to conclude that the AIDSS was relatively inefficient and costly; in large part, because the expenditure was paid from the public sector. The AIDSS was labor intensive. Being paper-driven, reporting dieases for which no practical public health interventions exist misallocated scarce resources. Other common public health activities with high opportunity cost in the AIDSS were indiscriminate disinfection of homes and work sites. Environmental background monitoring practices by district and regional SES included routine collection of specimens and laboratory testing (e.g., air and water samples, food products, and items that children may come in contact with such as toys or eating utensils) in addition to evaluation of physical factors (noise, vibration, microclimate, electromagnetic fields, levels of lighting, and ionic radiation) at several sites (e.g., work places and day-care centers). When cases of hepatitis A, acute gastroenteritis, tuberculosis, diphtheria, or pediculosis were reported, disinfection of homes, schools, day-care centers and work places was conducted by public health workers who used chloramine application and steam cleaning of all hard surfaces and laundering of all clothing and bedding materials. The effectiveness of such disinfection practices or environmental background monitoring has not been documented, and is likely of doubtful public health utility. During disease outbreaks, it was common practice to investigate every case and culture all available materials, and decontamination efforts were instituted regardless of epidemiologic evidence. It was common practice to hospitalize children < 1 year of age with pneumonia for 714 days, and children of all ages with acute gastroenteritis for 715 days. It was also common to hospitalize both adults and children with hepatitis A for 21 days, with syphilis for two weeks, with gonorrhea for three weeks, and with tuberculosis for one year. These isolation practices, meant to prevent disease transmission to the community, were consequences of central planning in which emphasis was placed on input indicators such as the occupancy rates of hospitals. Incentives (budget allocations) placed on input rather than output measures led to a level of medical infrastructure that has been difficult to maintain given current levels of funding available for the health sector. Because financial issues were a major driving factor, cost analyses of surveillance practices and control measures could identify areas for cost-savings. Analyses of the surveillance system in Ukraine (using 1996 budget figures) revealed that excessive culturing represented 47% of the cost per capita expenditure of the L'viv Regional SES and disinfection procedures accounted for almost 30% of the entire Pustomity District SES's budget (V. Carande-Kulis, CDC, personal communication). Recommendations Based on this assessment, we developed recommendations with respect to the three main surveillance functions of data collection, analysis and interpretation, and retrospective and prospective responses. Data Collection Eliminate punitive consequences to obtain accurate reporting; Restrict the number of routinely reportable diseases based on measures of mortality, morbidity, severity, communicability, and preventability; Categorize events under surveillance into a three-tiered surveillance system: disease elimination (e.g., polio); case-based (e.g., diphtheria); and indicator-based (e.g., number of children immunized by two years of age); Simplify reporting procedures and forms by limiting urgent reporting of diseases to those that require prompt institution of control measures; requiring only information necessary to direct control measures and perform basic analyses; and developing disease-specific forms with diseases chosen for case-based surveillance; Develop tiered (confirmed, probable, and suspected) standardized case definitions for all events under surveillance; and Computerize demographic and risk-factor data for systematic and detailed analysis of reported diseases and rapid dissemination of information. Analysis and Interpretation Provide ongoing capacity for training in analytic epidemiology; and Base interventions on epidemiologic evidence. Use analytic epidemiology (case-control and cohort studies and presentation of data using 2  2 tables, odds ratios, relative risks, and tests of significance) for hypothesis generation, risk factor identification, outbreak investigations and intervention design and monitoring. Retrospective and Prospective Responses Provide feedback to all reporting sources and share information across vertical program lines and with officials throughout the public health community in a timely fashion (e.g., via a monthly public health bulletin). A bulletin could include descriptions of important outbreak investigations, disease-specific analyses of surveillance data, graphic and tabular information on selected diseases, indicators of community health, and recommendations for public health concerns. Current HIS Reform Efforts in Armenia This assessment stimulated and guided reform efforts that were initiated in December 1992 through a cooperative project among the AMoH, United States Agency for International Development (USAID), and CDC. This project provided technical and material assistance toward reform of the Armenian HIS. The approach focused on training a cohort of public health officials and epidemiologists in the modern aspects of epidemiology, biostatistics, surveillance techniques, and scientific communications; developing Armenia-specific case definitions; facilitating HIS reform strategies through workshops and training sessions; and developing the capacity to publish an epidemiologic bulletin. Since 1996, this HIS reform activity has been self-sustained with no additional monetary support from USAID [6]. In 1996, the AMoH created a national HIS program for the development and reform of the HIS and the Armenian National Health Analytic-Information Centre [18]. The system has been transformed into a comprehensive HIS and includes chronic diseases, maternal and child health, and injury data. Diseases are now categorized by a three-tiered approach: disease elimination (e.g., polio), case-based (e.g., diphtheria) and indicator-based (e.g., number of children immunized by two years of age). Preparations are now being made for additional training and to assess and improve clinic case diagnoses, management, and recording, and clinic records. New regional centers equipped with computers and faxes have been organized for the collection, analyses, and reporting of health information. National and regional public health bulletins are being published monthly in three languages - Armenian, Russian and English  and distributed to wide audiences. Tiered, standardized case definitions and essential health indicators for decision-making at the clinic and community level have been developed and disseminated. These include health status, performance, and resource indicators. Comprehensive HIS reform is critical throughout the FSU. Timely, accurate, and relevant health information are necessary to assess the burden of disease and disability; understand changing health patterns; measure the needs for and improve services; address inequities in health; provide information for policy formulation and planning; and provide a basis for intra- and international comparisons on health status and care utilization [1]. Timeliness, accuracy, and relevancy are augmented by efficiency. Integrating all sources of data into one comprehensive HIS prevents duplicate recording and reporting across services and programs, averts labor-wasting inefficiencies, and saves scarce resources. A comprehensive HIS includes the capacity to obtain data from vital registries, clinical, administrative, and other records; from provider and population-based surveys and sentinel systems for infectious and chronic diseases, and disabilities; and maternal and child health, nutrition, and program implementation indicators, including access, coverage, and service quality [1]. This reform activity should include the development of an indicator monitoring system based on selected essential, action-oriented indicators of health status, service performance, and resources that can be used for decision-making at the local level. Summary We found the AIDSS to be a complex and sensitive, yet costly and inefficient surveillance system for infectious diseases. Despite the lack of standardized case definitions, feedback of information, and computer technology, it functioned fairly well before 1991. However, the functioning and continuation of the AIDSS has been affected both directly and indirectly by events of the past decade. Overall, the former AIDSS was useful because it detected cases of infectious diseases, estimated morbidity, monitored trends in disease occurrence, and documented outbreaks. The comprehensive no-cost health-care delivery system and compulsory reporting of diseases to the AMoH enhanced its flexibility, representativeness, and timeliness. The strengths of the AIDSS stemmed from the large numbers of health facilities and trained personnel and the separation of preventive from curative medicine, that secured an independent status (including separate budget) for preventive medicine and public health practitioners. The AIDSS also had weaknesses. Though the system meticulously tracked persons (from birth to death), very few of these data were computerized, analyzed, or used to develop, direct, or evaluate public health policy. In most cases, when used, data guided regulation and punishment rather than public health decision-making. Simply put, data were used to fix blame and punish rather than to find and implement effective interventions. Epidemiologists were motivated to perform actions that both pleased their supervisors and avoided the punishment of monetary loss or demotion. For example, it was common practice for epidemiologists to hide select cases of infectious disease and willfully underreport epidemic morbidity because outbreaks meant that the epidemiologists were not performing their duties of preventing and controlling infectious diseases. These disincentives to thoroughness and honesty resulted in surveillance data and reports that did not reflect true incidence and prevalence of disease, circumstances, needs, responses, or impacts [9]. Because the AIDSS was but a component of the systems designed for the entire FSU, the AIDSS did not address Armenia-specific needs. Further, because of the lack of tiered, standardized case definitions, there existed a potential for misclassification of diseases. Lack of feedback to reporting sources hampered improvements in clinical practice. Monitoring conditions for which there were no practical public health interventions, the multi-tiered and duplicative reporting processes, and the use of expensive and indirect monitoring and control measures such as excessive culturing, disinfection, and prolonged hospitalization led to waste of resources. As a result, the system did not guide control measures optimally nor use resources efficiently. As the republics of the FSU embrace various aspects of democratization, improvement of public health surveillance systems such as the IDSS should be a goal if decision makers are to use credible data for informed public health practice. Competing interests We certify that we have participated sufficiently in the conception and design of this work, as well as its execution and the analyses of the data. Further, we have collaboratively written the manuscript and take public responsibility for it. We believe the manuscript represents valid work. We have reviewed the final version of the submitted manuscript and approve it for publication. Neither this manuscript nor one with substantially similar content under our authorship has been published or is being considered for publication elsewhere. If requested, we shall produce the data upon which the manuscript is based for examination by the editors. We certify that we have no affiliations with or involvement in any organization or entity with a direct financial interest in the subject matter or materials discussed in the manuscript. Drs. Wuhib, Chorba, MacKenzie, and McNabb were employees of the U.S. federal government when this work was performed and prepared for publication; therefore, it is not protected by the Copyright Act, and there is no copyright of which the ownership can be transferred. Dr. McNabb serves as corresponding author; his address is listed. Pre-publication history The pre-publication history for this paper can be accessed here:
Background The loss of participants in longitudinal studies due to non-contact, refusal or death can introduce bias into the results of such studies. The study described here examines reasons for refusal over three waves of a survey of persons aged  70 years. Methods In a longitudinal study involving three waves, participants were compared to those who refused to participate but allowed an informant to be interviewed and to those who refused any participation. Results At Wave 1 both groups of Wave 2 non-participants had reported lower occupational status and fewer years of education, had achieved lower verbal IQ scores and cognitive performance scores and experienced some distress from the interview. Those with an informant interview only were in poorer physical health than those who participated and those who refused. Depression and anxiety symptoms were not associated with non-participation. Multivariate analyses found that verbal IQ and cognitive impairment predicted refusal. Results were very similar for refusers at both Waves 2 and 3. Conclusions Longitudinal studies of the elderly may over estimate cognitive performance because of the greater refusal rate of those with poorer performance. However, there is no evidence of bias with respect to anxiety or depression. Background Non-participation in epidemiological studies has the potential to introduce bias into the results of such studies. Lack of participation can come about through noncontact, refusal or death of the respondent. This problem becomes accentuated in longitudinal studies in which non-random attrition can be expected at each wave. In studies of elderly samples, the problem of attrition is accentuated by the increased rate of deaths. Also, such samples may have reasons for refusal which are specific to older age groups. Therefore, it is important to determine the differences between those who take part in survey research and those who do not. Often, very little is known about those who do not participate in the initial phase of any study although a number of studies have been able to determine sociodemographic differences and even health differences using hospital records. Romans-Clarkson et al.[1] have reviewed the literature on initial non-responders prior to 1988. They found that non-responders are usually older, more often male, of lower socioeconomic status and have less education. They are more likely to smoke, live in urban areas, to have a negative attitude to health surveys and have higher mortality in subsequent years. Their own study, of a random sample of community residing women, found similar results and, by examining hospital records, found no difference in occurrence of physical or psychiatric illness. Similar results for initial refusals have been found in more recent studies [2,3]. In longitudinal studies much more is known about those who do not participate in second and subsequent waves. Although non-response can be due to death of the respondent, no contact or refusal, not all studies examining non-response at follow-up have made this distinction. Lui & Anthony [4] found that non-response in an elderly sample one year after initial interview was associated with lower Mini Mental State Examination (MMSE) scores, being older and having fewer years of education. Schaie et al[5] found that non-response in a three wave study over 14 years was associated with lower scores on a number of IQ measures. In a study by Clark et al.[6] of depression in people aged 18 and above, non-response was not associated with depression after accounting for demographic variables. However, only one third of non-responders were refusals, most being no contacts. Looking specifically at refusals, Eaton et al.[7], in a follow-up of 1864 year olds from the Epidemiologic Catchment Area (ECA) Program, found that refusers were older, more likely to be married and had lower educational levels than those interviewed. Psychopathology (assessed by either diagnoses or symptoms) was not significantly associated with refusal. Bucholz et al[8] examined refusers amongst an 1849 year old community sample followed up after 11 years. Refusers were more likely to be male and to have a history of barbiturate abuse or dependence, but were no more likely to be in the problem-drinking, minimal alcoholic group than responders. Studies of elderly refusers have found that they are more likely to be cognitively impaired, to be in poor physical health and to have lower levels of education [9-14]. Of the possible reasons for non-response, refusal is of particular interest because it is amenable to change. The more we know about those who are alive and have been contacted, but refuse to take part, the more likely we are to be able to improve our approach and hopefully lower our refusal rates. The aim of this paper was to examine the relationship of non-response, particularly refusal, to the physical, psychological and cognitive state of elderly participants at subsequent waves of a longitudinal survey. Methods The sample In 199091 a sample of 945 (of 1377 selected) persons drawn from the Electoral Rolls of Canberra and the nearby town of Queanbeyan and representative of the age distribution of this community were interviewed using the Canberra Interview for the Elderly (CIE) [15]. An additional sample of 100 (of 145 selected) residents of Nursing homes and sheltered accommodation were also interviewed. Interviews were sought from both the subject and informant, usually a close relative. The structured interview was administered to the participant by a trained lay interviewer. It was undertaken in the participant's home, taking 1.5 to 2 hours to complete. All interviewing and tests were completed in one visit. Wave 2 of the study was undertaken at a mean of 3.6 (range 3.34.2) years later and Wave 3, at a mean of 4.0 (range 3.74.4) years after Wave 2. Refusers at Waves 2 and 3 were divided into two types: those who refused all participation and those who were not interviewed themselves but an interview was achieved with a relative or friend. In most cases where only an informant interview was possible, the respondent was approached, initially, to ask if they would take part. However, in some cases, a spouse or child intercepted the interviewer, decided that the respondent was unable to carry out the interview but offered to be interviewed themselves. For the purposes of this study, those subjects for whom only an informant interview was obtained at Wave 1 were excluded from the analysis of refusals at Wave 2, while Informant Only interviews at Wave 2 were excluded from analysis of refusal at Wave 3. No attempt was made at Wave 3, to interview anyone who had refused outright at Wave 2. Measures Physical health was assessed using 1) an Activities of Daily Living (ADL) scale [16] which asks the participant to rate their ability to perform a range of everyday activities, with answers ranging from "no difficulty" to "unable to do", 2) a measure of the number of chronic illnesses suffered by the respondent, and 3) self-reported measures of sight and hearing impairment. The sight impairment scale and the hearing impairment scale were each comprised of 5 questions, including a general question on how they rated their sight or hearing and 4 questions on sight or hearing problems in 4 different situations. Higher scores mean higher impairment. The Goldberg anxiety and depression scales [17] were used as indicators of mental health. This scale contains 18 questions, 9 on depression and 9 on anxiety with answer options being "yes" or "no". Cognitive impairment was assessed using the MMSE [18], a test covering a range of cognitive abilities including orientation for place and time, memory and spatial ability. A higher score for this test indicates better cognitive function. Premorbid IQ was measured by the National Adult Reading Test (NART) [19]. The NART is a measure of verbal IQ and is commonly used to estimate IQ in elderly subjects because it is highly resistant to the effects of ageing and dementia [19]. It involves the participant reading a list of words of increasing difficulty and being assessed on their pronunciation. Personality was assessed only at Wave 1 by extraversion and neuroticism scales from the short form of the Eysenck Personality Questionnaire-Revised [20], a measure consisting of 24 questions, 12 on extraversion and 12 on neuroticism. DSM-III-R dementia diagnosis was able to be determined from the interview. At the end of the Wave 1 interview only, participants were asked by the interviewer "Was the questionnaire at all distressing  did some of the questions upset you or make you feel anxious?" with options being "yes", "no" or "don't know". Five percent answered "don't know" or the question had not been asked. This 5% was treated as missing data. Sociodemographic variables included age, sex, living alone (in community sample only), years of education and previous occupation status (white collar versus blue collar / manual). Statistical analysis Univariate analyses on continuous variables were performed using one way analysis of variance followed by post hoc modified-Least Significant Difference (LSD) tests to compare the participants and refusers. Categorical variables were analysed using chi-square tests. Multivariate analyses were performed using logistic regression with simultaneous entry of predictor variables. Results The response rate at Wave 1 was 69%. The only information available on non-responders was their age and sex. Those who refused participation were not significantly different from participants in age. However, significantly more women than men refused to take part. In the community sample the response rate for males was 76% and for females, 62%. In the nursing home sample, the response rates were 75% and 65% for males and females respectively. Table 1 gives the breakdown of response status at Wave 2 and Wave 3. Of the 77% of respondents from Wave 1 who were contacted at Wave 2, 85% participated, 5% allowed an informant to be interviewed and 10% refused any participation. At Wave 3, of those who were able to be contacted, 86% participated, 5% allowed an informant interview, and 9% refused any participation. Table 1 Response rates at Wave 2 of those respondents interviewed at Wave 1 and response rates at Wave 3 of those interviewed at Wave 2. Wave 2 Wave 3 N % N % Respondent interviews at previous wave 981 100 638 100 Respondent interviews 638 65.0 379 59.6 Informant only interviews 36 3.7 22 3.6 Refusals 78 8.0 39 6.1 Died since previous wave 215 21.9 167 26.2 No contact 14 1.4 31 4.9 Those who were not contacted were people who either could not be found or had moved too far away to be followed up. This group plus those who had died have been omitted from all further analysis. Refusal at Wave 2 Table 2 compares three groups: those respondents who participated at Wave 2, those with an Informant Only interview and those who refused any participation. Significant differences were found for years of education, occupational status, NART, MMSE and ADL scores, sight impairment and distress at end of Wave 1 interview. Consideration of LSDs for continuous variables and adjusted residuals for categorical variables showed that both groups of refusers, but particularly outright refusers, were less likely to be white collar workers. However, only the outright refusers had significantly fewer years of education. Participants had significantly higher NART and MMSE scores than either those who refused or those with an Informant Only interview. The Informant Only group was significantly more physically impaired in terms of ADL, sight (but not hearing or chronic illness) and had lower MMSE scores. A dementia diagnosis at Wave 1 was significantly more likely in those who allowed an informant interview only at Wave 2, with those refusing being more likely to have had a dementia diagnosis than those who participated at Wave 2. Those who refused any participation were more likely to report distress at the end of the Wave 1 interview than those who participated at Wave 2. There was no difference between the groups on age, sex, anxiety or depression, extraversion or neuroticism. Table 2 Comparison of participants, those respondents who refused but allowed an informant interview, and those who refused all participation at Wave 2: Means (and SDs) or percentages Wave 1 variables Participants n = 560638 Refused but informant interview n = 2236 Refused all Participation N = 6078 P-value Age at Wave 1 76.5 (4.9) 77.9 (5.4) 75.9 (4.0) .123 Sex (% male) 48.58 44.44 42.31 .532 Years of education 11.45 (2.64) 10.82 (1.95) 10.56 (2.28) .009 Occupational status (% white collar) 65.78 48.57 42.86 .000 Lives alone (%) * 35.78 31.25 49.30 .066 NART 113.23 (9.05) 103.23 (11.51) 105.78 (10.89) .000 MMSE 27.69 (2.30) 23.10 (5.49) 26.14 (4.48) .000 DSM-III dementia (%) 4.7 34.6 13.5 .000 ADL 1.75 (2.51) 3.73 (4.56) 1.37 (1.56) .000 Chronic illness 2.94 (1.92) 2.94 (2.16) 2.67 (1.60) .498 Sight problems 0.88 (1.88) 2.03 (3.87) 0.92 (1.97) .005 Hearing problems 1.99 (2.45) 2.87 (3.11) 2.14 (2.66) .150 Anxiety 2.41 (2.23) 2.18 (2.58) 2.84 (2.63) .269 Depression 1.89 (1.88) 2.52 (2.38) 2.14 (2.15) .153 Extroversion 5.89 (3.23) 5.95 (3.14) 6.47 (3.11) .413 Neuroticism 3.04 (2.80) 4.05 (3.29) 3.28 (2.71) .223 Distressed by interview (%) 4.01 9.68 10.96 .016 * Living alone or with others was only examined in those living in the community. Logistic regression was used to compare 1) participants to refusers and 2) participants to the Informant Only group. Those variables that showed significant differences in Table 2, were entered simultaneously into the regression. Dementia diagnosis was not included because it was strongly associated with the MMSE score. The results are shown in Table 3. Premorbid IQ as measured by the NART was the only significant predictor of outright refusal at Wave 2, while both NART and MMSE scores were significant predictors of the Informant Only group at Wave 2. Stepwise entry of variables was also undertaken to further examine the role of predictors in the odds of refusal. The results were essentially the same as for simultaneous entry with the exception that in comparing participants with Informants Only, sight impairment was significantly worse in the latter group. Regression analyses were also undertaken using dichotomised variables: MMSE with a cut point of 23/24, sight impairment versus no impairment, ADL score indicating no need for assistance versus needing assistance. The results were substantively the same as when continuous variables were used. Table 3 Odds ratios (ORs) (and 95% confidence intervals) from logistic regression where the OR of being 1) a refuser or 2) an informant-only respondent at wave 2 is associated with an increase in the predictor. Wave 1 Variables Refusers compared to Participants n-716 Informant-only groups compared to Participants n-674 Age 0.96 (0.911.03) 1.00 (0.921.10) Sex (males = 1 females = 2) 1.25 (0.722.18) 1.21 (0.473.08) Years of education * 1.02 (0.901.16) 1.02 (0.821.27) Occupational status (white collar) 0.68 (0.381.23) 1.07 (0.422.72) NART* 0.95 (0.92 0.98) 0.94 (0.891.00) MMSE* 0.91 (0.881.02) 0.75 (0.630.89) Activities of daily living * 0.90 (0.761.07) 1.04 (0.891.21) Sight impairment scale * 1.03 (0.881.21) 1.20 (0.981.45) Distressed by interview (no = 1, yes = 2) 2.30 (0.866.15) 0.94 (0.165.37) *OR associated with an increase of one unit in the scale. Refusal at Wave 3 Refusal at Wave 3 was examined using the same analyses as for refusal at Wave 2. Significant differences between the three groups were found for age, years of education, NART, MMSE and ADL scores. Differences in years of education, NART and ADL scores were essentially the same as for the three Wave 2 groups, while MMSE scores were no different between participants and those refusing any participation but significantly lower in those for whom there was only an informant interview. This latter group was significantly older than the other two groups. Logistic regression comparing participants to outright refusers showed that the only significant predictor was years of education. No individual measure predicted refusal when participants were compared to the Informant Only group, although the block of variables did make a significant contribution. Change in physical and mental health and cognitive function between Waves 1 and 2 were examined as predictors of refusal at Wave 3. Changes in ADL showed the only significant association, with those providing an informant interview having a greater increase in ADL problems compared to respondents who were interviewed and those who refused any participation. Discussion At the third wave of this longitudinal study of an elderly sample, respondent interviews were achieved for 39% of those originally interviewed. Another 39% had died since Wave 1. At Wave 2, refusal, for both those who allowed an informant to be interviewed and those who refused any participation, was significantly associated with lower occupational status, fewer years of education, lower verbal IQ scores and poorer cognitive function than participants. These results confirm those of other studies which have examined refusal in the elderly [4,5,7,12,13]. A number of these studies also report poorer physical health in refusers. However, in this study, only those with an Informant Only interview showed significantly higher disability (ADL) than participants. Also, the Informant Only group were more likely to have had a diagnosis of dementia at the Wave 1 interview. These findings suggest that including an informant interview in the study has enabled us to gather information on those who were too physically frail or cognitively impaired to participate themselves. These are groups that are important to include in any studies of health in the elderly. Unfortunately, those who refused were also more likely to have had a dementia diagnosis. It is interesting also to consider those characteristics that were not associated with refusal. It has been suggested that psychological state may contribute to non-participation in community studies [21]. While the data presented here can say nothing about initial refusal, at subsequent waves we found no association between depression and anxiety scores at previous waves and refusal. As the measures of current anxiety and depression were taken 4 years prior to refusal at subsequent waves, it may be reasonable to expect no association. However, the trait measure of neuroticism, which is relatively stable over time and highly correlated to depression and anxiety was also found to have no association. Similarly, Baton et al[7], Norris [22] and Clark et al[6] did not find any association between non-response and depression symptoms, while Launer [11] found that non-responders had reported more psychiatric symptoms. Refusers at Wave 2 were no different in age to those who participated. Some other studies [11] have found similar results, while refusers in the study by von Strauss et al.[13] were significantly older. This latter study, however, was interviewing people in their mid eighties. At Wave 3 of our study, when the average age of the sample was 84 years, those for whom only an informant interview was achieved were significantly older than those who participated. This present study also found no difference in the proportion of males and females refusing at Waves 2 and 3, although more women refused at the initial approach. Other studies examining refusal at initial approach have found that males are more likely to refuse[1,8], however these tend to be samples covering the adult age range. Studies of older samples [10,11] have found little difference in the gender of initial refusers. It is difficult to know why more women than men refused at the initial approach in this study. Reports of follow-up of older samples [4,9,13,22] have found no differences in gender, as was found here. Multivariate analysis comparing those who refused any participation to those who participated showed that the verbal IQ score was the only significant predictor of refusal. Comparison of those who either refused or had an Informant Only interview found that both IQ and cognitive impairment contributed. However, the ADL score did not contribute significantly in this analysis. This was somewhat surprising as the ADL score, for those with an informant only interview, was more than double that of those who participated. Of the 44 people who said they felt some distress during the Wave 1 interview, 34 gave reasons; 17 (50%) gave the cognitive section as the reason for their distress, while a further five felt distressed by their inability to understand or answer the questions. A possible limitation of this measure is that some respondents may have been reluctant to admit to any distress to the interviewer and so the results may not be an adequate indicator of distress or discontent about the interview. Nevertheless, people who refused any participation at Wave 2 were more likely to have experienced distress. Von Strauss et al.[13], in a study of attitudes to a longitudinal study, found that those with impaired cognitive functioning and lower education showed the least positive attitude and that the first contact and the cognitive testing were the most stressful part of the study. Similarly, Levin et al. [14] examined refusal on follow-up in a sample of Parkinson's Disease patients and found that the only significant predictor was cognitive impairment. These authors commented that participants and their spouses were concerned about the development of dementia when contacted at follow-up. As with other studies, this one also found that those who refused had had lower MMSE scores and fewer years of education. One plausible interpretation is that those people who perform poorly on the cognitive tests are aware of this and become distressed by it. Non-response is a serious issue for survey research, in particular, longitudinal studies in which each wave results in further loss and the possibility of sample bias. In studies of the elderly, loss through death of respondents is inevitable. Kessler et al[23] and, more recently, Dunn [24], have examined statistical methods to adjust for non-response. These methods depend on the predictors of missingness and the purpose here has been to identify these predictors. These data can then be used for such methods as multiple imputation or Full Information Maximum Likelihood (FIML) estimation. An examination of the predictors of mortality in this sample [16] found physical ill health, cognitive functioning and being male to be significant predictors. This report has concentrated on those who are alive, have been contacted but refuse to participate. Conclusions In conclusion, refusal was predicted by low scores on a verbal IQ test and cognitive impairment, possibly because of distress caused by cognitive testing. It is important that cognitive testing be presented in a non-threatening manner and that participants in the initial wave of such studies be reassured about their performance. However, bias may be reduced by including an informant interview in the study. Competing interests None declared Pre-publication history The pre-publication history for this paper can be accessed here:
Background An earthquake in the coffee growing region of Colombia on January 25, 1999 destroyed 70% of the houses in Armenia city. Transitory housing camps still remained until two years after the disaster. Parasitological studies found that, in this population, giardiasis was the most frequent parasitic infection. This study was carried out in order to determine the epidemiological risk factors associated with this high prevalence. Methods Fecal samples were obtained from 217 children aged between 3 and 13 years. Stool samples were studied by direct wet examination and stained with ferric hematoxilin for microscopical examination. Epidemiological data were collected by questionnaire and analyzed by using the Epi-info software (CDC, Atlanta 2001). Results Giardia cysts were observed in 60.4% of the samples presented and trophozoites in 4.6%. The following epidemiological and laboratory factors were significantly associated with Giardia infection: 1. Use of communal toilet (vs. individual toilet) OR: 3.9, CI95%: 1.216; 2. water provision by municipal ducts (vs. water provision by individual tanks) OR: 3.5, CI95% 1.114, and 3. presence of mucus in stool OR: 2.3, IC95%: 0.96.7. Conclusions A high prevalence of giardiasis was found in children living in temporary houses after the 1999 earthquake in Armenia (Colombia). Giardiasis is an emerging disease in post-disaster situations and adequate prevention measures should be implemented during these circumstances. Background On the 25 January, 1999 an earthquake with an intensity of 6.2 points on the Richter's scale occurred in a focus located only 20 kilometers away from Armenia, Colombia. Although 28 towns or municipalities, including Armenia, were severely damaged by the earthquake, the worst effects occurred in Armenia itself and in its surrounding towns. The consequences of the catastrophe were devastating, mainly because of the loss of people's property and belongings: around 70% of Armenian houses were destroyed and in some neighboring towns the percentage was greater than 80%. In the rural area, the proportion of lost houses was also 80%. The death toll was estimated at 1184, and the number of people injured was more than 5,000 [1]. The region known as "Departmento del Quindo" includes, according to the political and administrative division of the Colombian territory, several small towns and its capital city, Armenia. The whole area of influence of the telluric movement is known as "Eje Cafetero" (Coffee Axis), because it is here that most of the world famous Colombian coffee is produced. This area is located within the country's central mountain range. Before the tragedy, Armenia had 296,330 inhabitants, with the whole Department of Quindo having approximately 500,000. The devastating effects on the houses of Armenia induced the organization of transitory housing facilities in tents and "alojamientos" (multi-family spaces), which caused difficulties in the provision of basic sanitary services, water supplies and food. Fears about epidemics were high, but no outbreaks of infectious diseases, including diarrhea, dengue or malaria occurred [2]. Temporary housing facilities remained until two years after the disaster, which raised concerns about public health conditions in this people. The University of Quindo undertook epidemiological studies in order to determine which health disorders were occurring. This included the effect on mental and physical health morbidity [2,3]. One of the pathological aspects that was studied was the prevalence of intestinal parasites in this people. During this survey it was found that giardiasis was the most prevalent intestinal parasitic infection. Therefore, a study was designed in order to ascertain the epidemiological factors related to this high prevalence. In this paper we report the findings of the laboratory examination of stools and their association with data collected by questionnaire. Methods Study design, characteristics of the camps and questionnaire A prospective study of the prevalence of Giardia in stool specimens of children living in temporary houses in Armenia was carried out. A total of 26,780 people lived in temporary housing camps built under the supervision of non-governmental organizations. This study was conducted in 18 camps ("asentamientos"), multi-family spaces which grouped temporary houses or tents, and were representative of a total of 86 camps which were established in Armenia after the January 25, 1999 earthquake. A census population of the families living in the 18 camps found 816 children aged between 3 and 13 years. A random sample of 193 targeted households was selected for a systematic household survey conducted between January 2000 and July 2001. Thus, 217 children aged between 3 and 13 years were sampled. The children were included only if their father or other adult with legal custody of the children agreed to give written informed consent. A household was defined as a unit of persons residing in one tent or temporary house. The median household size was made up of 5 persons. All households had municipal garbage disposal services. Electricity was available for all households. From each selected household one adult was interviewed using a standardized questionnaire which focused on both the demographic characteristics of children such as age and school years, as well as on the presence of clinical symptoms such as diarrhea (passing more than two liquid stools daily) or flu symptoms (rhinorrea without pharyngitis and with only minor or no fever at all) with a duration of less than eight days at the moment of interview. The weight of each child was recorded. Interviewers were selected from university students of biology who had previous experience in community work, were from the same city and knew the terms used by people to describe their typical symptoms and living conditions; they were previously trained by an epidemiologist. Stool samples of children were collected the day after the questionnaire was filled. The source of drinkable water and the type of toilet used were recorded for each child. Water supply varied between camps: in some camps water was provided by the municipal water ducts, in others there were individual tanks for each house. Municipal water (piped water) was the major source of drinking water and was used in 15 camps. The source of municipal water was surface water ("Ro Quindio") provided through the aqueduct system which existed before the earthquake. Water chlorination concentrations were between 2,5 and 3 mg/liter. This is a closed water delivery system; however after the earthquake some of the ducts collapsed and leakages occurred. For this reason, in 3 camps individual tanks were used as a source of drinkable water. The tanks were made of plastic material and had a 2,000 liter storage capacity. Individual tanks were filled by a tank car. In addition, municipal health services distributed 1,217 kg of sodium isocianyde dichloride in all camps to be used in individual tanks. There were two types of hygienic room services. One was the communal toilet service that consisted of one latrine for each 910 families placed in the center of camp; other type of toilet was the individual sanitary service located in each temporary house and used only by the members of that household. The researcher also registered the "clean status" of the temporary house; a house was considered "clean" if no particular odors were detected, the food was stored in proper recipients, and food waste was disposed of in garbage cans. The place for food storage was also recorded. No questions about animal contact were done. Specimen collection and stool examination Stool samples were collected early in the morning in sterile plastic containers supplied by the researchers and conserved on ice until laboratory examination. Samples with water or urine contamination were discarded. A second sample was requested from all children at day 5 after the first sample and a third sample one month later. Stool samples were stained with ferric hematoxilin and examined after centrifugation; this test has a sensitivity of 61% on the first sample and a cumulated sensitivity of 83% when three different serial samples are examined [4]. Statistical Analysis Statistical significance tests for comparison of rates were performed when sample sizes were insufficient and asymptotic tests when sizes were sufficient. All tests were two-sided, and a p value of <0.05 was considered as significant. Exact 95% confidence intervals (Cl) were calculated [5]. Results In total, stools from 217 children were examined, 97 were girls (44.7%) and 120 boys (55.3%). In the first sample, prevalence of Giardia cysts was 60.4% (131 children) and Giardia trophozoite forms 4.6%. A second sample was obtained in only 53 of 217 children. Families were not informed about the result of the first sample before collecting the second one. Collection was requested on a voluntary basis. From these 53 children with a second sample, 40 were positive and 13 negative on the first sample. Thirty children were positive for Giardia cysts on the second sample (56%), only three of them being negative at the first sample. Only six children provided a third sample; three of them, who were all previously positive for Giardia cysts, were positive. Analysis of infections by other parasites was also performed (Table 1). There were no statistically significant differences in the prevalence of giardiasis in children infected with other parasites. No cultures for bacteria were performed during this study. Table 1 Parasitic infections in stool samples of children (n= 217) living in temporary housing after the 1999 earthquake disaster in Armenia. Organism Number of infected children Percent of sampled children Number of infected children also infected with giardiasis Ascaris lumbricoides 36 16.6% 19 Balantidium coli 4 1.8% 3 Endamoeba coli 71 32.7% 40 Endolimax nana 125 57.6% 56 lodamoeba buschlii 31 14.3% 19 Entamoeba histolytica/dispar (determined by micrometry) 116 53.4% 75 The analysis of epidemiological characteristics (Table 2) showed that the presence of Giardia cysts was associated with the use of communal toilet versus use of individual toilet (OR 3.9). The water provision by municipal water services was associated with a higher risk for giardiasis, in contrast with people that used individual water tanks (OR 3.5). The finding of mucus in stool was higher in children with Giardia cysts (OR 2.3). A "clean house" was a protective factor for the presence of Giardia cysts (OR 0.4). Table 2 Association between epidemiological and laboratory findings with the presence Giardia cysts in stool samples from children living in temporary housing after the 1999 earthquake disaster in Armenia. Risk factor With factor Without factor OR 95% Confidence Intervals P Communal toilet (versus individual toilet) 21/25 110/192 3.9 1.216 0.01 Municipal water service versus individual water tank 27/58 104/159 3.5 1.114 0.02 Mucus in fecal sample 20/26 111/191 2.39 0.96.7 0.049 Leukocytes in stool 16/22 115/195 1.85 0.705.35 0.15 Plastic stock of food versus carton stock of food 49/93 82/124 1.7 0.93.1 0.06 Yeast in stool 57/93 74/124 1.06 0.611.82 0.46 Diarrhea 23/42 108/175 0.75 0.41.4 0.84 Flu symptoms 79/136 52/81 0.7 0.41.4 0.45 Lipids in stool 120/201 11/16 0.67 0.182.2 0.65 Clean lodgment 62/118 69/99 0.48 0.270.84 0.014 In total 199 of the 217 children studied attended school. The source of water at school was municipal water and communal toilet was the type of sanitary service there. As data were collected concerning the putative factor (water source) and/or disease (Giardia), data were also analyzed on the presence or absence of the confounding variable (attendance at school). The data were then divided into strata for confounding variable (attendance at school) in a series of 2  2 tables, one for each level of the confounding variable (Figure 1). Using Mantel-Haenszel odds ratio method the OR was 15.1 (CI95% 836) with a p value of 0.00000000. This analysis indicate that drinking municipal water at camps was strongly associated with giardiasis even with the confounding variable "school" taken into account. Figure 1 Stratified analysis of children who did and did not attend school and presence of Giardia cysts in stool samples from children living in temporary housing after the 1999 earthquake disaters in Armenia. Presence of Giardia trophozoites was higher in patients with municipal water service and with lipids in stools (Table 3); however due to the small number of children with trophozoites, no difference attained statistical significance. Other demographic factors including weight, age and the number of years at school were not associated with giardiasis (Table 4). Table 3 Association between epidemiological factors, laboratory findings and the presence of Giardia trophozoites in stool samples from children living in temporary housing after the 1999 earthquake disaster in Armenia. Risk factor With factor Without factor OR 95% Confidence Intervals P Municipal water service versus individual water tank 10/159 0/58 ? ?-1.1 0.06 Lipid drops in stools 2/16 8/201 3.41 0.4516.5 0.16 Flu symptoms 6/81 4/136 2.62 0.6910.8 0.11 Plastic stock of food versus carton stock of food 1/19 9/198 1.16 0.0507.71 0.97 Leukocytes 1/22 9/195 0.98 0.046.44 0.66 Yeast in stools 4/93 6/124 0.88 0.213.0 0.69 Communal toilet (versus individual toilet) 1/25 9/192 0.84 0.035.50 0.71 Mucus in fecal sample 1/26 9/191 0.80 0.035.24 0.72 Diarrhea 1/42 9/175 0.45 0.012.86 0.88 Clean lodgment 3/118 7/99 0.34 0.071.34 0.97 Table 4 Mean age, school years and weight and presence of cysts of Giardia in stool samples from children living in temporary housing after the 1999 earthquake disaster in Armenia (Colombia). Giardia Cysts Mean  SD P Mean age No 7.3  2.9 0.79 Yes 7.4  3.1 Weight (Kg) No 23.1  9.1 0.89 Yes 23.3  9.9 School years No 1.6  1.9 0.87 Yes 1.7  2.2 Discussion Two previous studies carried out by the Colombian National Institute of Health found that Giardia was the most prevalent intestinal parasite. In 1965 a prevalence of 9.4% was found and in the 1980 survey the rate was 21.4%. These rates were the reported for children in the 5 to 14 year-age group living in the central region of Colombia, where the department of Quindo is located. In both studies (in 1965 and in 1980) diagnostic methods were the same as those used in the present study, microscopic examination and the rates were estimated from the examination of only one sample [6]. We found a higher prevalence. Our findings could indicate that giardiasis emerges during events which alter the existing sanitary conditions. We found that a higher prevalence was associated mainly with communal toilets and municipal duct provision at camp, which indicates that both factors favored dissemination of giardiasis in temporary camps after the disaster. As demonstrated by stratified analysis, drinking municipal water at camps was strongly associated with giardiasis even when attendance to school was taken in account as confounding variable. In future in similar conditions it will be important to make efforts addressed to guarantee individual toilet for each household and to control the water source by checking the presence of Giardia in water at temporary camps. In the present study, clinical symptoms were not associated with the presence of Giardia cysts or trophozoites in stools, but presence of cysts were significantly associated with mucus in stool. Mucus in stool can indicate inflammatory responses and our finding could indicate that chronic infection may induce inflammatory changes without major clinical symptoms. However, a longitudinal prospective study comparing infected and non-infected children with Giardia would be necessary in order to establish if chronic Giardia infection has an impact on growth or nutritional status in this particular population. In humans, the clinical effects of Giardia infection range from the asymptomatic carrier state to a severe malabsorption syndrome [7]. Factors possibly contributing to the variation in clinical manifestations include the virulence of the Giardia strain, the number of cysts ingested, the age of the host, and the state of the host immune system at the time of infection. Giardiasis was most prevalent in children who drank municipal water. Giardia survives in chlorinated water [8]. To protect against transmission all drinking water should receive chemical pretreatment, preferably with sedimentation, and filtration in addition to disinfection [9]. Municipal water should supply water with a concentration of less than 0.770 cysts per 100 liters [10]. There are two previous reports of outbreaks of giardiasis after natural disasters, both linked to flooding after heavy water run-off. One occurred in Montana (United States) where an outbreak of gastrointestinal illness affected 780 persons [11]. In Utah (United States) 1,230 people were affected with diarrhea [12]. Water systems providing unfiltered surface water and contaminated with flooding were identified as the origin of both epidemics. In Armenia the earthquake damaged the municipal system of water ducts [2]; this probably induced their contamination, explaining the high rate of giardiasis that we found in children. It is interesting to note that individual tanks were filled with municipal water. One explanation for the lower prevalence of giardiasis in children that used individual tanks is that in tanks there is additional sedimentation that can reduce the contamination of water with Giardia[10]. Health municipal services also distributed chlorine to be used in individual tanks. Although we could not establish the extent to which this chlorine was used, it is possible that additional chlorination contributed to reduce the probability of Giardia infection. There are no data available on the prevalence of giardiasis in general population in Armenia. It will be necessary to begin studies in order to establish the public health impact of this parasite infection in the city. Conclusions Giardiasis was the most prevalent intestinal parasite found in children living in post-earthquake camps in Armenia, Colombia. Our results suggest that this is an emerging disease in post-disaster situations. However, data on prevalence of giardiasis before the earthquake and in population living away from camps is lacking. The use of communal toilets instead of individual household toilets was found to be an epidemiological risk factor associated to giardiasis. Adequate prevention measures which include efforts to make individual toilets available are also important to prevent dissemination of this parasite. The present study also found a relationship with the water provided by municipal closed ducts that were affected by the earthquake. This highlights the need for implementing adequate surveillance systems for the presence of Giardia in water sources in similar circumstances. Competing interests None declared Pre-publication history The pre-publication history for this paper can be accessed here:
Background Mefloquine is a clinically important antimalaria drug, which is often not well tolerated. We critically reviewed 516 published case reports of mefloquine adverse effects, to clarify the phenomenology of the harms associated with mefloquine, and to make recommendations for safer prescribing. Presentation We postulate that many of the adverse effects of mefloquine are a post-hepatic syndrome caused by primary liver damage. In some users we believe that symptomatic thyroid disturbance occurs, either independently or as a secondary consequence of the hepatocellular injury. The mefloquine syndrome presents in a variety of ways including headache, gastrointestinal disturbances, nervousness, fatigue, disorders of sleep, mood, memory and concentration, and occasionally frank psychosis. Previous liver or thyroid disease, and concurrent insults to the liver (such as from alcohol, dehydration, an oral contraceptive pill, recreational drugs, and other liver-damaging drugs) may be related to the development of severe or prolonged adverse reactions to mefloquine. Implications We believe that people with active liver or thyroid disease should not take mefloquine, whereas those with fully resolved neuropsychiatric illness may do so safely. Mefloquine users should avoid alcohol, recreational drugs, hormonal contraception and co-medications known to cause liver damage or thyroid damage. With these caveats, we believe that mefloquine may be safely prescribed in pregnancy, and also to occupational groups who carry out safety-critical tasks. Testing Mefloquine's adverse effects need to be investigated through a multicentre cohort study, with small controlled studies testing specific elements of the hypothesis. Background Mefloquine was developed by the US Army and introduced for the treatment of malaria in the late 1970s. [1] Mefloquine was first used for prophylaxis in 1985, and since then approximately 14.5 million people have been prescribed the drug for malaria prevention, versus 1.6 million for treatment. [2] In the first decade of mefloquine's use, the reported adverse effects were mainly gastrointestinal.[3] In the late 1980s it became clear that mefloquine could cause neuropsychiatric adverse effects.[4] The first randomised controlled trial of mefloquine prophylaxis in heterogeneous, non-immune Western travellers was published in 2001 and found that one-third of all mefloquine users reported neuropsychiatric adverse effects and 6% of all users reported at least one severe adverse event (defined as requiring medical advice).[5] On the evidence from a case series published in 1992 by the manufacturer, Hoffmann-La Roche, the World Health Organisation (WHO) recommends that travellers with a personal or family history of seizures or manic-depressive illness should not take mefloquine prophylaxis.[6,7] However the Centers for Disease Control and their Canadian equivalent, CATMAT, do not recognise this as a valid contraindication to taking mefloquine.[8] A recent analysis of spontaneous reports held on the Dutch national pharmacovigilance database suggested that there is a mefloquine syndrome consisting of excessive sweating accompanied by malaise, nausea, diarrhoea, agitation, concentration problems and nightmares.[9] The aetiology of the adverse effects associated with mefloquine use remains obscure. Ten cohort studies in tourists found that women generally experienced worse side effects from mefloquine than men; [6,10-18] an eleventh did not find this effect.[19] In randomised controlled trials, children have tolerated mefloquine well. [20,21] Surprisingly, one cohort study found that some older adult travellers tolerate mefloquine better than younger adults. [22] It has also been reported that Asian patients tolerate mefloquine better than Caucasians and Africans. [23,24] Despite early concerns, mefloquine appears to be safe in pregnancy.[25] Although the adverse effects of mefloquine are common, and often serious and long lasting, and the drug has been widely used for over 20 years, no real attempts have so far been made to investigate and explain these effects. A systematic review of mefloquine prophylaxis, performed within the Cochrane Collaboration, now tabulates 516 case reports published in 136 papers between 19762000, describing adverse effects from mefloquine at prophylactic and therapeutic dosages.[26] We retrieved and critically reviewed all the original papers listed in the Cochrane review to clarify the phenomenology of the adverse effects associated with mefloquine, and to look for clues to possibly safer use of the drug. Analysis of case reports Common features of the case reports Of the 516 published case reports of mefloquine adverse effects recorded in the mefloquine systematic review, 328 of the reports related to prophylactic mefloquine use, and 188 to treatment use. 324 of the 516 cases (63%) related to tourists or business travellers.[26] One-third of the reports were in languages other than English  mainly French, German and Danish. The search strategy for finding the case reports is described in the review.[26] An annotated bibliography of the 516 published reports can be found at . 26% of the case reports recorded three or fewer individual patient parameters (for example, the patient's age, sex and mefloquine dose taken), and many of these reports were little more than a short description of an unexpected adverse event, set in the context of a larger study. 52% of the reports however contained some discussion of the causality of the symptoms attributed to the drug, and 11% proposed a mechanism by which these symptoms might be occurring. 56% of the 516 case reports we reviewed described one or more symptoms consistent with a transient, anicteric chemical hepatitis (eg, malaise, fever, anorexia, headache, abdominal pain, nausea, diarrhoea, concentration difficulties). Of the remaining reports, many were consistent with a post-hepatic syndrome, although in some cases the disorders described could have been due to conditions such as anxiety, depression, chronic fatigue or jet lag. 15% of all the case reports described symptomatology suggesting acutely disturbed thyroid function (eg, anorexia, fatigue, tremor, palpitations, nervousness, increased sweating, mood and/or sleep disturbance, memory and concentration disorders, emotional lability, altered bowel habit, depression). Table 1 categorises the 516 published case reports according to whether the clinical features were 'very likely', 'plausibly' or probably 'unrelated' to liver or thyroid pathology. We have based this classification on standard lists of the common symptoms and signs of liver and thyroid disease. [27,28] The direction of thyroid symptoms was mainly towards hyperthyroidism, though some patients exhibited signs of both an over- and an under-active thyroid (for example, tachycardia alternating with bradycardia). Table 1 Clinical features of published case reports of adverse effects from mefloquine, and their possible relation to liver and thyroid pathology Prophylaxis case reports (N = 328) Treatment case reports (N = 188) Number (%) of reports very likely liver related 75/328 (23) 85/188 (45) Number (%) of reports plausibly but debatably liver related 102/328 (31) 29/188 (15) Number (%) of reports plausibly thyroid related 51/328 (16) 26/188 (14) Probably unrelated to liver or thyroid 100/328 (30) 48/188 (26) Median dose (range) of mefloquine taken (mg) 750 (2508750) 1250 (1505250) Median duration (range) of adverse effects (days) 16 (1550) 4 (1300) The table also shows the median duration of symptoms reported by 'prophylactic' and 'treatment' mefloquine users, and the median dose taken within each category. The median duration of adverse effects of patients who took mefloquine as treatment appears to have been shorter than that of those who took the drug as prophylaxis (4 days versus 16 days), even though the median dose of mefloquine taken was higher in the treatment group. We believe that one explanation for this unexpected finding may lie in the fact that the treatment users of mefloquine were more likely to have taken the drug as monotherapy, whereas the prophylaxis users more commonly took one or more co-medications, as well as alcohol. We discuss the possible significance of this later in this paper. Mefloquine and the liver Mefloquine is an aryl amino alcohol which accumulates in both the liver and the lungs, and is subject to enterohepatic circulation. [29] It has recently been found to cause acute hepatitis.[30] Mefloquine does not appear to cause florid signs of liver disease. However, transient subclinical disturbances of liver function are a common feature of many drugs metabolised in the liver, and this may explain the frequent finding of transaminase changes in safety studies of new drugs; these biochemical findings are usually dismissed as meaningless noise, but they may in fact be sensitive or oversensitive markers of vulnerability, of low specificity. That mefloquine induces liver enzymes is well documented. Jaspers et al reported significantly raised transaminases in Dutch marines who took mefloquine during 3 months in Cambodia, and who were not drinking alcohol at the time. [31] Takeshima found that of a cohort of healthy Japanese soldiers who took prophylactic mefloquine for 36 weeks without drinking alcohol, one-quarter developed symptoms compatible with liver pathology and four showed disturbed liver function. [32] Reisinger et al observed the same phenomenon in a cohort of short-stay European travellers to Africa, but it is not clear whether alcohol could have contributed to this effect.[33] One of the travellers, who was concurrently taking a liver-damaging agent, sulfadoxine,[34] showed gross morphological changes in his liver which were attributed to his use of prophylactic mefloquine. Liver biopsy showed intralobular cellular infiltrates consisting of macrophages and eosinophils as well as sporadic eosinophilic cell necroses; virology was negative. [33] Grieco et al described a 46-year-old woman who drank wine daily while taking mefloquine, and who became nervous and depressed, with nausea, vomiting and diarrhoea. She was dehydrated and in severe liver failure, with negative virology. Liver biopsy showed diffuse macrovesical hepatic steatosis. [35] 'Heavy sun exposure' is noted in a case report of a 60-year-old Frenchman who reacted acutely to his second mefloquine tablet; it is likely that this sun exposure would have caused dehydration.[5] A 20-year old French traveller, concurrently taking an oral contraceptive, had epileptic seizures in her sixth week of mefloquine prophylaxis, directly after 'severe exertion'.[5] It seems likely that in some mefloquine users dehydration will impose an added burden on the liver, and that this could contribute to a severe reaction to the drug. Many long-haul travellers using mefloquine are mildly dehydrated from in-flight alcohol and air conditioning, followed by hot and dry conditions, and more alcohol consumption, at their holiday or business destination. Of the 516 case reports we reviewed, eleven cited alcohol as possibly contributing to the adverse drug effects described. Wittes et al reported a remarkable challenge-rechallenge experiment where a healthy male geologist took both his third and his fourth weekly mefloquine tablet together with half a bottle of whisky, and on both occasions experienced acute paranoid delusions, depression and suicidal ideation; a fellow geologist who shared the same whisky bottle (and who was taking no antimalaria medication) experienced no such effects.[36] Vuurman et al, sponsored by Hoffmann-La Roche, tested in healthy volunteers whether or not alcohol might interact adversely with mefloquine.[37] They found psychomotor performance unimpaired, but their study design had important limitations. Only 20 participants took mefloquine and of these, two women dropped out due to adverse events (one with nausea, vomiting and dizziness, the other with malaise, fever and headache). The study protocol forbade 'strenuous physical activity' and any prescribed medications. Alcohol was given under strict laboratory conditions 24 h after mefloquine ingestion, and then in small and interrupted doses, such that the blood alcohol concentration in any participant never exceeded 0.50 mg ml-1. The authors admit that their study did not address 'the question of what might happen should (mefloquine users) consume intoxicating amounts of alcohol. [37] Their findings can thus not be generalised to the broad population of tourists and business travellers. Approximately half of the case reports listed in the Cochrane review note some co-medication taken along with mefloquine.[26] Other quinoline derivatives (chiefly chloroquine and quinine) are the commonest co-medications mentioned in the case reports. After antimalaria drugs, an oral contraceptive (noted in 8 reports) is the next most commonly reported co-medication, followed by sodium valproate (7) and diazepam (4). All these drugs can cause liver damage.[34] Diazepam is also a thyroid hormone antagonist, and we discuss below the possible significance of this. Meszaros et al reported a male traveller who in addition to mefloquine took thioridazine, amitriptyline and fluphenazine (all capable of damaging the liver), and whose mefloquine-associated neuropsychiatric symptoms persisted for over a year. [38] Gullahorn et al reported a series of patients who experienced delirium on emerging from anaesthesia, possibly because in addition to mefloquine they had received isoflurane, an anaesthetic known to cause hepatocellular necrosis. [34,39] One report describes an acute reaction in a man who took one mefloquine tablet each week together with two aspirin tablets. One hour after taking his fifth mefloquine tablet he experienced acute amnesia lasting approximately one hour.[40] Aspirin can cause hepatocellular necrosis,[34] and in addition can aggravate acute thyroid disturbance (discussed below) by competing with thyroid hormones for sites on binding proteins.[53] Mefloquine and the thyroid The preclinical studies of mefloquine by the US Army involved close monitoring in animal models and human volunteers of several organ systems, but not the thyroid. [1] The effect of mefloquine on thyroid function appears not to have been investigated in any phase III or phase IV study. Thyroid function has not been tested routinely in the diagnosis or management of patients suffering from mefloquine-related adverse effects. Thyroid disease, or some possible interference with thyroid activity, is reported in only three of the 516 case reports in the Cochrane review.[26] Bem et al described a 31-year old German woman with a 'thyroid condition', who was also taking 'tranquillisers' (unspecified) and alcohol, and who had an acute exacerbation of her schizophrenia after a single tablet of mefloquine; her symptoms persisted for 4 weeks.[5]Conget et al reported a 30-year old previously healthy woman who experienced abdominal pain, palpitations, instability, insomnia and a fine distal tremor in her second week of mefloquine prophylaxis. A thyroid function screen showed a raised serum thyroglobulin level (54 g/ml, normal range 18.7 to 27.1); this returned to normal within a month of her stopping mefloquine.[41] Bauer et al reported acute psychotic reactions in a healthy US Peace Corps worker who took prophylactic mefloquine concurrently with diiodohydroxyquinoline for a presumed parasitic infection.[42] Presentation of the hypothesis The mefloquine syndrome The published literature describes a mefloquine syndrome that presents in a variety of ways including headache, gastrointestinal disturbances, nervousness, fatigue, disorders of sleep, mood, memory and concentration, and occasionally frank psychosis. Young western women are particularly vulnerable to mefloquine's adverse effects. Certain groups however (children, older adult travellers and Asian patients) tolerate mefloquine well. Hypothesis The phenomenology of mefloquine's adverse effect profile, together with incidental details in some of the published case reports (references to alcohol, and to known hepatotoxic co-medications such as the oral contraceptive pill) suggest that for many mefloquine users adverse drug effects may be the result of primary hepatocellular injury, caused by the drug in association with one or more concurrent liver insults. Further, it seems that in some of these symptomatic users of mefloquine a transient thyroid disturbance may appear as well, either as an endocrine disorder secondary to the primary liver damage, or as an independent pathological process. We therefore postulate that many of the adverse effects of mefloquine are a post-hepatic syndrome caused by primary liver damage. In some individuals we believe that symptomatic thyroid disturbance occurs, either as an independent process, or as a secondary consequence of the initial hepatocellular injury. Previous liver or thyroid disease, and concurrent insults to the liver (such as from alcohol, dehydration, an oral contraceptive pill, recreational drugs, and other drugs that can damage the liver) may be related to the development of many severe or prolonged adverse reactions to mefloquine. Co-medications that are thyroid hormone antagonists may also be risk factors. Relevant reports with other quinoline derivatives Mefloquine is a synthetic quinoline; other quinolines include primaquine, amodiaquine and chloroquine.[43] High doses of primaquine in rhesus monkeys have caused acute fatal liver damage.[44] Amodiaquine is still used to treat malaria, but was withdrawn from general use for malaria prophylaxis in 1986 after it was found to cause liver damage and hepatitis, mostly anicteric. [45,50] Some of these reports mention co-medications known to damage liver cells (phenylbutazone, oral contraceptive, alcohol). [45,46] Amodiaquine has not been linked to disturbed thyroid function. The possibility of a three-way interaction has already been suggested between a quinoline derivative (chloroquine) and the liver and the thyroid. Munera et al described a woman with hypothyroidism, well stabilised on thyroxine sodium 125 g daily, who took prophylactic chloroquine and proguanil daily for 2 months for a vacation in Africa.[51] At four weeks her thyroid stimulating hormone (TSH) concentration was found to be very high (44.8 mU/1, normal range 0.356.0), but it returned to normal within a week of her stopping the drugs. Re-challenge with chloroquine and proguanil a year later again resulted in raised levels of TSH, a lowered concentration of free triiodothyronine (T3), and normal free thyroxine (T4) concentration. Liver function was not tested, but the authors speculated that 'Chloroquine... seems to have enhanced the induction of liver enzymes. [It] probably increased the catabolism of thyroid hormones by enzymatic induction.[51] They also suggested that chloroquine might act centrally on the hypothalamus, through disruption of the feedback system by which thyrotropin releasing hormone stimulates the pituitary to release and later synthesise TSH.[52] A third mechanism by which chloroquine and chemically related drugs such as mefloquine might interfere with thyroid function is through structural homology to T3, resulting in thyroid hormone antagonism.[53] In the rat, chloroquine injections more than halve the T3 concentration, without changing the level of free T4.[54] Chloroquine has been reported to inhibit T3 uptake in mammalian cells by inhibiting receptor-mediated endocytosis. [53] Testing the hypothesis Our hypothesis needs to be tested through a large multicentre cohort study of mefloquine prophylaxis in tourists and business travellers, perhaps recruited in collaboration with one or more airlines. Small randomised controlled trials should test specific elements of the hypothesis, such as the postulated link between mefloquine and thyroid disturbance, and the presumed interaction between mefloquine and oral contraception. National pharmacovigilance databases should also be analysed systematically to see if the spontaneous reports of mefloquine's adverse effects tend to support our hypothesis or not. The multicentre cohort study of prophylactic mefloquine use should be questionnaire-based, and should enquire specifically into the major risk factors (alcohol intake during travel, hydration status, use of hormonal contraception and recreational drugs, other potentially hepatotoxic or thyrotoxic drugs, previous history of proven or suspected liver and/or thyroid abnormality) that we have proposed. The study design should include pre- and post-exposure testing of liver and thyroid function in at least a sample of the cohort. One or more case-control studies should be nested within the cohort study. [55,56] These nested studies would allow for rigorous testing of the aetiological mechanisms which we have proposed for mefloquine's adverse effects. The studies should also resolve those prescribing issues on which experts' opinions differ (eg, Is mefloquine safe in pregnancy? Is it safe for long-term prophylaxis? Should airline pilots be prescribed mefloquine? Should mefloquine be prescribed to people with a personal or family history of neuropsychiatric illness? Can mefloquine be given safely as a pre-travel loading dose, for rapid induction of chemoprophylaxis? [17,57]). Mefloquine is a clinically important drug, commonly used by healthy people. A much higher standard of safety is therefore required for mefloquine prophylaxis than for drugs given to treat serious diseases.[58,59] The study we propose is urgently needed. Implications of the hypothesis What our hypothesis explains The hypothesis explains much of the complexity of the pattern of adverse effects of mefloquine, and also the fact that many healthy users of the drug suffer no adverse effects at all. It also explains some aspects of mefloquine's tolerability profile in travellers which until now have not been understood, notably that young women experience more adverse effects from mefloquine than men, and that children and older adult travellers (who do not use oral contraception, and who rarely misuse alcohol) seem to tolerate the drug well. In addition, our hypothesis may explain the ethnic and inter individual variations in the pharmacokinetics of this agent, which until now have not been understood. [23,24] It has been known for some years that mefloquine users who take co-medications are about 1.5 times more likely to experience an adverse drug event than those users who take no co-medications, and twice as likely to experience severe adverse drug events.[14] The frequency of reported adverse drug events also increases when multiple co-medications are taken.[14] Our hypothesis plausibly explains these earlier findings. The use of marijuana and other psychoactive agents by chloroquine users has been associated with acute psychotic reactions, and it has been suggested that mefloquine users who take recreational drugs may likewise be predisposed to neuropsychiatric problems.[60,61] We believe that this association is consistent with our hypothesis, since recreational drugs can cause hepatocellular damage and liver failure.[34] A puzzling observation is that although the adverse effects of mefloquine are usually reversible, in some patients these effects can persist for months or even years after the drug has been stopped.[61] We believe that the occasionally protracted time course of the adverse effects can be plausibly explained by supposing that in these patients mefloquine is just one of several concurrent insults to the liver, and that it is the continuance of the other insults (most commonly alcohol, and certain prescription drugs) that makes the mefloquine-induced syndrome persist. Some published evidence supports this view.[38,62,69] There is evidence from this study (Table 1) that the adverse effects of mefloquine in those who have taken the drug prophylactically persist longer than they do in patients who have been treated with it, even though the latter group mostly receive larger doses of the drug; this paradox might be explained by the fact that although prophylactic users usually stop mefloquine as soon as they have experienced adverse effects, they often continue to assault their livers in other ways, and it is this which makes the effects persist. Most of the reported adverse effects of mefloquine fit into the model we propose. For example, mefloquine has been associated with a wide variety of dermatological adverse effects, and most of these can be linked with effects on the liver or thyroid.[41,55] Convulsions and dizziness are other reported effects of mefloquine which can be related to liver or thyroid disturbance.[2,41] Who should not take mefloquine? We believe that people with a history of any proven or suspected liver or thyroid abnormality in the previous two years should avoid mefloquine. Travellers taking mefloquine should not drink alcohol, especially within 24 h of their weekly mefloquine dose. While taking mefloquine, travellers should be advised to maintain good hydration with water or carbonated drinks, especially on long plane journeys or during arduous work in hot conditions. Alcohol, tea or coffee should not be used to maintain hydration, since they all increase water loss. Travellers taking mefloquine should not take a hormonal contraceptive, nor any other drug known to injure liver cells.[34] They should not take any drug known to antagonise thyroid hormone. [53] We propose that drugs that are known to cause hepatocellular injury and also to be thyroid hormone antagonists should be absolutely contraindicated in mefloquine users; such drugs include amiodarone, benzodiazepines, calcium channel blockers and phenytoin.[34,53] Because of the potential for additive toxicity, travellers taking mefloquine should avoid concurrent use of any other quinoline derivative (eg, amodiaquine, chloroquine, primaquine, quinidine, quinine, tafenoquine), whether for additional prophylaxis or for treatment. The administration of a different quinoline derivative at the same time as mefloquine may increase the risk of adverse effects.[70,72] For the same reason, mefloquine users should avoid other quinine analogues, such as fluoroquinolone antibiotics. Fluoroquinolones, such as sparfloxacin, of loxacin and ciprofloxacin, are increasingly prescribed in severe cases of traveller's diarrhoea, and have been associated with severe reactions in mefloquine users. [73] Who should take mefloquine? Mefloquine is a safe and exceptionally useful drug for the mass prophylaxis and treatment of those resident populations in malaria-endemic areas which traditionally abstain from alcohol and hormonal contraception. It has been suggested that in such settings, mefloquine should be combined with artemisinin or a derivative to protect both drugs from resistance.[74] On the basis of our hypothesis, we believe that children, and also pregnant women, can safely be prescribed mefloquine (because neither group uses alcohol or oral contraceptives). Accompanied by explicit advice to avoid alcohol, maintain good hydration, and use co-medications with caution, we believe that prophylactic mefloquine could be recommended to certain occupational subsets of travellers who carry out safety-critical tasks and who until now have been denied the use of this drug. These occupational groups include airline pilots,[75] divers[76] and operators of heavy machinery.[77] Some authorities advise that travellers engaged in high-risk leisure pursuits, such as mountain climbing, should not use mefloquine. [78] We believe that this exclusion is unjustified, as long as there is no recent history of liver or thyroid disease, and provided the precautions we have proposed above (avoidance of alcohol, maintenance of hydration and non-use of hormonal contraception, recreational drugs and certain co-medications) are adhered to. Lobel et al consider that WHO's exclusion of people with a personal or family history of neuropsychiatric illness from taking mefloquine is based 'on limited evidence or theoretical concerns', and we believe their scepticism is justified.[79] Neuropsychiatric illness may not contraindicate use of mefloquine, provided that the patient is not currently taking anything that can cause liver damage or thyroid disturbance. Competing interests None declared. Table 2 Proposed contraindications to the use of mefloquine Mefloquine contraindications: 1. Known hypersensitivity to the drug. 2. History of any proven or suspected liver abnormality within the previous 2 years. 3. History of any proven or suspected thyroid abnormality within the previous 2 years, including any concurrent use of thyroid-stabilising medication. 4. Concurrent use of drugs known to cause hepatocellular injury. Paracetamol and (especially) aspirin to be used with caution, since both can damage the liver and/or the thyroid.[34,40] Mefloquine users should not take recreational drugs. 5. Concurrent use of oral contraceptive pill or HRT. Healthcare advisers should recognise that some female travellers will need alternative contraception. 6. Concurrent use of drugs known to be thyroid hormone antagonists.[53] 7. Up to 2 units of alcohol per day may be taken by users of mefloquine prophylaxis until 24 h before their weekly dose, and from 24 h afterwards. 8. Mefloquine users should avoid concurrent use of any other quinoline derivative (eg, amodiaquine, chloroquine, primaquine, quinidine, quinine, tafenoquine), whether for additional prophylaxis or for treatment. Pre-publication history The pre-publication history for this paper can be accessed here:
Background In the city of Chennai, India, registration of the fact of death is almost complete but the cause of death is often inadequately recorded on the death certificate. A special verbal autopsy (VA) study of 48 000 adult deaths in Chennai during 199597 was conducted to arrive at the probable underlying cause of death and to measure cause specific mortality rates for Chennai. Methods Trained non-medical graduates with at least 15 years of formal education interviewed the surviving family members or an associate of the deceased to write a report on the complaints, symptoms, signs, duration and treatment details of illness prior to death. Each report was reviewed centrally by two physicians independently. The reliability was assessed by comparing deaths attributed to cancer by VA with records in Vital Statistics Department and Chennai Cancer Registry. Results The VA reduced the proportion of deaths attributed to unspecified medical causes and unknown causes from 37% to 7% in early adult life and middle age (2569 yrs) and has yielded fewer unspecified causes (only 10%) than the death certificate. The sensitivity of VA to identify cancer was 94% in the age group 2569. Conclusion VA is practicable for deaths in early adult life or middle age and is of more limited value in old age. A systematic program of VA of a representative sample of deaths could assign broad causes not only to deaths in childhood (as has previously been established) but also to deaths in early adult life and middle age. Background Chennai is a South Indian city of 4.2 million people (in mid-1997) with 33 registered and 35 private burial or cremation grounds. The health authority requires a medical certificate for all deaths at the time of disposal of the body. A medical certificate giving the cause of death can be obtained before taking the body to the burial ground either from the hospital where death took place or (for the 75% of adult deaths attributable to medical causes that take place at home) from a private medical practitioner, which specifies whether the death was due to suicide, homicide, accident, or disease, and which should (but often does not) specify which disease was involved. Most of the time the private medical practitioner will not have treated or examined the deceased. Hence, the information on the death certificates of those who die at home may not reliably describe the underlying cause, if that cause was medical. If a medical certificate was not obtained beforehand, then symptoms, complaints and duration of illness prior to death are collected from the close relatives at the burial ground and forwarded to the Vital Statistics Department (VSD), where a medical officer certifies the "probable" cause of death (and, in particular, records whether death was probably due to medical or to external causes). Individuals whose deaths might have been due to external causes are often subjected to postmortem examination, but others are not. Elsewhere, systematic retrospective inquiry of family members about the symptoms and signs of illness prior to death has been used to help determine the underlying medical cause of death, particularly in childhood [1-3]. For childhood deaths in populations that are not covered by adequate medical services such "verbal autopsies" are now of established value in helping to classify the broad patterns of mortality. Verbal autopsies have also been used to assess the medical causes of maternal deaths [4-7]. Although in India there are about as many deaths in middle age as in childhood, there is less experience with verbal autopsies of adult deaths. The families of all adults (aged 25 and over) who were registered as having died of a medical cause during 199597 in Chennai were interviewed by trained non-medical graduate field workers about the complaints, symptoms, signs and duration of illness prior to death and treatment details of the illness. The field interviewers provided a written summary (usually about half a page) of the interview, which was then read by two independent medical doctors. This report is of the extent to which such verbal autopsies of the adult deaths appear to improve our knowledge of which diseases were the underlying causes of adult deaths before old age (ie at ages 2569). The probable diagnosis of underlying cause of death based on verbal autopsy report helped us to measure reasonably reliable cause specific adult mortality rates for Chennai, India. Methods Information on deaths that occur in Chennai has been maintained manually by trained staff in the Vital Statistics Department (VSD). From the death registers in the Vital Statistics Department, the staff of the Division of Epidemiology at the Cancer Institute abstracted the following data on to a register: deceased name, age, gender, marital status, father/spouse name, informant's name, occupation, place of death, address at the time of death, date of death and recorded cause of death (immediate, underlying and/or contributory). For the deaths that took place in the Government. hospitals these details were copied from the death registers maintained in the hospitals in order to get more information on cause of death. Where there was a stated cause of death on the death certificate, it was coded by the medical record officer, who was trained to code causes of death according to the 9th International Classification of Diseases, Injuries and Causes of Death [8] and it was checked by the first author. The total number of deaths recorded among those aged 25 and above in Chennai during 19951997 was 72 165, of which 5288 (7%) were attributed to external causes by the VSD, ie.. to accident, poisoning, suicide, homicide and other violence. The deaths due to external causes were excluded from this study, leaving the 66 877 deaths attributed by the VSD to medical causes. Male non-medical graduates with at least 15 years of formal education were trained to interview the spouse and/or other close relative of the deceased living in the same house on complaints, symptoms, signs and duration of illness and treatment details of the illness. To limit distress over the terminal event the field visit was carried out at least six months after death. Name of the deceased, father's name (if the deceased was a male) or spouse name (if the deceased was a female), age, gender, informant's name and address of the deceased at the time of death were given to field interviewers to locate the house of the deceased. Field interviewers were blind to the cause of death stated on the death certificate. They were taught about all major symptoms (they had the list with them while doing the field visit). Field interviewers enquired about the death. If a symptom was present (symptom was used as a filter to define what questions to be asked), additional questions were asked about the associated symptoms, duration of the symptoms, any treatment received (type of treatment received), if admitted to a hospital, name and location of the hospital and duration of hospitalisation and, history of past treatment and hospitalisation for the similar episode(s). For example, the filter symptom for heart attack was chest pain and the associated symptoms were breathlessness, sweating, vomiting and pain in the retrosternal area, hand, shoulder, back etc. Cough for more than 4 weeks was a filter for lung cancer and tuberculosis. If the surviving family members were not able to give any information on the symptoms of the illness prior to death, then field interviewers asked them one by one the symptoms given in the list. Field workers wrote the report in local language (Tamil) on the back side of the questionnaire as stated by the spouse / close relative. Thus open format was used for verbal autopsy in Chennai. Each report was reviewed centrally independently by two medical doctors to arrive at "probable underlying cause of death" that they coded it according to the 9th International Classification of Diseases, Injuries and Causes of Death [8]. Discrepancies were always discussed and resolved soon after they arose, which quickly led to the establishment of consistent coding conventions. Six operators entered data twice into the computer. Five supervisors monitored the daily numbers interviewed and checked the field visit reports. The field visit report was validated by re-interview of 5% of the collected data by one or other of two special interviewers, with random selection for re-interview taking place one week after completion of the main interview, and blind to its results. This re-interviewing was done partly because knowledge that a resurvey might well take place would ensure reliably motivated fieldwork at the initial survey, and partly to check whether there were any systematic defects in the technique of any of the field workers: none were found. The VSD records were linked with the Chennai Cancer Registry records to assess the validity of the method used for VA. The Chennai cancer registry is a demographic registry in the network of the Indian Council of Medical Research, Govt. of India. The Registry has been functioning since 1982 at the Cancer Institute(WIA), Chennai. Cancer is not a notifiable disease in India. Hence registration has been done by the direct interview and /or review of medical records for cancer diagnosis and treatment. The Registry operations are constantly monitored with emphasis on the quality of data collected which includes re-screening of cancer cases registered from various sources, exercises on re-abstraction and coding of the diagnosis and treatment on a random sample of cases and, validity checks for unlikely combinations of age, sex, site and morphology. We now report the results of the verbal autopsy among adult deaths and cause specific adult mortality rates for Chennai. Results The total number of deaths attributed by the VSD to medical causes among adults aged 25 and over during 199597 in Chennai was 66 877 (M: 38 649, F: 28 228) of which 75% (50 387) of them were recorded as having taken place at home. Of the total deaths attributed to medical causes by VSD, 18 520 could not be traced, because the address in the vital statistics department was missing or inadequate, the house no longer existed or the occupants had moved house after death. (The training programs for the Birth and Death clerks in the Chennai Vital Statistics Department have subsequently focused on the importance of collecting the complete address with pincode, which would help in tracing the house for any population study in Chennai). For the remaining 48 357 deaths, the spouse (or close associate) was interviewed to determine the probable underlying cause (Flow chart 1, Figure 1). Table 1 shows that the causes of death assigned by the VSD were similar for those traced and those not traced for verbal autopsy. Table 2 shows that the VA reduced the percentage of deaths attributed to unspecified medical causes in each age group. VA is practicable in early adult life and middle age and is of more limited value in old age ( 70 yrs). Figure 1 Figure Table 1 Cause of death from Vital Statistics Department* among adults aged  25 in Chennai, India: 19951997. Traced successfully for VA (n = 48 357) Not traced for VA (n = 18 520) Causes of death (ICD9 codes) Cause of death from VSD Cause of death from VSD M (%) F (%) M (%) F (%) Vascular disease (390415, 418459) 8319(30) 5168(25) 2950(27) 1731(23) Neoplasm (140239) 1163(4) 1002(5) 575(5) 443(6) Respiratory tuberculosis (011, 012, 018) 1399(5) 372(2) 659(6) 161(2) Other respiratory diseases (416, 417, 460519) 1088(4) 596(3) 659(6) 333(4) Other infection (rest of 1139, 279.8 [HIV],3206, 590, 6806) 584(2) 303(2) 33(0.3) 8(0.1) Other specified medical causes 1899(7) 1045(5) 797(7) 446(6) Unspecified medical causes (7809, 7979) 12291(44) 11511(56) 4480(41) 4066(54) No cause given in VSD (hence probably medical) 983(4) 634(3) 770(7) 409(5) Total attributed by VSD to medical causes 27726(100) 20631(100) 10923(100) 7597(100) *Attributed by VSD to non-medical causes - - 3644 1644 Re-assigned by VA to external causes 683 456 - - Assigned by VA To medical causes 27043 20175 - - *Deaths(M: 3644; F:1644) that were assigned by the Vital Statistics Department(VSD) to non-medical causes were excluded from the study. Table 2 Numbers of adult deaths assigned to unspecified medical causes (senility, unknown or ill-defined causes) in VSD and based on verbal autopsy for those in the special study Age-group Number and % of deaths assigned to senility, unknown or ill-defined causes All deaths attributed to medical causes Male Female Male Female VSD VA VSD VA No (%) No (%) No (%) No (%) No No 2534 244 (22) 51 (5) 187 (29) 48 (7) 1095 655 3544 460 (21) 39 (2) 240 (24) 54 (5) 2206 995 4554 790 (21) 65 (2) 396 (23) 71 (4) 3697 1693 5564 2108 (38) 291 (5) 1349 (42) 384 (12) 5614 3182 6569 1931 (56) 422 (12) 1491 (60) 499 (20) 3464 2500 Subtotal: 2569 5533 (34) 868 (5) 3663 (41) 1056 (12) 16076 9025 7074 2184 (60) 815 (23) 1861 (67) 848 (30) 3623 2786 75+ 5066 (69) 2727 (37) 6272 (75) 4056 (49) 7344 8364 Total deaths 12783 4410 11796 5960 27043 20175 In Tables 3 and 4 the underlying cause of death derived from verbal autopsy is compared with the cause of death stated on the death certificates for deaths that occurred in the hospital and did not occur in the hospital(in the community) at ages 2569. For those recorded as dying in hospital and elsewhere(not in the hospital) of natural causes, 25% (1853/7314) and 41% (7343/17787) respectively had unspecified medical cause on the death certificate, but 94% (1739/1853) and 78% (5747/7343) respectively of these deaths then had a specific cause assigned to them by verbal autopsy. The verbal autopsy reduced the proportion of deaths attributed to unspecified medical causes and unknown causes from 37% down to 7%. Of deaths, where causes other than ill-defined or unknown were given both by the death certificate and by VA about 80% of these causes among early adult life and middle age (2569 yrs) agreed in terms of broad groupings. Among those with some information on the cause the proportion attributed to vascular diseases and respiratory diseases in the VSD were decreased from 53% and 17% to 47% and 16% respectively by VA with corresponding increase in the proportions (and, still more so, the absolute numbers) attributed to other causes. Table 3 Deaths (male and female) at ages 2569 in hospital: comparison between cause of death on death certificate and verbal autopsy in Chennai, India in 199597: Verbal autopsy only Cause of death on death certificate Causes Vasc Resp Neop Infect Unsp Other Sp Total 1. Vascular disease (Vasc) 2114 196 19 11 947 129 3416 2. Respiratory diseases including pulmonary TB (Resp) 94 571 6 8 204 35 918 3. Neoplasm (Neop) 85 36 462 22 158 37 800 4. Infection (except Resp) (Infect) 51 29 4 203 111 53 451 5. Unspecified medical (unsp) 30 12 8 1 114 11 176 6.Other specific medical illness(Other Sp.) 219 67 11 45 319 892 1553 Total 2593 911 510 290 1853 1157 7314 ICD9 Codes: Vasc- 390415, 418459; Resp  011, 012, 018, 416, 417, 460519; Neop- 140239, Infect  1139, HIV, 320326, 590, 680686; Unsp  780789, 797799; Other sp  rest of 000799 Table 4 Deaths (male and female) at ages 2569 not in hospital: comparison between cause of death on death certificate and verbal autopsy in Chennai, India in 199597 Verbal autopsy only Cause of death on death certificate Causes Vasc Resp Neop Infect Unsp Other Sp Total 1. Vascular disease (Vasc) 4670 183 18 11 2593 94 7569 2. Respiratory diseases including pulmonary TB (Resp) 290 1438 13 16 971 46 2774 3. Neoplasm (Neop) 297 74 1182 24 670 41 2288 4. Infection (except Resp) (Infect) 103 38 2 217 399 49 808 5. Unspecified medical (unsp) 88 25 20 3 1596 15 1747 6.Other specific medical illness(Other Sp.) 372 53 12 194 1114 856 2601 Total 5820 1811 1247 465 7343 1101 17787 ICD9 Codes: Vasc- 390415, 418459; Resp  011, 012, 018, 416, 417, 460519; Neop- 140239, Infect  1139, HIV, 320326, 590, 680686; Unsp  780789, 797799; Other sp  rest of medical causes Where a cause recorded on the death certificate in the VSD differed from the underlying cause assigned by the VA, there was often no absolute way of knowing which was correct (where the doctor-assigned cause on the death certificate lacked detail, the VA may well be more reliable, and vice-versa) except for cancer deaths which could be verified with the Chennai cancer registry records. Hence, the validity of VA diagnosis was assessed only for cancer deaths(ICD 9:140208) by comparing with the stated cause of death in the VSD; all deaths registered in the VSD irrespective of stated cause of death on the death certificate were linked with the Chennai cancer registry records to confirm the deaths attributed to cancer. Table 5 shows that 3053 deaths were attributed to cancer by VA. Of these, 1435 deaths were attributed to non-cancer in the VSD and data on all except 288 deaths were available in the Chennai Cancer registry; Thus Chennai cancer registry missed data on 288 cancer cases both in the routine morbidity and mortality data registration process. Majority of the missed cancer deaths were attributed to ill-defined/unknown followed by vascular causes in the VSD. All cancer deaths identified by VA were confirmed by linking with Chennai Cancer Registry records and hospital medical records. So there were no false positive cancer deaths recorded by VA. However VA failed to identify 107 cancer deaths. Thus the sensitivity of VA to identify cancer was 94% (1618/1725) and the Chennai cancer registry missed 9% (288/3160) of total cancer deaths in the early adult life and middle age. Table 5 Cancer (ICD 9: 140208) deaths at ages 2569 in Verbal autopsy (VA) and VSD (linked with Chennai cancer registry records) VSD Cancer Noncancer Total VA Cancer 1618 1435 3053 Noncancer 107 21941 22048 Total 1725 23376 25101 Death rates for selected causes for adult men and women are given in Table 6. Higher death rates from complications of pregnancy were seen among young adults and the death rates attributed to diabetes were almost similar among men and women. Death rates for other causes were higher among men compared to women. Table 6 Selected causes and age group & sex-specific death rates/100 000 population (in mid 1996) based on VA reports in Chennai Age  group Causes of death (ICD 9) Sex 2534 3544 4554 5564 6569 7074 75+ 3569 All medical causes M 145.7 342.2 927.0 2153.5 4844.1 8298.6 20710.1 1670.0 F 94.5 180.6 466.5 1347.3 3777.6 7245.0 26011.5 1109.5 Tuberculosis M 27.3 64.5 130.1 189.5 293.7 405.4 603.5 151.7 (011,012,018) F 10.0 16.7 31.7 41.5 117.9 119.6 245.7 42.5 Other respiratory diseases M 4.4 14.4 47.4 128.1 383.2 558.9 1274.6 109.0 (416,417, 460519) F 3.6 8.4 25.4 72.4 207.0 275.7 951.6 59.9 HIV M 1.5 1.1 0.8 0.0 0.0 2.3 0.0 0.5 F 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.1 Other infective causes M 15.2 24.2 47.9 77.5 191.6 274.9 899.6 70.1 (1139,3206,590,6806) F 9.4 11.8 16.0 60.1 167.7 325.1 1219.1 49.1 Neoplasm M 11.6 302 99.5 249.3 478.3 662.0 902.4 176.6 (140239) F 11.4 37.2 106.1 205.8 395.9 595.5 842.8 156.3 Cardiovascular M 39.0 119.6 385.1 941.3 1903.2 2899.8 5662.6 685.0 (390415,41829,44059) F 24.2 63.4 177.5 532.7 1450.6 2319.7 6484.2 428.3 Cerebrovas-cular(stroke) M 2.7 10.1 36.6 115.5 299.3 577.2 1308.5 89.1 (43039) F 3.2 3.6 18.2 74.9 240.3 517.5 1645.2 62.0 Peptic ulcer M 1.1 1.6 4.0 7.3 14.0 34.4 45.1 5.7 (5313) F 1.0 1.3 1.7 5.1 18.1 26.0 49.8 4.9 Diabetes M 1.2 5.1 16.0 82.9 198.6 455.8 995.5 58.1 (250) F 1.3 3.3 24.0 78.8 195.0 522.7 1035.6 58.2 Hepatic M 17.4 38.5 75.0 113.9 186.0 226.8 296.1 91.5 (5703) F 7.5 9.1 15.7 29.6 45.3 62.4 130.6 22.0 Renal M 6.8 11.8 32.6 69.4 138.4 153.5 609.1 52.3 (58099) F 6.2 7.6 19.3 52.9 117.9 166.4 391.9 39.6 Complications of pregnancy (63076) F 4.2 1.3 0.0 0.4 0.0 0.0 0.0 0.5 All non-medical causes M 112.8 114.0 134.5 148.2 154.4 192.7 370.8 135.4 (E800E999, 800999) F 78.9 43.6 35.4 51.5 75.9 140.4 373.4 48.1 Discussion In developed countries, data on disease-specific mortality by age are readily available from national vital registration. In developing countries, the levels of coverage of vital registration and reliability of cause of death stated on the death certificate are generally low (especially in rural areas). Reliable assessment of disease-specific mortality rates is not yet possible in many parts of India, either because the underlying cause of the terminal illness was never known or because the relevant information was not recorded. For legal purposes death records do usually subdivide the causes of death into medical and non-medical (external) causes. But once non-medical causes have been excluded, specification of the underlying cause of a death from disease may be inaccurate, misclassified or missing for about 50% of adult deaths. This special study of verbal autopsy carried out in Chennai among 48 000 adults aged 25 and above at the time of death during 199597. Of the 25 000 male and female deaths at ages 2569, 7000 took place in hospital, and for these the underlying cause should have been entered onto the death certificate by a hospital doctor. For the most of the remaining 18 000 deaths at these ages, the death certificate was obtained from a physician practicing near the deceased's place of residence and he might not have had a chance of treating the deceased when he/she was alive. So, for these deaths, the cause on the death certificate may well have been unspecified (41%). VA yields fewer unspecified causes (only 10%) than the death certificate, particularly for the deaths that did not occur in hospital, and often yields somewhat more specific information, eg. about the approximate site of origin of a cancer, or about evidence of tuberculosis, stroke, myocardial infarction or diabetes. In addition, the probable cause of death arrived by verbal autopsy allows subdivision into the broad groups of disease. The validity of verbal autopsy may have depended on the training of the interviewer, on the immediate random checking of the 5% of interview data to assess the reliability and reproducibility of the procedures and on the availability of physicians to interpret the field reports to arrive at the probable underlying cause of death. In the present study, field reports were reviewed centrally by physicians to arrive at the probable cause of death; for those settings where physician review is not possible, algorithms provide an alternative approach for assigning cause of death [9]. The cause of death arrived based on reviewing verbal autopsy reports by physicians is better than that arrived by opinion-based algorithm [10]. In Chennai, registration of the fact of death is almost complete, as is the subdivision into medical and other causes but certification of the underlying cause of death is less reliable. A study done in Chennai [11] showed that the sensitivity of the death certificate to identify cancer as the cause of death was 57%; in Chennai about 7580% of cancer patients attend health care facilities at late stage of the disease; for about half of those who died at home soon after the diagnosis of cancer (and whose deaths were therefore, in almost all cases, likely to have been caused by their cancer) do not have cancer mentioned on their death certificate [11], and for other diseases the problems might well be even worse. In the present study the sensitivity of VA to identify cancer deaths was 94% and there were no false positive cancer deaths. The high sensitivity seen in this study may be due to the ascertainment of data from the spouse (close associate) by the field interviewers on the type and duration of treatment received by the deceased for their illness and the name of the hospital (location of the hospital) and/ or name of the unit (eg. Cancer unit / Coronary Care Unit etc) where they were admitted for treatment. Most of the close associates of the deceased were aware of the diagnosis of the illness as told by the treating physician. Higher sensitivity of verbal autopsy suggests that there is less likelihood of over estimation of underlying cause of death. In this study wife appeared to be a better responder than husband. This may be due to the following reasons: wife remembers the circumstances that led to death of her spouse better than husband remembering his spouse's death because of more attention and care is given to the health of males in the community than of females in general. About 80% of world's death occur in developing countries; but estimation of cause of death is more difficult in developing countries because of paucity of mortality statistics. Verbal autopsy of all adult deaths helped us to compute mortality rates for Chennai. Among infectious diseases death rate for respiratory tuberculosis was higher compared to other infectious diseases from 2574 years among men and from 2554 years among women. Among vascular diseases death rates from cardiovascular was higher in all age groups in both genders compared to death from stroke. Over all, the mortality rates in Chennai are higher than the rates in developed countries [12]. However mortality rates for neoplasm and cerebrovascular diseases are lower and that for cardiovascular and respiratory diseases are higher in Chennai in the age group 3569 compared to similar age group in developed countries in both genders. The mortality rates among males and females for tuberculosis in the early adult life and middle age in Chennai is about 14-fold and more than 20-fold higher, respectively, than the mortality rates in developed countries. The strengths of the study are: 1. large sample size that included all deaths attributed to medical causes in a defined area to avoid selection bias, 2. open format used for verbal autopsy to collect data on all causes of death instead of restricting to few causes as in a structured questionnaire, 3. review of verbal autopsy reports centrally by 2 physicians, independently, to arrive at probable underlying cause of death and 4. validation of the field visit reports by re-interview of 5% of the collected data by special interviewers. Conclusions The present study shows that the sensitivity of verbal autopsy to identify cancer deaths as 94%. For deaths in early adult life or middle age 'verbal autopsy' yields broad classification of the underlying causes of about 90% of all deaths: in old age, however, the proportion classifiable is substantially lower. The verbal autopsy reduced the proportion of deaths attributed to unspecified medical causes and unknown causes from 37% down to 7%. VA identified 288 deaths at ages 2569 which were not registered in the Chennai Cancer Registry. This study revealed that the possibility of ascertaining at least the leading causes of death, reducing the misclassification of cause of death on the death certificate and deriving the probable underlying cause of death when it has not been reported to the Chennai Vital Statistics Department. Since the proportion of people who die while under medical care is low, about 75% of adult deaths attributable to medical causes took place at home, and more than half of these involved no certified cause in Form 4A, verbal autopsy can be of substantial help in assessing the underlying cause of death. More work, however is still needed to develop optimal strategies for making it more specific, and for combining information from verbal autopsy interviews with information from other sources. Competing interests None declared Authors' contributions Author 1 VG: design, implementaion, conduct, analysis, interpretation of the results and manuscript; review of VA reports. Author 2 RP: design, coordination, interpretation of the results and manuscript Author 3 SK: review of VA reports Author 4 SB: assistance in data base management and statistical work Pre-publication history The pre-publication history for this paper can be accessed here:
Background The aim was to evaluate and validate a bowel disease questionnaire in patients attending an out-patient gastroenterology clinic in Greece. Methods This was a prospective study. Diagnosis was based on detailed clinical and laboratory evaluation. The questionnaire was tested on a pilot group of patients. Interviewer-administration technique was used. One-hundred-and-forty consecutive patients attending the out-patient clinic for the first time and fifty healthy controls selected randomly participated in the study. Reliability (kappa statistics) and validity of the questionnaire were tested. We used logistic regression models and binary recursive partitioning for assessing distinguishing ability among irritable bowel syndrome (IBS), functional dyspepsia and organic disease patients. Results Mean time for questionnaire completion was 18 min. In test-retest procedure a good agreement was obtained (kappa statistics 0.82). There were 55 patients diagnosed as having IBS, 18 with functional dyspepsia (Rome I criteria), 38 with organic disease. Location of pain was a significant distinguishing factor, patients with functional dyspepsia having no lower abdominal pain (p < 0.001). Significant factors distinguishing between IBS and functional dyspepsia were relief of pain by either antacids or defecation (19% vs 71% and 66% vs 0% respectively). Awakening from pain at night was also a factor distinguishing between IBS and organic disease groups (26% vs 61%, p < 0.01). Conclusions This questionnaire for functional bowel disease is a valid and reliable instrument that can distinguish satisfactorily between organic and functional disease in an out-patient setting. Introduction Functional bowel disorders form a heterogeneous group of clinical syndromes related to the gastrointestinal tract that present no histological, endoscopic or imaging abnormalities and are not the result of infectious or metabolic disease. Due to our limited understanding of their pathogenesis, functional bowel disorders remain largely a diagnosis of exclusion. This fact, together with a feeling of uncertainty on the part of the physician, may lead to many unnecessary and expensive tests and examinations in order to rule out cancer or a possibly serious organic disease [1,2]. If there is a situation where the medical history makes an essential contribution towards reaching the correct diagnosis, this holds true for the patient with functional bowel disorders. The need for simple and valid diagnostic criteria for these disorders which are best defined by their symptoms, has led to the search of clusters of positive symptoms that are thought of as characteristic for patients with functional disorders [3,4]. A symptom-based diagnostic classification system has recently been developed by multinational working teams, better known as Rome Committees, resulting in diagnostic criteria for functional gastrointestinal disorders [5]. In order to elicit symptoms relevant to functional disorders, the administration of questionnaires has been proved as valuable. It has been shown that a bowel disease questionnaire may be of value in the gastroenterology outpatient setting, where functional bowel symptoms are commonly reported. Several questionnaires have been evaluated assessing the two major functional bowel disorders, i.e. the irritable bowel syndrome (IBS) and functional dyspepsia (non-ulcer dyspepsia), and trying to distinguish them both from organic diseases and from each other [6-8]. The aim of the present study was 1) to evaluate a bowel disease questionnaire that had been designed for Greek patients attending an out-patient gastroenterology clinic and 2) to analyze the data obtained. Methods Geography and health system The island of Crete, which has approximately 550,000 inhabitants, is divided into four administration prefectures. Each prefecture has a local hospital but there is only one tertiary care hospital: the University Hospital, in Heraklion. The prefecture of Heraklion has approximately 280,000 inhabitants, of which 150,000 are urban and the remaining 130,000 are rural residents. The Gastroenterology Department of the University Hospital of Heraklion is the referral centre for gastrointestinal patients of the island. At the same time it fulfils the function of a primary care centre for subjects with gastrointestinal complaints who are residents of the prefecture of Heraklion. This is due to the structure of the Greek National Health System, according to which subjects from a prefecture are entitled to attend outpatient clinics of a tertiary care Hospital without referral from their general practitioners or rural physicians. Questionnaire creation and mode of administration We created a questionnaire based on the symptom-oriented questionnaire described by Talley et al [9], with several adaptations, and the consensus Rome I criteria [10]. The text of symptom-related questions was formulated by a physician with extensive experience in the management of gastrointestinal outpatients (NF). Two gastroenterologists (IM, PS) revised the text and made necessary changes by eliminating ambiguous questions and expressions in order to get a clear and relevant questionnaire. After reaching the final version, the questionnaire was tested on a pilot group of 30 out-patients using self-administration technique. Only 12 out of the 21 returned questionnaire forms were adequately filled-in so as they could be further evaluated. In order to get reliable results we decided to continue the study using the interviewer-administration technique. Our decision was buttressed by the fact that in a group of 15 patients where it was possible to use both administration techniques, we obtained identical results. The interviewer for all patients was a final year medical student (AK). Participants One-hundred-and-forty consecutive patients attending the outpatient Gastroenterology clinic of the University Hospital of Heraklion for the first time and complaining of abdominal pain, discomfort or disturbed stool movements were invited to provide responses to the questionnaire. There was a refusal rate of 8.6% (12 patients). Fifty healthy controls were randomly selected from visitors to the Orthopedic department of the same hospital. None of them had visited a physician for gastrointestinal symptoms during the last three years. After one to two weeks, the questionnaire was given to be completed for a second time in 12 patients whose symptoms remained stable during this period and in 6 controls in order to check for the concordance of the answers (reliability). After a period of one to three month of clinical evaluation and follow up, a diagnosis was provided by two experienced gastroenterologists (IM, NF). Responses to the questionnaire were not used for the final diagnosis. Healthy controls did not undergo any clinical tests. The final diagnosis was made independently of the questionnaire responses and was based on the results of the clinical and laboratory investigation. All patients underwent endoscopic evaluation of upper or/and lower digestive tract, and ultrasound of upper abdomen, while CT scan was performed when needed. The Rome I diagostic criteria as described in [10] were used, consisting for IBS in the followings: at least 3 months of continuous or recurrent symptoms of 1. Abdominal pain or discomfort that is a) relieved with defecation; and/or b) associated pain with a change in frequency of stool and/or c) associated with a change in consistency of stool; and/or 2. Two or more of the following, at least one-fourth of occasions or days: a)altered stool frequency; b)altered stool form;c)altered stool passage; d)passage of mucus; e) bloating or feeling of abdominal distention [10]. Patients were reached by telephone call 1.52 years after diagnosis for a further confirmation of the final diagnosis. In four cases no conclusive diagnosis was reached. Fourteen patients were lost to follow-up. Statistical analysis Pearson's chi-square tests were performed to assess whether the patients differed from the control subjects with respect to qualitative sociodemographic variables. The factors assessed were gender, marital status, occupation, birth rank, educational and area of residence. Comparisons of the age distribution between patients and controls were made using the Student's t-test. One-way analysis of variance (one-way ANOVA) was used to assess possible differences in age distribution between the patient groups. Subsequently, the applicability of the bowel disease questionnaire in distinguishing between the three bowel disease groups was investigated. Initially, logistic regression models were fitted to examine differences in responses between the disease groups for each of the questionnaire items separately, having adjusted for possible age and sex effects. The significance of each of the factors was obtained by calculating the decrease in deviance when the factor was included in the model (given the null model including age and sex only) and comparing this to the appropriate chi-square distribution. In those questions involving abdominal pain, a separate category was included within each item for those patients who did not respond that they had abdominal pain more than six times in the previous year. In order to determine whether the three patient groups could be separated on the basis of their questionnaire responses, classification rules for the diagnostic groups based on patient responses were derived. Models were fitted using binary recursive partitioning. With these classification models, the initial split is on the most significant predictor and the construction method chooses the next split in an optimal way. In order to determine whether the model could be made more parsimonious without sacrificing its goodness-of-fit, the least important splits were removed using the cost-complexity measure D(T')=D(T') + a size(T'), where D(T') is the deviance of sub-tree T', size(T') is the number of terminal nodes of T' and a is the cost-complexity parameter. For the present study, with three classification groups, taking a = 4 enables one to find the subtree with minimum Akaike's Information Criterion, (this criterion penalizes minus twice the log-likelihood by twice the number of independent parameters) [11]. Finally, cross-validation was performed by splitting the data into ten mutually exclusive sets. The test-retest reliability of the questionnaire was judged with the use of the Kappa statistic to assess concordance between questionnaire responses on two separate occasions. A kappa value of 1 corresponds to a perfect concordance and a value of 0 to a concordance not different from chance [12]. The statistical packages used were SPSS version 7.5 and S-Plus (version 4.5). Results The mean time for interview and completion of the questionnaire was 18 minutes. The subjects who participated understood and answered the questions easily. On re-testing, which was performed on 18 persons at an interval of 714 days, significant agreement on all answers was obtained. Median kappa statistic for all questions was 0.82 (range 0.56 to 1.0). There were 55 patients diagnosed as having IBS, 18 patients with functional dyspepsia, both groups according to the Rome criteria [13], and 38 patients with organic disease (14 with peptic ulcer or gastroesophageal reflux disease, 7 with diseases of the biliary tract, 6 with inflammatory bowel disease, 2 with self-limited infectious colitis, 3 with bacterial overgrowth syndrome, and the remaining 6 patients with various diseases, among these 2 malignancies: cancer of the ampulla of Vater, and of the colon). The one patient who was diagnosed as having both functional dyspepsia and organic disease was excluded from the statistical analysis. Also excluded were the 18 subjects that did not completed the evaluation (14 lost to follow-up and 4 with no conclusive diagnosis). The demographic characteristics of the patients and controls are presented in Table 1. There was no significant difference in age or in sex ratios between patients (median age 54 years, 43% males) and controls (median age 50 years, 58% males). More patients than expected (p < 0.0005) were educated at most to primary level; 74% of patients (95 observed, 85 expected) compared to 46% (23 observed, 33 expected) of the control group (2 = 12.81 on 1 df). Also, more than expected patients came from rural areas (p = 0.012), 73 observed versus 65 expected (2 = 6.4 on 1 df). The homogeneity between the patient groups with regard to their demographic characteristics is indicated by the percentages presented in Table 1. The only significant difference between groups was with respect to age distribution, with organic disease patients being older than those suffering from IBS (p = 0.002). Table 1 Demographic characteristics of patients and controls answering bowel disease questionnaires Irritable bowel syndrome patients Functional dyspepsia patients Organic disease patients Patients lost to follow-up All Patientss Control group Number of subjects 55 17 37 14 128 50 Sex male (%) : female (%) 18 (33) : 37 (67) 9 (53) : 8 (47) 19 (51) : 18 (49) 7 (50) : 7 (50) 56 (43) : 72 (57) 29 (58) : 21 (42) Residence rural (%) 34 (62) 8 (47) 23 (62) 6 (43) 73 (57) 18 (36) Education none or primary (%) 39 (71) 14 (82) 29 (78) 11 (79) 95 (74) 23 (46) Age, median (mean, SE) 46 (49, 1.6) 58 (56, 3.3) 63 (58, 2.8) 54 (56, 4.3) 54 (53, 1.3) 50 (48, 2.5) sOf the 128 patients who agreed to participate and answered the questionnaire, no conclusive diagnosis was reached for four subjects and one person diagnosed as having both dyspepsia and organic disease was omitted from the study. The only evidence of a difference between the patient groups for the demographic variables was in the age distributions. *p < 0.05, chi-square **p < 0.0005, chi-square test The prevalence of symptoms in the subgroups of patients with IBS, functional dyspepsia and organic disease and also in the control group is presented in table 2. Having had abdominal pain on at least six occasions in the previous year was a very common symptom (over 88%) in every disease group. It can be seen that the location of the abdominal pain is a significant distinguishing factor, with patients with functional dyspepsia having no lower abdominal pain (p < 0.001). Other significant factors distinguishing the IBS from the functional dyspepsia group were whether there was pain relief by antacids (19% in IBS, 71% in functional dyspepsia patients), whether pain was relieved on defecation (66% in IBS, 0 in functional dyspepsia patients) and having more stools when the pain began (49% in IBS, 6% in functional dyspepsia patients). For the IBS versus organic disease comparisons, awaking from the pain at nighttime was significantly more often present in patients with organic disease (26% in IBS, 61% in organic disease patients, p < 0.01). Table 2 Prevalence of signs and symptoms in patients with IBS, functional dyspepsia or organic disease and controls, and comparisons of IBS with dyspepsia and organic disease patients using logistic regression models. IBS (n = 55) Functional dyspepsia (n = 17) Organic disease (n = 37) Healthy controls (n = 50) IBS vs Dyspepsia IBS vs Organic Disease % % % % 2statistic (change df), p-value 2statistic (change df), p-value Abdominal pain >6 times last year 96 100 89 36 ns ns Location of pain*: Upper abdomen 4 94 55 50 58.01 (2), p < 0.0001 32.70 (2), p < 0.001 Lower abdomen 45 0 15 22 17.70 (2), p < 0.001 30.42 (2), p < 0.001 All abdomen 51 6 30 28 13.33 (2), p < 0.001 12.87 (2), p < 0.001 Presence of pain*: <1 year 34 29 55 28 ns ns <2 years 17 24 18 17 ns ns >2 years 49 47 27 56 ns ns Duration of pain*: >6 hours 40 35 39 17 ns ns Frequency of pain*: > once a week 74 77 70 22 ns ns Pain reflection*: No reflection 73 70 61 88 ns ns Spine 6 12 21 6 ns ns Hips 13 6 12 6 ns ns Elsewhere 8 12 6 0 ns ns Night pain (wakes subject)* 26 71 61 28 9.76 (2), p < 0.01 9.48 (2), p < 0.01 Pain relieved by antacids* 19 71 36 50 17.48 (2), p < 0.001 6.06 (2), p < 0.05 Pain affected by eating* 30 77 46 50 13.13 (2), p < 0.01 ns Pain appears after meal* 21 47 36 22 7.30 (2), p < 0.05 ns Pain alleviated by eating* 6 24 6 33 ns ns Pain made worse by eating* 19 41 21 11 ns ns Pain relieved by defecation* 66 0 39 33 29.05 (2), p < 0.001 7.24 (2), p < 0.05 More stools when pain begins* 49 6 30 17 14.82 (2), p < 0.001 ns Looser stools when pain begins* 53 6 33 17 13.72 (2), p < 0.05 ns Pain worse after defecation* 4 0 0 0 ns ns Change noticed in bowel habits in the last year 24 6 35 4 3.91 (1), p < 0.05 ns >3 bowel movements/day 16 0 35 4 6.46 (1), p < 0.05 ns <3 bowel movements/week 20 18 11 20 ns ns Have both constipation and diarrhoea 24 12 16 6 ns ns Take medication for constipation 16 6 11 6 ns ns Stools often hard 40 41 27 20 ns ns Have difficulty defecating 51 29 30 14 ns ns Stools often loose & watery 38 12 38 10 5.07 (1), p < 0.05 ns Feeling of incomplete emptying 47 12 32 4 9.22 (1), p < 0.01 ns Often feel that can't delay defecation 36 18 35 8 4.14 (1), p < 0.05 ns Passage of mucus 27 0 24 2 7.48 (1), p < 0.01 ns See blood on defecation: 31 6 24 14 ns ns when wiping 29 6 22 14 ns ns in stools 15 6 16 2 ns ns Have haemorrhoids 36 12 24 16 ns ns Need to defecate wakes subject 9 6 22 0 ns ns Incontinence 4 0 3 0 ns ns Abdominal distension 82 88 51 28 ns 5.58 (1), p < 0.05 Have previously visited physician for one of the problems mentioned 86 94 70 12 ns ns *For pain related questions, the percentages displayed are the percentages of those subjects who have the pain symptom (55 IBS, all 17 functional dyspepsia, 33 organic disease, 18 controls) An example of classification using the model provided in fig. 1 is as follows: a subject aged 45 presenting with pain in the lower abdomen which he/she has had for less than two years, feeling bloated, the pain not being related to eating a meal, would be classified as having IBS (with a probability greater than 0.99) as opposed to having functional dyspepsia or organic disease. If the same subject stated that the pain was related to eating a meal, he/she would again be classified as having IBS using the model, but the probability is now 0.625 versus 0.375 of having organic disease. When the model in fig. 1 was subjected to cost-complexity pruning, the pruned model had nine terminal nodes, with the splits following the pain reflection question now not included (i.e. feeling bloated and the subject's age). The cost of increasing the simplicity of the model was that the misclassification rate rose from 20% to 24%. Cross-validation that was performed on the model indicated that perhaps the most important questions contained in the questionnaire were the presence and location of the abdominal pain and having loose bowel movements. Figure 1 A graphical depiction of the recursive partitioning model classifying gastrointestinal patients as having IBS, functional dyspepsia or organic disease based on responses to a bowel disease questionnaire. Discussion In this study we evaluated a questionnaire that was developed for patients attending a Gastroenterology out-patient Clinic in Greece. In the design of our questionnaire several instruments proposed by other authors were taken into account [4,8,14-19]. As most of these studies were performed in selected populations, the question has been already raised whether the results might not be representative for persons belonging to other groups [20]. We intended to elaborate an instrument that could differentiate among patients with functional dyspepsia, irritable bowel syndrome and organic gastrointestinal disease. We had to adapt our instrument for use within the Greek linguistic and cultural milieu. To provide a disease questionnaire for respondents belonging to other groups requires adaptation, modification and establishing its validity within a different cultural context [21]. In the present study we took these steps under the guidance of a panel of physicians familiar with functional bowel disease. The instrument was validated in subjects having an open access to a Gastroenterology Department and coming from a referral area of 250.000 inhabitants, both rural and urban. Rural residents and subjects with only primary educational level were met more often in the patients' group than in that of healthy controls, the latter consisting of visitors to hospitalized patients. This fact reflects the constitution of the patients' group, which was representative of the whole referral area, in contrast to the controls who came from the urban area of the Hospital. Besides, the control group represented a random sample of the population, thus explaining the fact that several of the controls met the Rome IBS criteria. It has to be reminded that surveys of Western populations have revealed IBS in 1520% of adolescents and adults [22]. Due to a low yield of answered questionnaires when self-administered mode was first undertaken, we used the interviewer-administered technique. All respondents understood the items without difficulty. The instrument was shown to discriminate well among the disease groups of organic disease, functional dyspepsia and irritable bowel syndrome. For reasons of consistency and applicability of the results of this study, we used the Rome I criteria for IBS [10]. The questionnaire for functional bowel disease was shown to be reliable: persons who were asked at two different occasions gave comparable answers. Concerning the symptoms differentiating between IBS and functional dyspepsia, the most important features in our patients were 1) the location of pain, 2) whether the pain was relieved on defecation or by antacids and 3) whether there were more stools when the pain began. In distinguishing between IBS and organic disease, the most prominent features were the awaking from the pain at night and whether the pain was relieved by defecation or antacids. Our results, and especially the criterion referring to pain being relieved by defecation, can be considered as a further validation of the Rome II criteria distinguishing IBS from other groups [22]. When trying to interpret our results, the recursive partitioning model offers the advantage of simplification. Figure 1 is a graphical representation of the model used to discriminate between the three disease groups forming the hospital outpatient sample. The discrimination rules and corresponding probabilities of being in each disease group are presented. There are 13 terminal nodes. The model has a correct classification rate of 80% (22 patients misclassified out of 109). The most significant binary split in the discrimination process is the question related to the location of abdominal pain, and more specifically whether pain is present in the upper abdomen. On the basis of this question alone, the model splits the patients in two groups: those with either IBS or organic disease and those with functional dyspepsia or organic disease. The other significant factors in distinguishing between possible IBS and organic disease patients are the age of the subject, the duration of time for which they have had such a pain, the feeling of bloatedness, the frequency of the pain and whether pain is relieved on defecation. Having determined the localization of pain, significant factors in distinguishing between possible functional dyspepsia and organic disease are the presence of loose stools, whether or not there was reflection of pain to the spine, the presence of bloating and the age of the subject. An important advantage of the recursive partitioning approach over logistic regression is that enables the IBS, functional dyspepsia and organic disease groups to be modeled simultaneously. At the same time, the rules derived are easy to interpret. A summary of the rules derived from the model is provided in Additional File:Appendix. There are a few drawbacks in this study, some of them being shared with similar studies. The model we used assumes that patients fall into exactly one of the three categories, excluding the possibility that a patient may not have any of the three conditions or may have two of them. In fact, only one patient in this study had both organic and functional disease. As this coincidence may not be a rarity in other, differently selected, populations, it constitutes a drawback of this kind of model. A more important point may be the finding that the results of bowel disease questionnaires were not reproduced in comparable and unselected populations [23]. Therefore, the diagnostic value of our instrument may also have little external validity. A further drawback of this study may be the small number of the sample, especially the group with functional dyspepsia. This fact may influence the validity of the comparisons concerning functional dyspepsia but not those concerning functional as opposed to organic diseases. At last, self-administration of the questionnaire, a process that is considered both unbiased for the patient and time-saving for the doctor, was not feasible in the context of this study. None the less, the interview technique proved to be time-saving and to give a better yield of answers and a minimal rate of uncompleted questions. In conclusion, our study showed that the questionnaire for functional bowel disease we have developed is a valid and reliable instrument in the particular cultural and linguistic setting of Greek patients. This questionnaire can distinguish satisfactorily between organic and functional disease. The classification oriented model derived from the evaluation of the results obtained is easy to interpret and it could be used in the out-patient setting. Conpeting interests None declared. Pre-publication history The pre-publication history for this paper can be accessed here: Supplementary Material Additional file Appendix. Rules for the classification of gastrointestinal patients into one of three disease groups derived from a recursive partitioning model. Click here for file